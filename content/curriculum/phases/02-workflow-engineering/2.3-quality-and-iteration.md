---
id: "2.3-quality-and-iteration"
title: "Quality and Iteration"
phase: 2
module: 3
estimatedMinutes: 12
concepts:
  - quality-gates
  - validation-strategies
  - iteration-loops
  - feedback-mechanisms
bloomLevel: "apply"
---

# Quality and Iteration

## WHY This Matters

AI doesn't know when its output is wrong. It can produce confident, well-formatted nonsense. Without quality controls, you're gambling with every AI output.

**The quality imperative:**
- AI outputs need verification (it doesn't self-correct)
- Early feedback is cheaper than late discovery
- Iteration is often faster than perfect prompts
- Quality gates protect downstream work

Professionals don't ship first drafts. AI operators don't ship unvalidated outputs.

---

## WHAT You Need to Know

### Quality Gate Architecture

:::concept[quality-gates]
**Quality gates** are checkpoints where output must meet specific criteria before proceeding. For AI workflows, gates typically check:

- **Completeness**: Did the AI address all requirements?
- **Accuracy**: Are facts and figures correct?
- **Format**: Does output match expected structure?
- **Tone**: Is the voice appropriate?
- **Constraints**: Were restrictions followed?
:::

**Quality gate types:**

| Gate Type | Mechanism | Use When |
|-----------|-----------|----------|
| **Automated** | Rule-based checks | Format, length, required elements |
| **AI-Assisted** | Second AI evaluates first | Subjective quality, consistency |
| **Human** | Person reviews | High stakes, nuanced judgment |
| **Statistical** | Sampling approach | High volume, lower stakes |

### Validation Strategies

**1. Checklist Validation**
Define explicit criteria and verify each:

```
✓ Contains executive summary
✓ All sections from template present
✓ No placeholder text remaining
✓ Word count within range (800-1200)
✓ No competitor mentions
✓ CTA included
✗ Links verified (needs review)
```

**2. AI Cross-Check**
Use a second prompt to evaluate the first output:

```
Review this [content type] against these criteria:
1. [Criterion 1]: Pass/Fail + Explanation
2. [Criterion 2]: Pass/Fail + Explanation
...

Output: JSON with pass/fail for each criterion and overall assessment.
```

**3. Rubric Scoring**
:::concept[rubric-scoring]
**Rubric scoring** uses defined quality levels (not just pass/fail) to assess output. This enables nuanced evaluation and tracking quality over time.

Example rubric dimensions:
- **Accuracy**: 1 (errors) → 5 (verified correct)
- **Completeness**: 1 (missing major elements) → 5 (comprehensive)
- **Clarity**: 1 (confusing) → 5 (crystal clear)
- **Tone**: 1 (inappropriate) → 5 (perfectly matched)
:::

### The Iteration Loop

**When to iterate vs. when to restart:**

| Situation | Action | Rationale |
|-----------|--------|-----------|
| 80%+ acceptable, minor issues | Iterate | Refine what's working |
| 50-80% acceptable, structural issues | Targeted regenerate | Keep good parts, redo bad |
| <50% acceptable | Restart with better prompt | Foundation is wrong |
| Wrong direction entirely | Restart with new approach | Prompt design failed |

**Effective iteration prompts:**

| Issue | Iteration Approach |
|-------|-------------------|
| Too long | "Condense to X words, preserving [key elements]" |
| Missing elements | "Add [specific element] after [location]" |
| Wrong tone | "Rewrite in [desired tone], maintaining content" |
| Factual error | "[X] is incorrect. The correct information is [Y]. Revise." |
| Structural problem | "Reorganize to follow [structure]: first X, then Y, then Z" |

### Building Feedback Loops

:::concept[feedback-loop]
A **feedback loop** captures information about output quality and uses it to improve future performance. In AI workflows:

- **Input**: What was the prompt?
- **Output**: What did AI produce?
- **Evaluation**: How good was it? (score, pass/fail)
- **Learning**: What can improve? (prompt refinement, examples)
:::

**Feedback capture template:**

| Field | Purpose |
|-------|---------|
| Prompt used | Reference for refinement |
| Model/provider | Track performance by model |
| Output quality (1-5) | Quantitative tracking |
| Issues found | Specific problems to address |
| Iterations needed | Efficiency metric |
| Final outcome | Did it ship? Why/why not? |
| Improvement notes | What would work better? |

---

## HOW to Apply This

### Exercise: Design Quality Gates

:::exercise[quality-gate-design]
**Scenario**: You're building an AI workflow to generate weekly customer intelligence reports. The reports:
- Summarize key customer feedback themes
- Highlight urgent issues requiring attention
- Include sentiment trends
- Are read by the executive team

**Design the quality system:**

1. **Define 5+ specific quality criteria** (what makes a good report?)

2. **Create a rubric** with 1-5 scoring for each criterion

3. **Design validation checkpoints:**
   - What can be checked automatically?
   - What needs AI cross-check?
   - What requires human review?

4. **Plan the iteration loop:**
   - What issues would trigger iteration?
   - What would trigger full restart?
   - How many iterations maximum before human escalation?

5. **Design feedback capture:**
   - What would you log for each report?
   - How would you use this data to improve?
:::

### Quality Metrics to Track

| Metric | What It Measures | Target |
|--------|------------------|--------|
| **First-pass rate** | % outputs acceptable without iteration | 70%+ |
| **Iteration count** | Average revisions needed | <2 |
| **Human intervention rate** | % requiring human correction | <20% |
| **Quality score trend** | Rubric scores over time | Improving |
| **Rejection rate** | % outputs not usable | <5% |
| **Time to acceptable** | Duration from prompt to final | Decreasing |

### Common Quality Failures and Fixes

| Failure Mode | Detection | Prevention |
|--------------|-----------|------------|
| Hallucinated facts | Cross-reference, fact-check prompt | Provide source documents |
| Incomplete response | Checklist validation | Explicit requirements in prompt |
| Wrong tone | AI tone check, human spot-check | Clear tone examples in prompt |
| Inconsistent with previous | Compare against prior outputs | Include relevant context |
| Template broken | Format validation | Structured output formats |

### Self-Check

:::checklist[module-2.3-complete]
- [ ] I can design quality gates for AI workflows
- [ ] I understand different validation strategies
- [ ] I know when to iterate vs. restart
- [ ] I can create quality rubrics
- [ ] I understand how to capture feedback for improvement
:::

---

## Up Next

In **Module 2.4: Human-AI Handoffs**, you'll learn how to design the boundaries between AI work and human work—the critical transition points where quality and control live.
