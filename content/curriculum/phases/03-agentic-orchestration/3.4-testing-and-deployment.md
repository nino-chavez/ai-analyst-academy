---
id: "3.4-testing-and-deployment"
title: "Testing and Deployment"
phase: 3
module: 4
estimatedMinutes: 10
concepts:
  - testing-strategies
  - deployment-practices
  - monitoring
  - iterative-improvement
bloomLevel: "apply"
---

# Testing and Deployment

## WHY This Matters

AI applications fail in surprising ways. Unlike traditional software with predictable bugs, AI can produce plausible-but-wrong outputs that look fine until they cause problems. Rigorous testing and careful deployment protect:

- **Your users** from AI mistakes
- **Your reputation** from embarrassing failures
- **Your organization** from liability
- **Your resources** from costly fixes

Ship fast, but ship responsibly.

---

## WHAT You Need to Know

### AI-Specific Testing Challenges

:::concept[ai-testing]
AI testing differs from traditional software testing:

**Traditional software:**
- Given input X, always produces output Y
- Bugs are deterministic
- Edge cases are finite

**AI applications:**
- Same input may produce different outputs
- Failures may be subtle (wrong tone, missed nuance)
- Edge cases are infinite
- Quality is often subjective
:::

### Testing Strategies

**1. Golden Dataset Testing**
Create a set of inputs with known-good outputs:

| Input | Expected Output | Evaluation Criteria |
|-------|-----------------|---------------------|
| [Test case 1] | [Expected response] | Matches key points, appropriate tone |
| [Test case 2] | [Expected response] | Includes required elements |
| [Edge case 1] | [Expected handling] | Graceful failure, escalation |

**2. Persona Testing**
Test with different user types:
- Friendly user with clear request
- Confused user with vague request
- Hostile user testing boundaries
- Technical user with complex needs
- Non-native English speaker

**3. Failure Mode Testing**
Deliberately try to break it:
- Extremely long inputs
- Empty or minimal inputs
- Conflicting instructions
- Off-topic requests
- Prompt injection attempts

**4. Consistency Testing**
Run same input multiple times:
- Are outputs meaningfully similar?
- Does quality vary unacceptably?
- Are there occasional failures?

### Pre-Launch Checklist

:::concept[pre-launch]
Before deploying an AI application:

**Functional checks:**
- [ ] Core use cases work correctly
- [ ] Edge cases handled gracefully
- [ ] Error messages are helpful
- [ ] Performance is acceptable

**Quality checks:**
- [ ] Output quality meets standards
- [ ] Tone and voice are appropriate
- [ ] No hallucinations in test set
- [ ] Sensitive topics handled correctly

**Safety checks:**
- [ ] Prompt injection tested
- [ ] Harmful request handling verified
- [ ] PII handling appropriate
- [ ] Escalation paths work

**Operational checks:**
- [ ] API keys secured
- [ ] Rate limits understood
- [ ] Costs projected
- [ ] Monitoring in place
:::

### Deployment Approaches

**Approach 1: Shadow Mode**
```
User request → Current system responds
            ↘ AI system also runs (not shown to user)
            → Compare outputs, evaluate AI readiness
```

**Approach 2: Limited Rollout**
```
Week 1: 5% of requests → AI
Week 2: 25% of requests → AI (if Week 1 OK)
Week 3: 100% of requests → AI (if Week 2 OK)
```

**Approach 3: Human-in-the-Loop**
```
AI generates → Human reviews → Approved outputs go live
            → Rejected outputs inform improvements
```

**Approach 4: Internal First**
```
Internal users test → Feedback incorporated → External users
```

### Monitoring in Production

| What to Monitor | Why | Red Flags |
|-----------------|-----|-----------|
| **Response quality** | Catch degradation | Complaints, low ratings |
| **Latency** | User experience | >5s response times |
| **Error rate** | System health | >1% API errors |
| **Token usage** | Cost control | Unexpected spikes |
| **Escalation rate** | AI capability | Rising escalations |
| **User satisfaction** | Overall success | Declining feedback |

### Iterative Improvement

After launch, establish feedback loops:

```
Production usage
      ↓
Collect feedback (automated + manual)
      ↓
Identify improvement opportunities
      ↓
Update prompts / configuration
      ↓
Test changes
      ↓
Deploy updates
      ↓
[Repeat]
```

---

## HOW to Apply This

### Exercise: Create a Test Plan

:::exercise[test-plan]
**Scenario**: You've built an AI assistant that helps employees draft expense report justifications. Given expense details, it generates professional justifications suitable for finance approval.

**Create a comprehensive test plan:**

**1. Golden dataset (5 cases):**
Write 5 test inputs and expected outputs:
- Routine expense (lunch with client)
- Large purchase (conference tickets)
- Unusual expense (thank you gift for vendor)
- Ambiguous expense (software subscription)
- Edge case (personal expense mistakenly submitted)

**2. Persona tests:**
How should the system respond to:
- New employee unfamiliar with policies?
- Executive with vague expense descriptions?
- Someone trying to justify personal expenses?

**3. Failure mode tests:**
What happens with:
- Expense over policy limits?
- Missing required information?
- Potentially fraudulent patterns?

**4. Success criteria:**
How will you measure if the system is working?
- Quality metrics
- Operational metrics
- User satisfaction metrics

**5. Deployment plan:**
How would you roll this out?
- Which users first?
- What checkpoints?
- Rollback criteria?
:::

### Testing Template

```
TEST PLAN: [Application Name]

1. GOLDEN DATASET
| ID | Input | Expected Output | Pass Criteria |
|----|-------|-----------------|---------------|
| G1 | ... | ... | ... |
| G2 | ... | ... | ... |

2. PERSONA TESTS
| Persona | Scenario | Expected Behavior |
|---------|----------|-------------------|
| P1 | ... | ... |
| P2 | ... | ... |

3. FAILURE MODE TESTS
| Mode | Input | Expected Handling |
|------|-------|-------------------|
| F1 | ... | ... |
| F2 | ... | ... |

4. SUCCESS METRICS
| Metric | Target | Measurement |
|--------|--------|-------------|
| ... | ... | ... |

5. DEPLOYMENT STAGES
| Stage | Users | Duration | Success Gate |
|-------|-------|----------|--------------|
| ... | ... | ... | ... |
```

### Common Launch Failures

| Failure | Cause | Prevention |
|---------|-------|------------|
| Quality disaster | Insufficient testing | Comprehensive test plan |
| Cost overrun | Token usage underestimated | Load testing, cost projections |
| User confusion | Poor UX | Beta testing with real users |
| Security incident | Prompt injection | Security testing |
| Performance issues | Scale not considered | Load testing |
| Rollback chaos | No plan | Document rollback procedure |

### Self-Check

:::checklist[module-3.4-complete]
- [ ] I understand AI-specific testing challenges
- [ ] I can create golden datasets for testing
- [ ] I know multiple deployment approaches
- [ ] I can set up production monitoring
- [ ] I understand iterative improvement cycles
:::

---

## Phase 3 Complete!

You've built your Implementation skills. Before moving to Phase 4, complete:

**Lab 5: Build an AI Assistant** — Create a functional AI assistant using no-code tools

**Phase 3 Deliverable: Working AI Application** — Build and deploy a functional AI-powered tool that solves a real business problem
