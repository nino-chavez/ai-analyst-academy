{
  "metadata": {
    "id": "phase-3",
    "title": "Agentic Orchestration",
    "description": "Design and deploy autonomous AI agents, multi-agent systems, and intelligent workflows that reason and act",
    "order": 3,
    "color": "phase-3",
    "icon": "agents",
    "estimatedHours": 7,
    "modules": [
      "3.1-no-code-ai-tools",
      "3.2-api-fundamentals",
      "3.3-automation-platforms",
      "3.4-testing-and-deployment",
      "3.5-customization-fine-tuning"
    ],
    "labs": [
      "build-ai-assistant",
      "multi-agent-orchestration"
    ],
    "deliverable": {
      "title": "Multi-Agent System",
      "description": "Design and deploy a working multi-agent system where AI agents collaborate to complete complex tasks"
    }
  },
  "modules": [
    {
      "id": "3.1-no-code-ai-tools",
      "slug": "3.1-no-code-ai-tools",
      "title": "No-Code AI Tools",
      "phase": 3,
      "module": 1,
      "phaseId": "phase-3",
      "estimatedMinutes": 12,
      "bloomLevel": "apply",
      "content": "# No-Code AI Tools\n\n## WHY This Matters\n\nYou don't need to code to build powerful AI applications. The no-code AI ecosystem has exploded, enabling business professionals to:\n\n- **Build custom AI assistants** in hours, not months\n- **Automate workflows** without engineering resources\n- **Prototype ideas** before investing in custom development\n- **Solve real problems** immediately\n\nThe best AI operators know when to use existing tools versus when to build custom solutions.\n\n---\n\n## WHAT You Need to Know\n\n### The No-Code AI Landscape\n\n### Platform Categories\n\n| Category | Examples | Best For |\n|----------|----------|----------|\n| **Custom Assistants** | GPT Builder, Claude Projects, Poe | Specialized chatbots, internal tools |\n| **Automation** | Zapier, Make, n8n | Connecting AI to workflows |\n| **Document Processing** | Docsumo, Parsio, Nanonets | Invoice processing, form extraction |\n| **Content Creation** | Jasper, Copy.ai, Writesonic | Marketing copy, blog content |\n| **Research** | Perplexity, Elicit, Consensus | Research synthesis, citations |\n| **Visual AI** | Midjourney, DALL-E, Canva AI | Image generation, design |\n| **Meeting/Voice** | Otter, Fireflies, Grain | Transcription, meeting notes |\n| **Data** | Julius, Rows, Excel Copilot | Analysis, visualization |\n\n### Building Custom GPTs / Assistants\n\nMost major AI providers now offer \"build your own assistant\" features:\n\n**Core components:**\n\n| Component | Purpose | Example |\n|-----------|---------|---------|\n| **Instructions** | Define behavior and expertise | \"You are a financial analyst who...\" |\n| **Knowledge base** | Upload reference documents | Company policies, product docs |\n| **Capabilities** | Enable features | Web search, code interpreter, image generation |\n| **Conversation starters** | Suggested first questions | \"Help me analyze...\", \"Compare...\" |\n\n**Building process:**\n\n```\n1. Define the use case\n   └─> What problem does this solve? For whom?\n\n2. Write instructions\n   └─> Role, expertise, constraints, format\n\n3. Add knowledge\n   └─> Upload relevant documents\n\n4. Configure capabilities\n   └─> Which tools does it need?\n\n5. Test thoroughly\n   └─> Try edge cases, verify accuracy\n\n6. Iterate and refine\n   └─> Improve based on testing\n\n7. Deploy and monitor\n   └─> Share with users, collect feedback\n```\n\n### Tool Selection Framework\n\n**Decision matrix:**\n\n| If You Need... | Consider... |\n|----------------|-------------|\n| Internal Q&A bot | Custom GPT with company docs |\n| Customer support automation | Intercom, Zendesk AI, custom assistant |\n| Content generation pipeline | Jasper + Zapier |\n| Document data extraction | Docsumo, Parsio |\n| Meeting summaries | Otter, Fireflies |\n| Research synthesis | Perplexity, Elicit |\n| Workflow automation | Zapier, Make |\n\n---\n\n## HOW to Apply This\n\n### Exercise: Build a Custom Assistant\n\n### Custom Assistant Checklist\n\n| Phase | Items |\n|-------|-------|\n| **Planning** | ☐ Use case defined ☐ Target users identified ☐ Success criteria clear |\n| **Building** | ☐ Instructions written ☐ Knowledge uploaded ☐ Capabilities configured |\n| **Testing** | ☐ Happy path tested ☐ Edge cases tested ☐ Errors handled gracefully |\n| **Launch** | ☐ Users trained ☐ Feedback mechanism ready ☐ Monitoring in place |\n\n### Common Mistakes\n\n| Mistake | Problem | Solution |\n|---------|---------|----------|\n| Instructions too vague | Inconsistent behavior | Be specific about format, constraints |\n| Too much knowledge | Confused retrieval | Curate and organize documents |\n| No conversation starters | Users don't know capabilities | Provide clear examples |\n| Skip testing | Fails in real use | Test with real scenarios |\n| No feedback loop | Can't improve | Collect user feedback |\n\n### Self-Check\n\n---\n\n## Up Next\n\nIn **Module 3.2: API Fundamentals**, you'll learn how AI APIs work—enabling you to integrate AI into any system or build more sophisticated applications.",
      "htmlContent": "<h1>No-Code AI Tools</h1>\n<h2>WHY This Matters</h2>\n<p>You don&#39;t need to code to build powerful AI applications. The no-code AI ecosystem has exploded, enabling business professionals to:</p>\n<ul>\n<li><strong>Build custom AI assistants</strong> in hours, not months</li>\n<li><strong>Automate workflows</strong> without engineering resources</li>\n<li><strong>Prototype ideas</strong> before investing in custom development</li>\n<li><strong>Solve real problems</strong> immediately</li>\n</ul>\n<p>The best AI operators know when to use existing tools versus when to build custom solutions.</p>\n<hr>\n<h2>WHAT You Need to Know</h2>\n<h3>The No-Code AI Landscape</h3>\n<h3>Platform Categories</h3>\n<table>\n<thead>\n<tr>\n<th>Category</th>\n<th>Examples</th>\n<th>Best For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Custom Assistants</strong></td>\n<td>GPT Builder, Claude Projects, Poe</td>\n<td>Specialized chatbots, internal tools</td>\n</tr>\n<tr>\n<td><strong>Automation</strong></td>\n<td>Zapier, Make, n8n</td>\n<td>Connecting AI to workflows</td>\n</tr>\n<tr>\n<td><strong>Document Processing</strong></td>\n<td>Docsumo, Parsio, Nanonets</td>\n<td>Invoice processing, form extraction</td>\n</tr>\n<tr>\n<td><strong>Content Creation</strong></td>\n<td>Jasper, Copy.ai, Writesonic</td>\n<td>Marketing copy, blog content</td>\n</tr>\n<tr>\n<td><strong>Research</strong></td>\n<td>Perplexity, Elicit, Consensus</td>\n<td>Research synthesis, citations</td>\n</tr>\n<tr>\n<td><strong>Visual AI</strong></td>\n<td>Midjourney, DALL-E, Canva AI</td>\n<td>Image generation, design</td>\n</tr>\n<tr>\n<td><strong>Meeting/Voice</strong></td>\n<td>Otter, Fireflies, Grain</td>\n<td>Transcription, meeting notes</td>\n</tr>\n<tr>\n<td><strong>Data</strong></td>\n<td>Julius, Rows, Excel Copilot</td>\n<td>Analysis, visualization</td>\n</tr>\n</tbody></table>\n<h3>Building Custom GPTs / Assistants</h3>\n<p>Most major AI providers now offer &quot;build your own assistant&quot; features:</p>\n<p><strong>Core components:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Purpose</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Instructions</strong></td>\n<td>Define behavior and expertise</td>\n<td>&quot;You are a financial analyst who...&quot;</td>\n</tr>\n<tr>\n<td><strong>Knowledge base</strong></td>\n<td>Upload reference documents</td>\n<td>Company policies, product docs</td>\n</tr>\n<tr>\n<td><strong>Capabilities</strong></td>\n<td>Enable features</td>\n<td>Web search, code interpreter, image generation</td>\n</tr>\n<tr>\n<td><strong>Conversation starters</strong></td>\n<td>Suggested first questions</td>\n<td>&quot;Help me analyze...&quot;, &quot;Compare...&quot;</td>\n</tr>\n</tbody></table>\n<p><strong>Building process:</strong></p>\n<pre><code>1. Define the use case\n   └─&gt; What problem does this solve? For whom?\n\n2. Write instructions\n   └─&gt; Role, expertise, constraints, format\n\n3. Add knowledge\n   └─&gt; Upload relevant documents\n\n4. Configure capabilities\n   └─&gt; Which tools does it need?\n\n5. Test thoroughly\n   └─&gt; Try edge cases, verify accuracy\n\n6. Iterate and refine\n   └─&gt; Improve based on testing\n\n7. Deploy and monitor\n   └─&gt; Share with users, collect feedback\n</code></pre>\n<h3>Tool Selection Framework</h3>\n<p><strong>Decision matrix:</strong></p>\n<table>\n<thead>\n<tr>\n<th>If You Need...</th>\n<th>Consider...</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Internal Q&amp;A bot</td>\n<td>Custom GPT with company docs</td>\n</tr>\n<tr>\n<td>Customer support automation</td>\n<td>Intercom, Zendesk AI, custom assistant</td>\n</tr>\n<tr>\n<td>Content generation pipeline</td>\n<td>Jasper + Zapier</td>\n</tr>\n<tr>\n<td>Document data extraction</td>\n<td>Docsumo, Parsio</td>\n</tr>\n<tr>\n<td>Meeting summaries</td>\n<td>Otter, Fireflies</td>\n</tr>\n<tr>\n<td>Research synthesis</td>\n<td>Perplexity, Elicit</td>\n</tr>\n<tr>\n<td>Workflow automation</td>\n<td>Zapier, Make</td>\n</tr>\n</tbody></table>\n<hr>\n<h2>HOW to Apply This</h2>\n<h3>Exercise: Build a Custom Assistant</h3>\n<h3>Custom Assistant Checklist</h3>\n<table>\n<thead>\n<tr>\n<th>Phase</th>\n<th>Items</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Planning</strong></td>\n<td>☐ Use case defined ☐ Target users identified ☐ Success criteria clear</td>\n</tr>\n<tr>\n<td><strong>Building</strong></td>\n<td>☐ Instructions written ☐ Knowledge uploaded ☐ Capabilities configured</td>\n</tr>\n<tr>\n<td><strong>Testing</strong></td>\n<td>☐ Happy path tested ☐ Edge cases tested ☐ Errors handled gracefully</td>\n</tr>\n<tr>\n<td><strong>Launch</strong></td>\n<td>☐ Users trained ☐ Feedback mechanism ready ☐ Monitoring in place</td>\n</tr>\n</tbody></table>\n<h3>Common Mistakes</h3>\n<table>\n<thead>\n<tr>\n<th>Mistake</th>\n<th>Problem</th>\n<th>Solution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Instructions too vague</td>\n<td>Inconsistent behavior</td>\n<td>Be specific about format, constraints</td>\n</tr>\n<tr>\n<td>Too much knowledge</td>\n<td>Confused retrieval</td>\n<td>Curate and organize documents</td>\n</tr>\n<tr>\n<td>No conversation starters</td>\n<td>Users don&#39;t know capabilities</td>\n<td>Provide clear examples</td>\n</tr>\n<tr>\n<td>Skip testing</td>\n<td>Fails in real use</td>\n<td>Test with real scenarios</td>\n</tr>\n<tr>\n<td>No feedback loop</td>\n<td>Can&#39;t improve</td>\n<td>Collect user feedback</td>\n</tr>\n</tbody></table>\n<h3>Self-Check</h3>\n<hr>\n<h2>Up Next</h2>\n<p>In <strong>Module 3.2: API Fundamentals</strong>, you&#39;ll learn how AI APIs work—enabling you to integrate AI into any system or build more sophisticated applications.</p>\n",
      "sections": [
        {
          "id": "why-this-matters",
          "title": "WHY This Matters",
          "type": "why",
          "content": "You don't need to code to build powerful AI applications. The no-code AI ecosystem has exploded, enabling business professionals to:\n\n- **Build custom AI assistants** in hours, not months\n- **Automate workflows** without engineering resources\n- **Prototype ideas** before investing in custom development\n- **Solve real problems** immediately\n\nThe best AI operators know when to use existing tools versus when to build custom solutions.\n\n---",
          "htmlContent": "<p>You don&#39;t need to code to build powerful AI applications. The no-code AI ecosystem has exploded, enabling business professionals to:</p>\n<ul>\n<li><strong>Build custom AI assistants</strong> in hours, not months</li>\n<li><strong>Automate workflows</strong> without engineering resources</li>\n<li><strong>Prototype ideas</strong> before investing in custom development</li>\n<li><strong>Solve real problems</strong> immediately</li>\n</ul>\n<p>The best AI operators know when to use existing tools versus when to build custom solutions.</p>\n<hr>\n"
        },
        {
          "id": "what-you-need-to-know",
          "title": "WHAT You Need to Know",
          "type": "what",
          "content": "### The No-Code AI Landscape\n\n### Platform Categories\n\n| Category | Examples | Best For |\n|----------|----------|----------|\n| **Custom Assistants** | GPT Builder, Claude Projects, Poe | Specialized chatbots, internal tools |\n| **Automation** | Zapier, Make, n8n | Connecting AI to workflows |\n| **Document Processing** | Docsumo, Parsio, Nanonets | Invoice processing, form extraction |\n| **Content Creation** | Jasper, Copy.ai, Writesonic | Marketing copy, blog content |\n| **Research** | Perplexity, Elicit, Consensus | Research synthesis, citations |\n| **Visual AI** | Midjourney, DALL-E, Canva AI | Image generation, design |\n| **Meeting/Voice** | Otter, Fireflies, Grain | Transcription, meeting notes |\n| **Data** | Julius, Rows, Excel Copilot | Analysis, visualization |\n\n### Building Custom GPTs / Assistants\n\nMost major AI providers now offer \"build your own assistant\" features:\n\n**Core components:**\n\n| Component | Purpose | Example |\n|-----------|---------|---------|\n| **Instructions** | Define behavior and expertise | \"You are a financial analyst who...\" |\n| **Knowledge base** | Upload reference documents | Company policies, product docs |\n| **Capabilities** | Enable features | Web search, code interpreter, image generation |\n| **Conversation starters** | Suggested first questions | \"Help me analyze...\", \"Compare...\" |\n\n**Building process:**\n\n```\n1. Define the use case\n   └─> What problem does this solve? For whom?\n\n2. Write instructions\n   └─> Role, expertise, constraints, format\n\n3. Add knowledge\n   └─> Upload relevant documents\n\n4. Configure capabilities\n   └─> Which tools does it need?\n\n5. Test thoroughly\n   └─> Try edge cases, verify accuracy\n\n6. Iterate and refine\n   └─> Improve based on testing\n\n7. Deploy and monitor\n   └─> Share with users, collect feedback\n```\n\n### Tool Selection Framework\n\n**Decision matrix:**\n\n| If You Need... | Consider... |\n|----------------|-------------|\n| Internal Q&A bot | Custom GPT with company docs |\n| Customer support automation | Intercom, Zendesk AI, custom assistant |\n| Content generation pipeline | Jasper + Zapier |\n| Document data extraction | Docsumo, Parsio |\n| Meeting summaries | Otter, Fireflies |\n| Research synthesis | Perplexity, Elicit |\n| Workflow automation | Zapier, Make |\n\n---",
          "htmlContent": "<h3>The No-Code AI Landscape</h3>\n<h3>Platform Categories</h3>\n<table>\n<thead>\n<tr>\n<th>Category</th>\n<th>Examples</th>\n<th>Best For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Custom Assistants</strong></td>\n<td>GPT Builder, Claude Projects, Poe</td>\n<td>Specialized chatbots, internal tools</td>\n</tr>\n<tr>\n<td><strong>Automation</strong></td>\n<td>Zapier, Make, n8n</td>\n<td>Connecting AI to workflows</td>\n</tr>\n<tr>\n<td><strong>Document Processing</strong></td>\n<td>Docsumo, Parsio, Nanonets</td>\n<td>Invoice processing, form extraction</td>\n</tr>\n<tr>\n<td><strong>Content Creation</strong></td>\n<td>Jasper, Copy.ai, Writesonic</td>\n<td>Marketing copy, blog content</td>\n</tr>\n<tr>\n<td><strong>Research</strong></td>\n<td>Perplexity, Elicit, Consensus</td>\n<td>Research synthesis, citations</td>\n</tr>\n<tr>\n<td><strong>Visual AI</strong></td>\n<td>Midjourney, DALL-E, Canva AI</td>\n<td>Image generation, design</td>\n</tr>\n<tr>\n<td><strong>Meeting/Voice</strong></td>\n<td>Otter, Fireflies, Grain</td>\n<td>Transcription, meeting notes</td>\n</tr>\n<tr>\n<td><strong>Data</strong></td>\n<td>Julius, Rows, Excel Copilot</td>\n<td>Analysis, visualization</td>\n</tr>\n</tbody></table>\n<h3>Building Custom GPTs / Assistants</h3>\n<p>Most major AI providers now offer &quot;build your own assistant&quot; features:</p>\n<p><strong>Core components:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Purpose</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Instructions</strong></td>\n<td>Define behavior and expertise</td>\n<td>&quot;You are a financial analyst who...&quot;</td>\n</tr>\n<tr>\n<td><strong>Knowledge base</strong></td>\n<td>Upload reference documents</td>\n<td>Company policies, product docs</td>\n</tr>\n<tr>\n<td><strong>Capabilities</strong></td>\n<td>Enable features</td>\n<td>Web search, code interpreter, image generation</td>\n</tr>\n<tr>\n<td><strong>Conversation starters</strong></td>\n<td>Suggested first questions</td>\n<td>&quot;Help me analyze...&quot;, &quot;Compare...&quot;</td>\n</tr>\n</tbody></table>\n<p><strong>Building process:</strong></p>\n<pre><code>1. Define the use case\n   └─&gt; What problem does this solve? For whom?\n\n2. Write instructions\n   └─&gt; Role, expertise, constraints, format\n\n3. Add knowledge\n   └─&gt; Upload relevant documents\n\n4. Configure capabilities\n   └─&gt; Which tools does it need?\n\n5. Test thoroughly\n   └─&gt; Try edge cases, verify accuracy\n\n6. Iterate and refine\n   └─&gt; Improve based on testing\n\n7. Deploy and monitor\n   └─&gt; Share with users, collect feedback\n</code></pre>\n<h3>Tool Selection Framework</h3>\n<p><strong>Decision matrix:</strong></p>\n<table>\n<thead>\n<tr>\n<th>If You Need...</th>\n<th>Consider...</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Internal Q&amp;A bot</td>\n<td>Custom GPT with company docs</td>\n</tr>\n<tr>\n<td>Customer support automation</td>\n<td>Intercom, Zendesk AI, custom assistant</td>\n</tr>\n<tr>\n<td>Content generation pipeline</td>\n<td>Jasper + Zapier</td>\n</tr>\n<tr>\n<td>Document data extraction</td>\n<td>Docsumo, Parsio</td>\n</tr>\n<tr>\n<td>Meeting summaries</td>\n<td>Otter, Fireflies</td>\n</tr>\n<tr>\n<td>Research synthesis</td>\n<td>Perplexity, Elicit</td>\n</tr>\n<tr>\n<td>Workflow automation</td>\n<td>Zapier, Make</td>\n</tr>\n</tbody></table>\n<hr>\n"
        },
        {
          "id": "how-to-apply-this",
          "title": "HOW to Apply This",
          "type": "how",
          "content": "### Exercise: Build a Custom Assistant\n\n### Custom Assistant Checklist\n\n| Phase | Items |\n|-------|-------|\n| **Planning** | ☐ Use case defined ☐ Target users identified ☐ Success criteria clear |\n| **Building** | ☐ Instructions written ☐ Knowledge uploaded ☐ Capabilities configured |\n| **Testing** | ☐ Happy path tested ☐ Edge cases tested ☐ Errors handled gracefully |\n| **Launch** | ☐ Users trained ☐ Feedback mechanism ready ☐ Monitoring in place |\n\n### Common Mistakes\n\n| Mistake | Problem | Solution |\n|---------|---------|----------|\n| Instructions too vague | Inconsistent behavior | Be specific about format, constraints |\n| Too much knowledge | Confused retrieval | Curate and organize documents |\n| No conversation starters | Users don't know capabilities | Provide clear examples |\n| Skip testing | Fails in real use | Test with real scenarios |\n| No feedback loop | Can't improve | Collect user feedback |\n\n### Self-Check\n\n---",
          "htmlContent": "<h3>Exercise: Build a Custom Assistant</h3>\n<h3>Custom Assistant Checklist</h3>\n<table>\n<thead>\n<tr>\n<th>Phase</th>\n<th>Items</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Planning</strong></td>\n<td>☐ Use case defined ☐ Target users identified ☐ Success criteria clear</td>\n</tr>\n<tr>\n<td><strong>Building</strong></td>\n<td>☐ Instructions written ☐ Knowledge uploaded ☐ Capabilities configured</td>\n</tr>\n<tr>\n<td><strong>Testing</strong></td>\n<td>☐ Happy path tested ☐ Edge cases tested ☐ Errors handled gracefully</td>\n</tr>\n<tr>\n<td><strong>Launch</strong></td>\n<td>☐ Users trained ☐ Feedback mechanism ready ☐ Monitoring in place</td>\n</tr>\n</tbody></table>\n<h3>Common Mistakes</h3>\n<table>\n<thead>\n<tr>\n<th>Mistake</th>\n<th>Problem</th>\n<th>Solution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Instructions too vague</td>\n<td>Inconsistent behavior</td>\n<td>Be specific about format, constraints</td>\n</tr>\n<tr>\n<td>Too much knowledge</td>\n<td>Confused retrieval</td>\n<td>Curate and organize documents</td>\n</tr>\n<tr>\n<td>No conversation starters</td>\n<td>Users don&#39;t know capabilities</td>\n<td>Provide clear examples</td>\n</tr>\n<tr>\n<td>Skip testing</td>\n<td>Fails in real use</td>\n<td>Test with real scenarios</td>\n</tr>\n<tr>\n<td>No feedback loop</td>\n<td>Can&#39;t improve</td>\n<td>Collect user feedback</td>\n</tr>\n</tbody></table>\n<h3>Self-Check</h3>\n<hr>\n"
        },
        {
          "id": "up-next",
          "title": "Up Next",
          "type": "generic",
          "content": "In **Module 3.2: API Fundamentals**, you'll learn how AI APIs work—enabling you to integrate AI into any system or build more sophisticated applications.",
          "htmlContent": "<p>In <strong>Module 3.2: API Fundamentals</strong>, you&#39;ll learn how AI APIs work—enabling you to integrate AI into any system or build more sophisticated applications.</p>\n"
        }
      ],
      "concepts": [
        {
          "id": "no-code-ai",
          "term": "no code ai",
          "definition": "**No-code AI tools** provide visual interfaces and templates that let you build AI applications without writing code. Categories include:\n\n- **Custom GPTs / Assistants**: Build specialized chatbots\n- **Automation platforms**: Connect AI to other tools\n- **Document AI**: Extract and process document data\n- **Content generators**: Create text, images, video\n- **Voice/transcription**: Audio processing\n- **Data analysis**: AI-powered spreadsheets and dashboards",
          "htmlDefinition": "<p><strong>No-code AI tools</strong> provide visual interfaces and templates that let you build AI applications without writing code. Categories include:</p>\n<ul>\n<li><strong>Custom GPTs / Assistants</strong>: Build specialized chatbots</li>\n<li><strong>Automation platforms</strong>: Connect AI to other tools</li>\n<li><strong>Document AI</strong>: Extract and process document data</li>\n<li><strong>Content generators</strong>: Create text, images, video</li>\n<li><strong>Voice/transcription</strong>: Audio processing</li>\n<li><strong>Data analysis</strong>: AI-powered spreadsheets and dashboards</li>\n</ul>\n"
        },
        {
          "id": "tool-selection",
          "term": "tool selection",
          "definition": "Choosing the right no-code tool requires matching capabilities to requirements:\n\n**Selection criteria:**\n- **Use case fit**: Does it solve your specific problem?\n- **Integration**: Does it connect to your existing tools?\n- **Pricing**: Does the cost model match your usage?\n- **Data handling**: Where does data go? Is it secure?\n- **Customization**: Can you modify to fit your needs?\n- **Support**: What happens when something breaks?",
          "htmlDefinition": "<p>Choosing the right no-code tool requires matching capabilities to requirements:</p>\n<p><strong>Selection criteria:</strong></p>\n<ul>\n<li><strong>Use case fit</strong>: Does it solve your specific problem?</li>\n<li><strong>Integration</strong>: Does it connect to your existing tools?</li>\n<li><strong>Pricing</strong>: Does the cost model match your usage?</li>\n<li><strong>Data handling</strong>: Where does data go? Is it secure?</li>\n<li><strong>Customization</strong>: Can you modify to fit your needs?</li>\n<li><strong>Support</strong>: What happens when something breaks?</li>\n</ul>\n"
        }
      ],
      "exercises": [
        {
          "id": "build-assistant",
          "title": "Task",
          "instructions": "Design and build a custom AI assistant for a specific business use case.\n\n**Choose one scenario:**\n\n**A. Sales Call Prep Assistant**\n- Purpose: Help sales reps prepare for customer calls\n- Knowledge: Product catalog, pricing, competitor info\n- Capabilities: Research, generate talking points, role-play objections\n\n**B. HR Policy Assistant**\n- Purpose: Answer employee questions about company policies\n- Knowledge: Employee handbook, benefits guide, leave policies\n- Capabilities: Quote relevant policies, clarify ambiguities\n\n**C. Content Brief Generator**\n- Purpose: Create detailed briefs for content creators\n- Knowledge: Style guide, brand voice, successful examples\n- Capabilities: Research topics, generate outlines, suggest keywords\n\n**For your chosen scenario:**\n1. Write the system instructions (role, expertise, constraints)\n2. List documents to upload to knowledge base\n3. Specify which capabilities to enable\n4. Create 5 conversation starters\n5. Write 3 test scenarios to verify quality",
          "htmlInstructions": "<p>Design and build a custom AI assistant for a specific business use case.</p>\n<p><strong>Choose one scenario:</strong></p>\n<p><strong>A. Sales Call Prep Assistant</strong></p>\n<ul>\n<li>Purpose: Help sales reps prepare for customer calls</li>\n<li>Knowledge: Product catalog, pricing, competitor info</li>\n<li>Capabilities: Research, generate talking points, role-play objections</li>\n</ul>\n<p><strong>B. HR Policy Assistant</strong></p>\n<ul>\n<li>Purpose: Answer employee questions about company policies</li>\n<li>Knowledge: Employee handbook, benefits guide, leave policies</li>\n<li>Capabilities: Quote relevant policies, clarify ambiguities</li>\n</ul>\n<p><strong>C. Content Brief Generator</strong></p>\n<ul>\n<li>Purpose: Create detailed briefs for content creators</li>\n<li>Knowledge: Style guide, brand voice, successful examples</li>\n<li>Capabilities: Research topics, generate outlines, suggest keywords</li>\n</ul>\n<p><strong>For your chosen scenario:</strong></p>\n<ol>\n<li>Write the system instructions (role, expertise, constraints)</li>\n<li>List documents to upload to knowledge base</li>\n<li>Specify which capabilities to enable</li>\n<li>Create 5 conversation starters</li>\n<li>Write 3 test scenarios to verify quality</li>\n</ol>\n"
        }
      ],
      "checklists": [
        {
          "id": "module-3.1-complete",
          "items": [
            {
              "id": "module-3.1-complete-0",
              "text": "I know the major categories of no-code AI tools",
              "completed": false
            },
            {
              "id": "module-3.1-complete-1",
              "text": "I understand the components of custom AI assistants",
              "completed": false
            },
            {
              "id": "module-3.1-complete-2",
              "text": "I can select appropriate tools for different use cases",
              "completed": false
            },
            {
              "id": "module-3.1-complete-3",
              "text": "I can design a custom assistant from scratch",
              "completed": false
            },
            {
              "id": "module-3.1-complete-4",
              "text": "I know the common mistakes to avoid",
              "completed": false
            }
          ]
        }
      ]
    },
    {
      "id": "3.2-api-fundamentals",
      "slug": "3.2-api-fundamentals",
      "title": "API Fundamentals",
      "phase": 3,
      "module": 2,
      "phaseId": "phase-3",
      "estimatedMinutes": 12,
      "bloomLevel": "understand",
      "content": "# API Fundamentals\n\n## WHY This Matters\n\nEvery AI application—from ChatGPT to custom enterprise tools—runs on APIs. Understanding APIs is the bridge between using off-the-shelf tools and building custom solutions. You need API knowledge to:\n\n- **Configure integrations** that automation tools provide\n- **Debug problems** when AI workflows break\n- **Evaluate vendors** and understand their capabilities\n- **Collaborate with developers** effectively\n- **Build more sophisticated** AI applications\n\nYou don't need to write code, but you need to speak the language.\n\n---\n\n## WHAT You Need to Know\n\n### What is an API?\n\n### The Request-Response Cycle\n\n```\n┌─────────────┐                  ┌─────────────┐\n│   CLIENT    │   ──Request──>   │   SERVER    │\n│  (your app) │                  │  (AI API)   │\n│             │   <──Response──  │             │\n└─────────────┘                  └─────────────┘\n```\n\n**Request components:**\n\n| Component | Purpose | Example |\n|-----------|---------|---------|\n| **Endpoint** | URL to call | `https://api.openai.com/v1/chat/completions` |\n| **Method** | Type of action | `POST` (send data), `GET` (retrieve data) |\n| **Headers** | Metadata | Authentication, content type |\n| **Body** | The actual data | Your prompt, parameters |\n\n**Response components:**\n\n| Component | Purpose | Example |\n|-----------|---------|---------|\n| **Status code** | Success/failure | 200 (OK), 400 (bad request), 500 (server error) |\n| **Headers** | Response metadata | Rate limit info, content type |\n| **Body** | The actual data | AI response, usage stats |\n\n### Authentication\n\n**Authentication patterns:**\n\n| Pattern | Usage | Example |\n|---------|-------|---------|\n| **API Key** | Most common | OpenAI, Anthropic, Google |\n| **Bearer Token** | OAuth flows | Google Cloud, Microsoft |\n| **Basic Auth** | Legacy systems | Username + password encoded |\n\n### AI API Request Structure\n\nA typical AI API request (simplified):\n\n```json\n{\n  \"model\": \"gpt-4o\",\n  \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n    {\"role\": \"user\", \"content\": \"Explain APIs simply\"}\n  ],\n  \"temperature\": 0.7,\n  \"max_tokens\": 500\n}\n```\n\n**Key parameters:**\n\n| Parameter | Purpose | Impact |\n|-----------|---------|--------|\n| `model` | Which AI model | Capability, cost, speed |\n| `messages` | Conversation history | Context for response |\n| `temperature` | Randomness (0-1) | Creativity vs. consistency |\n| `max_tokens` | Response length limit | Cost, completeness |\n| `top_p` | Probability threshold | Alternative to temperature |\n| `stop` | Stop sequences | Control response endings |\n\n### AI API Response Structure\n\nA typical response (simplified):\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"model\": \"gpt-4o\",\n  \"choices\": [\n    {\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"An API is like a waiter...\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 25,\n    \"completion_tokens\": 150,\n    \"total_tokens\": 175\n  }\n}\n```\n\n**What to look for:**\n\n| Field | Importance |\n|-------|------------|\n| `choices[0].message.content` | The actual AI response |\n| `finish_reason` | Why it stopped (stop, length, error) |\n| `usage` | Token counts for billing/limits |\n\n### Error Handling\n\n**Error handling strategies:**\n\n| Error Type | Strategy |\n|------------|----------|\n| 400 errors | Fix your request |\n| 401/403 errors | Check API key |\n| 429 errors | Wait and retry, use backoff |\n| 500/503 errors | Retry, fall back to alternative |\n\n---\n\n## HOW to Apply This\n\n### Exercise: Read API Documentation\n\n### API Terminology Quick Reference\n\n| Term | Definition |\n|------|------------|\n| **Endpoint** | URL where API lives |\n| **Request** | What you send to API |\n| **Response** | What API sends back |\n| **Header** | Metadata for request/response |\n| **Body** | Main content of request/response |\n| **JSON** | Data format for API communication |\n| **Status code** | Numeric response indicator |\n| **Rate limit** | How often you can call API |\n| **Token** | Unit of text for AI processing |\n| **Latency** | Time for API to respond |\n\n### Self-Check\n\n---\n\n## Up Next\n\nIn **Module 3.3: Automation Platforms**, you'll learn how to connect AI APIs to other tools using no-code automation platforms—building workflows that run without manual intervention.",
      "htmlContent": "<h1>API Fundamentals</h1>\n<h2>WHY This Matters</h2>\n<p>Every AI application—from ChatGPT to custom enterprise tools—runs on APIs. Understanding APIs is the bridge between using off-the-shelf tools and building custom solutions. You need API knowledge to:</p>\n<ul>\n<li><strong>Configure integrations</strong> that automation tools provide</li>\n<li><strong>Debug problems</strong> when AI workflows break</li>\n<li><strong>Evaluate vendors</strong> and understand their capabilities</li>\n<li><strong>Collaborate with developers</strong> effectively</li>\n<li><strong>Build more sophisticated</strong> AI applications</li>\n</ul>\n<p>You don&#39;t need to write code, but you need to speak the language.</p>\n<hr>\n<h2>WHAT You Need to Know</h2>\n<h3>What is an API?</h3>\n<h3>The Request-Response Cycle</h3>\n<pre><code>┌─────────────┐                  ┌─────────────┐\n│   CLIENT    │   ──Request──&gt;   │   SERVER    │\n│  (your app) │                  │  (AI API)   │\n│             │   &lt;──Response──  │             │\n└─────────────┘                  └─────────────┘\n</code></pre>\n<p><strong>Request components:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Purpose</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Endpoint</strong></td>\n<td>URL to call</td>\n<td><code>https://api.openai.com/v1/chat/completions</code></td>\n</tr>\n<tr>\n<td><strong>Method</strong></td>\n<td>Type of action</td>\n<td><code>POST</code> (send data), <code>GET</code> (retrieve data)</td>\n</tr>\n<tr>\n<td><strong>Headers</strong></td>\n<td>Metadata</td>\n<td>Authentication, content type</td>\n</tr>\n<tr>\n<td><strong>Body</strong></td>\n<td>The actual data</td>\n<td>Your prompt, parameters</td>\n</tr>\n</tbody></table>\n<p><strong>Response components:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Purpose</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Status code</strong></td>\n<td>Success/failure</td>\n<td>200 (OK), 400 (bad request), 500 (server error)</td>\n</tr>\n<tr>\n<td><strong>Headers</strong></td>\n<td>Response metadata</td>\n<td>Rate limit info, content type</td>\n</tr>\n<tr>\n<td><strong>Body</strong></td>\n<td>The actual data</td>\n<td>AI response, usage stats</td>\n</tr>\n</tbody></table>\n<h3>Authentication</h3>\n<p><strong>Authentication patterns:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Pattern</th>\n<th>Usage</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>API Key</strong></td>\n<td>Most common</td>\n<td>OpenAI, Anthropic, Google</td>\n</tr>\n<tr>\n<td><strong>Bearer Token</strong></td>\n<td>OAuth flows</td>\n<td>Google Cloud, Microsoft</td>\n</tr>\n<tr>\n<td><strong>Basic Auth</strong></td>\n<td>Legacy systems</td>\n<td>Username + password encoded</td>\n</tr>\n</tbody></table>\n<h3>AI API Request Structure</h3>\n<p>A typical AI API request (simplified):</p>\n<pre><code class=\"language-json\">{\n  &quot;model&quot;: &quot;gpt-4o&quot;,\n  &quot;messages&quot;: [\n    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant&quot;},\n    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Explain APIs simply&quot;}\n  ],\n  &quot;temperature&quot;: 0.7,\n  &quot;max_tokens&quot;: 500\n}\n</code></pre>\n<p><strong>Key parameters:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Parameter</th>\n<th>Purpose</th>\n<th>Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>model</code></td>\n<td>Which AI model</td>\n<td>Capability, cost, speed</td>\n</tr>\n<tr>\n<td><code>messages</code></td>\n<td>Conversation history</td>\n<td>Context for response</td>\n</tr>\n<tr>\n<td><code>temperature</code></td>\n<td>Randomness (0-1)</td>\n<td>Creativity vs. consistency</td>\n</tr>\n<tr>\n<td><code>max_tokens</code></td>\n<td>Response length limit</td>\n<td>Cost, completeness</td>\n</tr>\n<tr>\n<td><code>top_p</code></td>\n<td>Probability threshold</td>\n<td>Alternative to temperature</td>\n</tr>\n<tr>\n<td><code>stop</code></td>\n<td>Stop sequences</td>\n<td>Control response endings</td>\n</tr>\n</tbody></table>\n<h3>AI API Response Structure</h3>\n<p>A typical response (simplified):</p>\n<pre><code class=\"language-json\">{\n  &quot;id&quot;: &quot;chatcmpl-abc123&quot;,\n  &quot;object&quot;: &quot;chat.completion&quot;,\n  &quot;model&quot;: &quot;gpt-4o&quot;,\n  &quot;choices&quot;: [\n    {\n      &quot;message&quot;: {\n        &quot;role&quot;: &quot;assistant&quot;,\n        &quot;content&quot;: &quot;An API is like a waiter...&quot;\n      },\n      &quot;finish_reason&quot;: &quot;stop&quot;\n    }\n  ],\n  &quot;usage&quot;: {\n    &quot;prompt_tokens&quot;: 25,\n    &quot;completion_tokens&quot;: 150,\n    &quot;total_tokens&quot;: 175\n  }\n}\n</code></pre>\n<p><strong>What to look for:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Importance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>choices[0].message.content</code></td>\n<td>The actual AI response</td>\n</tr>\n<tr>\n<td><code>finish_reason</code></td>\n<td>Why it stopped (stop, length, error)</td>\n</tr>\n<tr>\n<td><code>usage</code></td>\n<td>Token counts for billing/limits</td>\n</tr>\n</tbody></table>\n<h3>Error Handling</h3>\n<p><strong>Error handling strategies:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>400 errors</td>\n<td>Fix your request</td>\n</tr>\n<tr>\n<td>401/403 errors</td>\n<td>Check API key</td>\n</tr>\n<tr>\n<td>429 errors</td>\n<td>Wait and retry, use backoff</td>\n</tr>\n<tr>\n<td>500/503 errors</td>\n<td>Retry, fall back to alternative</td>\n</tr>\n</tbody></table>\n<hr>\n<h2>HOW to Apply This</h2>\n<h3>Exercise: Read API Documentation</h3>\n<h3>API Terminology Quick Reference</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Endpoint</strong></td>\n<td>URL where API lives</td>\n</tr>\n<tr>\n<td><strong>Request</strong></td>\n<td>What you send to API</td>\n</tr>\n<tr>\n<td><strong>Response</strong></td>\n<td>What API sends back</td>\n</tr>\n<tr>\n<td><strong>Header</strong></td>\n<td>Metadata for request/response</td>\n</tr>\n<tr>\n<td><strong>Body</strong></td>\n<td>Main content of request/response</td>\n</tr>\n<tr>\n<td><strong>JSON</strong></td>\n<td>Data format for API communication</td>\n</tr>\n<tr>\n<td><strong>Status code</strong></td>\n<td>Numeric response indicator</td>\n</tr>\n<tr>\n<td><strong>Rate limit</strong></td>\n<td>How often you can call API</td>\n</tr>\n<tr>\n<td><strong>Token</strong></td>\n<td>Unit of text for AI processing</td>\n</tr>\n<tr>\n<td><strong>Latency</strong></td>\n<td>Time for API to respond</td>\n</tr>\n</tbody></table>\n<h3>Self-Check</h3>\n<hr>\n<h2>Up Next</h2>\n<p>In <strong>Module 3.3: Automation Platforms</strong>, you&#39;ll learn how to connect AI APIs to other tools using no-code automation platforms—building workflows that run without manual intervention.</p>\n",
      "sections": [
        {
          "id": "why-this-matters",
          "title": "WHY This Matters",
          "type": "why",
          "content": "Every AI application—from ChatGPT to custom enterprise tools—runs on APIs. Understanding APIs is the bridge between using off-the-shelf tools and building custom solutions. You need API knowledge to:\n\n- **Configure integrations** that automation tools provide\n- **Debug problems** when AI workflows break\n- **Evaluate vendors** and understand their capabilities\n- **Collaborate with developers** effectively\n- **Build more sophisticated** AI applications\n\nYou don't need to write code, but you need to speak the language.\n\n---",
          "htmlContent": "<p>Every AI application—from ChatGPT to custom enterprise tools—runs on APIs. Understanding APIs is the bridge between using off-the-shelf tools and building custom solutions. You need API knowledge to:</p>\n<ul>\n<li><strong>Configure integrations</strong> that automation tools provide</li>\n<li><strong>Debug problems</strong> when AI workflows break</li>\n<li><strong>Evaluate vendors</strong> and understand their capabilities</li>\n<li><strong>Collaborate with developers</strong> effectively</li>\n<li><strong>Build more sophisticated</strong> AI applications</li>\n</ul>\n<p>You don&#39;t need to write code, but you need to speak the language.</p>\n<hr>\n"
        },
        {
          "id": "what-you-need-to-know",
          "title": "WHAT You Need to Know",
          "type": "what",
          "content": "### What is an API?\n\n### The Request-Response Cycle\n\n```\n┌─────────────┐                  ┌─────────────┐\n│   CLIENT    │   ──Request──>   │   SERVER    │\n│  (your app) │                  │  (AI API)   │\n│             │   <──Response──  │             │\n└─────────────┘                  └─────────────┘\n```\n\n**Request components:**\n\n| Component | Purpose | Example |\n|-----------|---------|---------|\n| **Endpoint** | URL to call | `https://api.openai.com/v1/chat/completions` |\n| **Method** | Type of action | `POST` (send data), `GET` (retrieve data) |\n| **Headers** | Metadata | Authentication, content type |\n| **Body** | The actual data | Your prompt, parameters |\n\n**Response components:**\n\n| Component | Purpose | Example |\n|-----------|---------|---------|\n| **Status code** | Success/failure | 200 (OK), 400 (bad request), 500 (server error) |\n| **Headers** | Response metadata | Rate limit info, content type |\n| **Body** | The actual data | AI response, usage stats |\n\n### Authentication\n\n**Authentication patterns:**\n\n| Pattern | Usage | Example |\n|---------|-------|---------|\n| **API Key** | Most common | OpenAI, Anthropic, Google |\n| **Bearer Token** | OAuth flows | Google Cloud, Microsoft |\n| **Basic Auth** | Legacy systems | Username + password encoded |\n\n### AI API Request Structure\n\nA typical AI API request (simplified):\n\n```json\n{\n  \"model\": \"gpt-4o\",\n  \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n    {\"role\": \"user\", \"content\": \"Explain APIs simply\"}\n  ],\n  \"temperature\": 0.7,\n  \"max_tokens\": 500\n}\n```\n\n**Key parameters:**\n\n| Parameter | Purpose | Impact |\n|-----------|---------|--------|\n| `model` | Which AI model | Capability, cost, speed |\n| `messages` | Conversation history | Context for response |\n| `temperature` | Randomness (0-1) | Creativity vs. consistency |\n| `max_tokens` | Response length limit | Cost, completeness |\n| `top_p` | Probability threshold | Alternative to temperature |\n| `stop` | Stop sequences | Control response endings |\n\n### AI API Response Structure\n\nA typical response (simplified):\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"model\": \"gpt-4o\",\n  \"choices\": [\n    {\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"An API is like a waiter...\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 25,\n    \"completion_tokens\": 150,\n    \"total_tokens\": 175\n  }\n}\n```\n\n**What to look for:**\n\n| Field | Importance |\n|-------|------------|\n| `choices[0].message.content` | The actual AI response |\n| `finish_reason` | Why it stopped (stop, length, error) |\n| `usage` | Token counts for billing/limits |\n\n### Error Handling\n\n**Error handling strategies:**\n\n| Error Type | Strategy |\n|------------|----------|\n| 400 errors | Fix your request |\n| 401/403 errors | Check API key |\n| 429 errors | Wait and retry, use backoff |\n| 500/503 errors | Retry, fall back to alternative |\n\n---",
          "htmlContent": "<h3>What is an API?</h3>\n<h3>The Request-Response Cycle</h3>\n<pre><code>┌─────────────┐                  ┌─────────────┐\n│   CLIENT    │   ──Request──&gt;   │   SERVER    │\n│  (your app) │                  │  (AI API)   │\n│             │   &lt;──Response──  │             │\n└─────────────┘                  └─────────────┘\n</code></pre>\n<p><strong>Request components:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Purpose</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Endpoint</strong></td>\n<td>URL to call</td>\n<td><code>https://api.openai.com/v1/chat/completions</code></td>\n</tr>\n<tr>\n<td><strong>Method</strong></td>\n<td>Type of action</td>\n<td><code>POST</code> (send data), <code>GET</code> (retrieve data)</td>\n</tr>\n<tr>\n<td><strong>Headers</strong></td>\n<td>Metadata</td>\n<td>Authentication, content type</td>\n</tr>\n<tr>\n<td><strong>Body</strong></td>\n<td>The actual data</td>\n<td>Your prompt, parameters</td>\n</tr>\n</tbody></table>\n<p><strong>Response components:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Purpose</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Status code</strong></td>\n<td>Success/failure</td>\n<td>200 (OK), 400 (bad request), 500 (server error)</td>\n</tr>\n<tr>\n<td><strong>Headers</strong></td>\n<td>Response metadata</td>\n<td>Rate limit info, content type</td>\n</tr>\n<tr>\n<td><strong>Body</strong></td>\n<td>The actual data</td>\n<td>AI response, usage stats</td>\n</tr>\n</tbody></table>\n<h3>Authentication</h3>\n<p><strong>Authentication patterns:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Pattern</th>\n<th>Usage</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>API Key</strong></td>\n<td>Most common</td>\n<td>OpenAI, Anthropic, Google</td>\n</tr>\n<tr>\n<td><strong>Bearer Token</strong></td>\n<td>OAuth flows</td>\n<td>Google Cloud, Microsoft</td>\n</tr>\n<tr>\n<td><strong>Basic Auth</strong></td>\n<td>Legacy systems</td>\n<td>Username + password encoded</td>\n</tr>\n</tbody></table>\n<h3>AI API Request Structure</h3>\n<p>A typical AI API request (simplified):</p>\n<pre><code class=\"language-json\">{\n  &quot;model&quot;: &quot;gpt-4o&quot;,\n  &quot;messages&quot;: [\n    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant&quot;},\n    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Explain APIs simply&quot;}\n  ],\n  &quot;temperature&quot;: 0.7,\n  &quot;max_tokens&quot;: 500\n}\n</code></pre>\n<p><strong>Key parameters:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Parameter</th>\n<th>Purpose</th>\n<th>Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>model</code></td>\n<td>Which AI model</td>\n<td>Capability, cost, speed</td>\n</tr>\n<tr>\n<td><code>messages</code></td>\n<td>Conversation history</td>\n<td>Context for response</td>\n</tr>\n<tr>\n<td><code>temperature</code></td>\n<td>Randomness (0-1)</td>\n<td>Creativity vs. consistency</td>\n</tr>\n<tr>\n<td><code>max_tokens</code></td>\n<td>Response length limit</td>\n<td>Cost, completeness</td>\n</tr>\n<tr>\n<td><code>top_p</code></td>\n<td>Probability threshold</td>\n<td>Alternative to temperature</td>\n</tr>\n<tr>\n<td><code>stop</code></td>\n<td>Stop sequences</td>\n<td>Control response endings</td>\n</tr>\n</tbody></table>\n<h3>AI API Response Structure</h3>\n<p>A typical response (simplified):</p>\n<pre><code class=\"language-json\">{\n  &quot;id&quot;: &quot;chatcmpl-abc123&quot;,\n  &quot;object&quot;: &quot;chat.completion&quot;,\n  &quot;model&quot;: &quot;gpt-4o&quot;,\n  &quot;choices&quot;: [\n    {\n      &quot;message&quot;: {\n        &quot;role&quot;: &quot;assistant&quot;,\n        &quot;content&quot;: &quot;An API is like a waiter...&quot;\n      },\n      &quot;finish_reason&quot;: &quot;stop&quot;\n    }\n  ],\n  &quot;usage&quot;: {\n    &quot;prompt_tokens&quot;: 25,\n    &quot;completion_tokens&quot;: 150,\n    &quot;total_tokens&quot;: 175\n  }\n}\n</code></pre>\n<p><strong>What to look for:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Importance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>choices[0].message.content</code></td>\n<td>The actual AI response</td>\n</tr>\n<tr>\n<td><code>finish_reason</code></td>\n<td>Why it stopped (stop, length, error)</td>\n</tr>\n<tr>\n<td><code>usage</code></td>\n<td>Token counts for billing/limits</td>\n</tr>\n</tbody></table>\n<h3>Error Handling</h3>\n<p><strong>Error handling strategies:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>400 errors</td>\n<td>Fix your request</td>\n</tr>\n<tr>\n<td>401/403 errors</td>\n<td>Check API key</td>\n</tr>\n<tr>\n<td>429 errors</td>\n<td>Wait and retry, use backoff</td>\n</tr>\n<tr>\n<td>500/503 errors</td>\n<td>Retry, fall back to alternative</td>\n</tr>\n</tbody></table>\n<hr>\n"
        },
        {
          "id": "how-to-apply-this",
          "title": "HOW to Apply This",
          "type": "how",
          "content": "### Exercise: Read API Documentation\n\n### API Terminology Quick Reference\n\n| Term | Definition |\n|------|------------|\n| **Endpoint** | URL where API lives |\n| **Request** | What you send to API |\n| **Response** | What API sends back |\n| **Header** | Metadata for request/response |\n| **Body** | Main content of request/response |\n| **JSON** | Data format for API communication |\n| **Status code** | Numeric response indicator |\n| **Rate limit** | How often you can call API |\n| **Token** | Unit of text for AI processing |\n| **Latency** | Time for API to respond |\n\n### Self-Check\n\n---",
          "htmlContent": "<h3>Exercise: Read API Documentation</h3>\n<h3>API Terminology Quick Reference</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Endpoint</strong></td>\n<td>URL where API lives</td>\n</tr>\n<tr>\n<td><strong>Request</strong></td>\n<td>What you send to API</td>\n</tr>\n<tr>\n<td><strong>Response</strong></td>\n<td>What API sends back</td>\n</tr>\n<tr>\n<td><strong>Header</strong></td>\n<td>Metadata for request/response</td>\n</tr>\n<tr>\n<td><strong>Body</strong></td>\n<td>Main content of request/response</td>\n</tr>\n<tr>\n<td><strong>JSON</strong></td>\n<td>Data format for API communication</td>\n</tr>\n<tr>\n<td><strong>Status code</strong></td>\n<td>Numeric response indicator</td>\n</tr>\n<tr>\n<td><strong>Rate limit</strong></td>\n<td>How often you can call API</td>\n</tr>\n<tr>\n<td><strong>Token</strong></td>\n<td>Unit of text for AI processing</td>\n</tr>\n<tr>\n<td><strong>Latency</strong></td>\n<td>Time for API to respond</td>\n</tr>\n</tbody></table>\n<h3>Self-Check</h3>\n<hr>\n"
        },
        {
          "id": "up-next",
          "title": "Up Next",
          "type": "generic",
          "content": "In **Module 3.3: Automation Platforms**, you'll learn how to connect AI APIs to other tools using no-code automation platforms—building workflows that run without manual intervention.",
          "htmlContent": "<p>In <strong>Module 3.3: Automation Platforms</strong>, you&#39;ll learn how to connect AI APIs to other tools using no-code automation platforms—building workflows that run without manual intervention.</p>\n"
        }
      ],
      "concepts": [
        {
          "id": "api",
          "term": "api",
          "definition": "An **API (Application Programming Interface)** is a structured way for software to communicate. Think of it like a restaurant:\n\n- **Menu**: The API documentation (what you can order)\n- **Order**: Your API request (what you want)\n- **Kitchen**: The server processing your request\n- **Food**: The API response (what you get back)\n\nYou don't need to know how the kitchen works—you just need to know how to read the menu and place an order.",
          "htmlDefinition": "<p>An <strong>API (Application Programming Interface)</strong> is a structured way for software to communicate. Think of it like a restaurant:</p>\n<ul>\n<li><strong>Menu</strong>: The API documentation (what you can order)</li>\n<li><strong>Order</strong>: Your API request (what you want)</li>\n<li><strong>Kitchen</strong>: The server processing your request</li>\n<li><strong>Food</strong>: The API response (what you get back)</li>\n</ul>\n<p>You don&#39;t need to know how the kitchen works—you just need to know how to read the menu and place an order.</p>\n"
        },
        {
          "id": "api-authentication",
          "term": "api authentication",
          "definition": "**API authentication** proves you're authorized to use the service. Most AI APIs use API keys:\n\n```\nAuthorization: Bearer sk-your-api-key-here\n```\n\n**Key security rules:**\n- Never share your API key publicly\n- Don't commit keys to code repositories\n- Use environment variables or secret managers\n- Rotate keys periodically\n- Monitor for unauthorized usage",
          "htmlDefinition": "<p><strong>API authentication</strong> proves you&#39;re authorized to use the service. Most AI APIs use API keys:</p>\n<pre><code>Authorization: Bearer sk-your-api-key-here\n</code></pre>\n<p><strong>Key security rules:</strong></p>\n<ul>\n<li>Never share your API key publicly</li>\n<li>Don&#39;t commit keys to code repositories</li>\n<li>Use environment variables or secret managers</li>\n<li>Rotate keys periodically</li>\n<li>Monitor for unauthorized usage</li>\n</ul>\n"
        },
        {
          "id": "api-errors",
          "term": "api errors",
          "definition": "APIs communicate problems through status codes and error messages:\n\n| Code | Meaning | Common Cause |\n|------|---------|--------------|\n| 400 | Bad Request | Malformed input |\n| 401 | Unauthorized | Invalid API key |\n| 403 | Forbidden | Key lacks permission |\n| 404 | Not Found | Wrong endpoint |\n| 429 | Rate Limited | Too many requests |\n| 500 | Server Error | Provider issue |\n| 503 | Service Unavailable | Temporary outage |",
          "htmlDefinition": "<p>APIs communicate problems through status codes and error messages:</p>\n<table>\n<thead>\n<tr>\n<th>Code</th>\n<th>Meaning</th>\n<th>Common Cause</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>400</td>\n<td>Bad Request</td>\n<td>Malformed input</td>\n</tr>\n<tr>\n<td>401</td>\n<td>Unauthorized</td>\n<td>Invalid API key</td>\n</tr>\n<tr>\n<td>403</td>\n<td>Forbidden</td>\n<td>Key lacks permission</td>\n</tr>\n<tr>\n<td>404</td>\n<td>Not Found</td>\n<td>Wrong endpoint</td>\n</tr>\n<tr>\n<td>429</td>\n<td>Rate Limited</td>\n<td>Too many requests</td>\n</tr>\n<tr>\n<td>500</td>\n<td>Server Error</td>\n<td>Provider issue</td>\n</tr>\n<tr>\n<td>503</td>\n<td>Service Unavailable</td>\n<td>Temporary outage</td>\n</tr>\n</tbody></table>\n"
        }
      ],
      "exercises": [
        {
          "id": "api-documentation",
          "title": "Task",
          "instructions": "Review the documentation for a major AI API and answer these questions.\n\n**Choose one:**\n- [OpenAI API Documentation](https://platform.openai.com/docs)\n- [Anthropic API Documentation](https://docs.anthropic.com)\n- [Google Gemini API Documentation](https://ai.google.dev/docs)\n\n**Answer:**\n1. What is the endpoint URL for chat completions?\n2. How is authentication handled?\n3. What parameters are required vs. optional?\n4. What models are available and how do they differ?\n5. What are the rate limits?\n6. How is usage billed?\n7. What do the error codes mean?",
          "htmlInstructions": "<p>Review the documentation for a major AI API and answer these questions.</p>\n<p><strong>Choose one:</strong></p>\n<ul>\n<li><a href=\"https://platform.openai.com/docs\">OpenAI API Documentation</a></li>\n<li><a href=\"https://docs.anthropic.com\">Anthropic API Documentation</a></li>\n<li><a href=\"https://ai.google.dev/docs\">Google Gemini API Documentation</a></li>\n</ul>\n<p><strong>Answer:</strong></p>\n<ol>\n<li>What is the endpoint URL for chat completions?</li>\n<li>How is authentication handled?</li>\n<li>What parameters are required vs. optional?</li>\n<li>What models are available and how do they differ?</li>\n<li>What are the rate limits?</li>\n<li>How is usage billed?</li>\n<li>What do the error codes mean?</li>\n</ol>\n"
        }
      ],
      "checklists": [
        {
          "id": "module-3.2-complete",
          "items": [
            {
              "id": "module-3.2-complete-0",
              "text": "I understand what an API is and why it matters",
              "completed": false
            },
            {
              "id": "module-3.2-complete-1",
              "text": "I can explain the request-response cycle",
              "completed": false
            },
            {
              "id": "module-3.2-complete-2",
              "text": "I know how API authentication works",
              "completed": false
            },
            {
              "id": "module-3.2-complete-3",
              "text": "I can interpret AI API request and response structures",
              "completed": false
            },
            {
              "id": "module-3.2-complete-4",
              "text": "I understand common error codes and handling strategies",
              "completed": false
            }
          ]
        }
      ]
    },
    {
      "id": "3.3-automation-platforms",
      "slug": "3.3-automation-platforms",
      "title": "Automation Platforms",
      "phase": 3,
      "module": 3,
      "phaseId": "phase-3",
      "estimatedMinutes": 16,
      "bloomLevel": "apply",
      "content": "# Automation Platforms\n\n## WHY This Matters\n\nAI is most powerful when it's connected to your other tools. Automation platforms are the glue—they let you:\n\n- **Trigger AI workflows** from events (new email, form submission, schedule)\n- **Connect AI outputs** to other systems (CRM, email, spreadsheets)\n- **Build complex logic** without coding\n- **Scale operations** beyond what manual work allows\n\nThe combination of AI + automation creates systems that work while you sleep.\n\n---\n\n## WHAT You Need to Know\n\n### The Automation Platform Landscape\n\n| Platform | Strengths | Best For |\n|----------|-----------|----------|\n| **Zapier** | Easiest, huge app library | Simple workflows, beginners |\n| **Make (Integromat)** | Visual, flexible, powerful | Complex logic, branching |\n| **n8n** | Open source, self-hostable | Technical teams, privacy needs |\n| **Power Automate** | Microsoft ecosystem | Microsoft-heavy organizations |\n| **Workato** | Enterprise features | Large organizations, governance |\n\n### Core Concepts\n\n**Workflow structure:**\n\n```\n[TRIGGER] → [ACTION 1] → [ACTION 2] → [ACTION N]\n              │              │\n              ↓              ↓\n        AI Processing    Output to tool\n```\n\n### AI Integration Patterns\n\n**Pattern 1: AI Processing Step**\n```\nTrigger → Get Data → Send to AI → Use AI Output → End\n```\nExample: New support email → Extract email content → AI categorizes & drafts response → Create ticket with draft\n\n**Pattern 2: AI Decision Branch**\n```\nTrigger → AI Analyzes → Branch based on AI output\n                        ├─> Path A (if condition 1)\n                        ├─> Path B (if condition 2)\n                        └─> Path C (default)\n```\nExample: New lead form → AI scores lead quality → High score → Sales team | Low score → Nurture sequence\n\n**Pattern 3: AI Enhancement Loop**\n```\nTrigger → Get records → For each record:\n                        ├─> Send to AI\n                        ├─> Process response\n                        └─> Update record\n          → End\n```\nExample: Daily schedule → Get all pending reviews → AI analyzes each → Update with insights\n\n### Building Blocks\n\n**Data manipulation:**\n- Parse JSON\n- Format text\n- Transform dates\n- Filter arrays\n- Map fields\n\n**Logic:**\n- If/then conditions\n- Loops (for each)\n- Delays\n- Error paths\n\n**AI-specific:**\n- HTTP request (to call AI APIs)\n- Built-in AI steps (Zapier AI, Make AI)\n- Custom AI actions (GPT, Claude integrations)\n\n### Configuration Best Practices\n\n---\n\n## HOW to Apply This\n\n### Exercise: Design an AI Automation\n\n### Workflow Blueprint Template\n\n```\nWORKFLOW: [Name]\nTRIGGER: [What starts it]\n\nSTEPS:\n1. [Step name]\n   - Input: [What this step receives]\n   - Action: [What it does]\n   - Output: [What it produces]\n\n2. [Step name]\n   - Input: [Uses output from step 1]\n   - Action: [AI processing - include prompt]\n   - Output: [Structured AI response]\n\n3. [Step name]\n   - Input: [Uses output from step 2]\n   - Action: [What it does with AI output]\n   - Output: [Final result]\n\nERROR HANDLING:\n- If step [N] fails: [What happens]\n- Notification: [Who gets alerted]\n```\n\n### Common Integration Recipes\n\n| Use Case | Trigger | AI Step | Output |\n|----------|---------|---------|--------|\n| Email triage | New email | Categorize & prioritize | Label + notify |\n| Content creation | Schedule | Generate content | Post to platforms |\n| Data enrichment | New CRM record | Research & summarize | Update record |\n| Support automation | New ticket | Draft response | Queue for review |\n| Feedback analysis | New survey | Analyze sentiment | Dashboard update |\n| Document processing | File upload | Extract data | Spreadsheet row |\n\n---\n\n### Multi-Agent Framework Landscape\n\nWhen workflows exceed what Zapier/Make can handle, dedicated agent frameworks take over.\n\n**72% of enterprise AI projects now use multi-agent architectures** (up from 23% in 2024). Understanding this landscape is essential.\n\n#### The Big Three (2025)\n\n| Framework | Architecture | Best For | Learning Curve |\n|-----------|-------------|----------|----------------|\n| **LangGraph** | Graph-based workflows | Complex branching, state management | High |\n| **CrewAI** | Role-based teams | Defined job roles, delegation | Medium |\n| **AutoGen** | Conversational agents | Dynamic collaboration, research | Medium |\n\n#### LangGraph (LangChain ecosystem)\n\n**Architecture:** Graph nodes = agent steps, edges = conditional transitions\n\n**Best for:**\n- Complex decision trees with many branches\n- Workflows needing precise state management\n- Parallel processing with convergence points\n- Fine-grained control over agent behavior\n\n**Pattern:** \"If customer sentiment is negative AND order > $500, route to manager agent; otherwise, route to support agent\"\n\n```\n         ┌─────────────┐\n         │   START     │\n         └──────┬──────┘\n                │\n         ┌──────▼──────┐\n         │  Analyze    │\n         │  Request    │\n         └──────┬──────┘\n                │\n        ┌───────┼───────┐\n        ▼       ▼       ▼\n    [Agent A] [Agent B] [Agent C]\n        │       │       │\n        └───────┼───────┘\n                ▼\n         ┌──────────────┐\n         │   Combine    │\n         │   Results    │\n         └──────────────┘\n```\n\n#### CrewAI\n\n**Architecture:** Crew = team of agents with defined roles, Tasks assigned to specific agents\n\n**Best for:**\n- Mimicking human team structures\n- Clear handoffs between specialists\n- Workflows where roles are well-defined\n- Business processes with established responsibilities\n\n**Pattern:** \"Researcher gathers information → Analyst evaluates → Writer drafts → Editor reviews\"\n\n```\nCREW: Content Production Team\n├── Researcher Agent (role: find information)\n├── Writer Agent (role: create drafts)\n├── Editor Agent (role: review and polish)\n└── Manager (coordinates, handles escalations)\n```\n\n#### AutoGen (Microsoft)\n\n**Architecture:** Agents converse to solve problems, dynamic role-taking based on context\n\n**Best for:**\n- Exploratory tasks where the path isn't clear\n- Research and brainstorming\n- Problems requiring back-and-forth reasoning\n- Collaborative problem-solving\n\n**Pattern:** \"Agents discuss the problem until they reach consensus or need human input\"\n\n#### Framework Selection Decision Tree\n\n#### The No-Code Alternative\n\n**Before reaching for frameworks, consider:**\n\n| Platform | Agent Capability | Best For |\n|----------|-----------------|----------|\n| **Zapier Central** | Natural language agent instructions | Business users, quick prototypes |\n| **Make AI** | Visual agent building with tools | Complex flows without code |\n| **n8n AI nodes** | Self-hosted agent workflows | Privacy-conscious, technical teams |\n\n**Reality check:** Most business problems don't need custom frameworks. Start with no-code, graduate to frameworks only when you hit limits.\n\n#### When to Graduate to Frameworks\n\nYou need a framework when:\n- No-code tools can't express your logic\n- You need custom tool integrations\n- Performance or cost requires optimization\n- You need fine-grained control over agent behavior\n- Compliance requires audit trails no-code can't provide\n\nYou DON'T need a framework when:\n- The workflow is mostly linear\n- Existing integrations cover your tools\n- The team can't maintain code\n- Time-to-value matters more than optimization\n\n### Debugging Checklist\n\n| Issue | Check |\n|-------|-------|\n| Workflow doesn't trigger | Is trigger configured correctly? Test data format? |\n| AI step fails | Check API key, rate limits, prompt format |\n| Wrong output | Verify data mapping, check AI response parsing |\n| Partial completion | Look for error in specific step, check conditions |\n| Performance slow | Too many steps? Optimize AI calls, batch where possible |\n\n### Self-Check\n\n---\n\n## Up Next\n\nIn **Module 3.4: Testing and Deployment**, you'll learn how to validate your AI applications work correctly and deploy them for real use.",
      "htmlContent": "<h1>Automation Platforms</h1>\n<h2>WHY This Matters</h2>\n<p>AI is most powerful when it&#39;s connected to your other tools. Automation platforms are the glue—they let you:</p>\n<ul>\n<li><strong>Trigger AI workflows</strong> from events (new email, form submission, schedule)</li>\n<li><strong>Connect AI outputs</strong> to other systems (CRM, email, spreadsheets)</li>\n<li><strong>Build complex logic</strong> without coding</li>\n<li><strong>Scale operations</strong> beyond what manual work allows</li>\n</ul>\n<p>The combination of AI + automation creates systems that work while you sleep.</p>\n<hr>\n<h2>WHAT You Need to Know</h2>\n<h3>The Automation Platform Landscape</h3>\n<table>\n<thead>\n<tr>\n<th>Platform</th>\n<th>Strengths</th>\n<th>Best For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Zapier</strong></td>\n<td>Easiest, huge app library</td>\n<td>Simple workflows, beginners</td>\n</tr>\n<tr>\n<td><strong>Make (Integromat)</strong></td>\n<td>Visual, flexible, powerful</td>\n<td>Complex logic, branching</td>\n</tr>\n<tr>\n<td><strong>n8n</strong></td>\n<td>Open source, self-hostable</td>\n<td>Technical teams, privacy needs</td>\n</tr>\n<tr>\n<td><strong>Power Automate</strong></td>\n<td>Microsoft ecosystem</td>\n<td>Microsoft-heavy organizations</td>\n</tr>\n<tr>\n<td><strong>Workato</strong></td>\n<td>Enterprise features</td>\n<td>Large organizations, governance</td>\n</tr>\n</tbody></table>\n<h3>Core Concepts</h3>\n<p><strong>Workflow structure:</strong></p>\n<pre><code>[TRIGGER] → [ACTION 1] → [ACTION 2] → [ACTION N]\n              │              │\n              ↓              ↓\n        AI Processing    Output to tool\n</code></pre>\n<h3>AI Integration Patterns</h3>\n<p><strong>Pattern 1: AI Processing Step</strong></p>\n<pre><code>Trigger → Get Data → Send to AI → Use AI Output → End\n</code></pre>\n<p>Example: New support email → Extract email content → AI categorizes &amp; drafts response → Create ticket with draft</p>\n<p><strong>Pattern 2: AI Decision Branch</strong></p>\n<pre><code>Trigger → AI Analyzes → Branch based on AI output\n                        ├─&gt; Path A (if condition 1)\n                        ├─&gt; Path B (if condition 2)\n                        └─&gt; Path C (default)\n</code></pre>\n<p>Example: New lead form → AI scores lead quality → High score → Sales team | Low score → Nurture sequence</p>\n<p><strong>Pattern 3: AI Enhancement Loop</strong></p>\n<pre><code>Trigger → Get records → For each record:\n                        ├─&gt; Send to AI\n                        ├─&gt; Process response\n                        └─&gt; Update record\n          → End\n</code></pre>\n<p>Example: Daily schedule → Get all pending reviews → AI analyzes each → Update with insights</p>\n<h3>Building Blocks</h3>\n<p><strong>Data manipulation:</strong></p>\n<ul>\n<li>Parse JSON</li>\n<li>Format text</li>\n<li>Transform dates</li>\n<li>Filter arrays</li>\n<li>Map fields</li>\n</ul>\n<p><strong>Logic:</strong></p>\n<ul>\n<li>If/then conditions</li>\n<li>Loops (for each)</li>\n<li>Delays</li>\n<li>Error paths</li>\n</ul>\n<p><strong>AI-specific:</strong></p>\n<ul>\n<li>HTTP request (to call AI APIs)</li>\n<li>Built-in AI steps (Zapier AI, Make AI)</li>\n<li>Custom AI actions (GPT, Claude integrations)</li>\n</ul>\n<h3>Configuration Best Practices</h3>\n<hr>\n<h2>HOW to Apply This</h2>\n<h3>Exercise: Design an AI Automation</h3>\n<h3>Workflow Blueprint Template</h3>\n<pre><code>WORKFLOW: [Name]\nTRIGGER: [What starts it]\n\nSTEPS:\n1. [Step name]\n   - Input: [What this step receives]\n   - Action: [What it does]\n   - Output: [What it produces]\n\n2. [Step name]\n   - Input: [Uses output from step 1]\n   - Action: [AI processing - include prompt]\n   - Output: [Structured AI response]\n\n3. [Step name]\n   - Input: [Uses output from step 2]\n   - Action: [What it does with AI output]\n   - Output: [Final result]\n\nERROR HANDLING:\n- If step [N] fails: [What happens]\n- Notification: [Who gets alerted]\n</code></pre>\n<h3>Common Integration Recipes</h3>\n<table>\n<thead>\n<tr>\n<th>Use Case</th>\n<th>Trigger</th>\n<th>AI Step</th>\n<th>Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Email triage</td>\n<td>New email</td>\n<td>Categorize &amp; prioritize</td>\n<td>Label + notify</td>\n</tr>\n<tr>\n<td>Content creation</td>\n<td>Schedule</td>\n<td>Generate content</td>\n<td>Post to platforms</td>\n</tr>\n<tr>\n<td>Data enrichment</td>\n<td>New CRM record</td>\n<td>Research &amp; summarize</td>\n<td>Update record</td>\n</tr>\n<tr>\n<td>Support automation</td>\n<td>New ticket</td>\n<td>Draft response</td>\n<td>Queue for review</td>\n</tr>\n<tr>\n<td>Feedback analysis</td>\n<td>New survey</td>\n<td>Analyze sentiment</td>\n<td>Dashboard update</td>\n</tr>\n<tr>\n<td>Document processing</td>\n<td>File upload</td>\n<td>Extract data</td>\n<td>Spreadsheet row</td>\n</tr>\n</tbody></table>\n<hr>\n<h3>Multi-Agent Framework Landscape</h3>\n<p>When workflows exceed what Zapier/Make can handle, dedicated agent frameworks take over.</p>\n<p><strong>72% of enterprise AI projects now use multi-agent architectures</strong> (up from 23% in 2024). Understanding this landscape is essential.</p>\n<h4>The Big Three (2025)</h4>\n<table>\n<thead>\n<tr>\n<th>Framework</th>\n<th>Architecture</th>\n<th>Best For</th>\n<th>Learning Curve</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>LangGraph</strong></td>\n<td>Graph-based workflows</td>\n<td>Complex branching, state management</td>\n<td>High</td>\n</tr>\n<tr>\n<td><strong>CrewAI</strong></td>\n<td>Role-based teams</td>\n<td>Defined job roles, delegation</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td><strong>AutoGen</strong></td>\n<td>Conversational agents</td>\n<td>Dynamic collaboration, research</td>\n<td>Medium</td>\n</tr>\n</tbody></table>\n<h4>LangGraph (LangChain ecosystem)</h4>\n<p><strong>Architecture:</strong> Graph nodes = agent steps, edges = conditional transitions</p>\n<p><strong>Best for:</strong></p>\n<ul>\n<li>Complex decision trees with many branches</li>\n<li>Workflows needing precise state management</li>\n<li>Parallel processing with convergence points</li>\n<li>Fine-grained control over agent behavior</li>\n</ul>\n<p><strong>Pattern:</strong> &quot;If customer sentiment is negative AND order &gt; $500, route to manager agent; otherwise, route to support agent&quot;</p>\n<pre><code>         ┌─────────────┐\n         │   START     │\n         └──────┬──────┘\n                │\n         ┌──────▼──────┐\n         │  Analyze    │\n         │  Request    │\n         └──────┬──────┘\n                │\n        ┌───────┼───────┐\n        ▼       ▼       ▼\n    [Agent A] [Agent B] [Agent C]\n        │       │       │\n        └───────┼───────┘\n                ▼\n         ┌──────────────┐\n         │   Combine    │\n         │   Results    │\n         └──────────────┘\n</code></pre>\n<h4>CrewAI</h4>\n<p><strong>Architecture:</strong> Crew = team of agents with defined roles, Tasks assigned to specific agents</p>\n<p><strong>Best for:</strong></p>\n<ul>\n<li>Mimicking human team structures</li>\n<li>Clear handoffs between specialists</li>\n<li>Workflows where roles are well-defined</li>\n<li>Business processes with established responsibilities</li>\n</ul>\n<p><strong>Pattern:</strong> &quot;Researcher gathers information → Analyst evaluates → Writer drafts → Editor reviews&quot;</p>\n<pre><code>CREW: Content Production Team\n├── Researcher Agent (role: find information)\n├── Writer Agent (role: create drafts)\n├── Editor Agent (role: review and polish)\n└── Manager (coordinates, handles escalations)\n</code></pre>\n<h4>AutoGen (Microsoft)</h4>\n<p><strong>Architecture:</strong> Agents converse to solve problems, dynamic role-taking based on context</p>\n<p><strong>Best for:</strong></p>\n<ul>\n<li>Exploratory tasks where the path isn&#39;t clear</li>\n<li>Research and brainstorming</li>\n<li>Problems requiring back-and-forth reasoning</li>\n<li>Collaborative problem-solving</li>\n</ul>\n<p><strong>Pattern:</strong> &quot;Agents discuss the problem until they reach consensus or need human input&quot;</p>\n<h4>Framework Selection Decision Tree</h4>\n<h4>The No-Code Alternative</h4>\n<p><strong>Before reaching for frameworks, consider:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Platform</th>\n<th>Agent Capability</th>\n<th>Best For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Zapier Central</strong></td>\n<td>Natural language agent instructions</td>\n<td>Business users, quick prototypes</td>\n</tr>\n<tr>\n<td><strong>Make AI</strong></td>\n<td>Visual agent building with tools</td>\n<td>Complex flows without code</td>\n</tr>\n<tr>\n<td><strong>n8n AI nodes</strong></td>\n<td>Self-hosted agent workflows</td>\n<td>Privacy-conscious, technical teams</td>\n</tr>\n</tbody></table>\n<p><strong>Reality check:</strong> Most business problems don&#39;t need custom frameworks. Start with no-code, graduate to frameworks only when you hit limits.</p>\n<h4>When to Graduate to Frameworks</h4>\n<p>You need a framework when:</p>\n<ul>\n<li>No-code tools can&#39;t express your logic</li>\n<li>You need custom tool integrations</li>\n<li>Performance or cost requires optimization</li>\n<li>You need fine-grained control over agent behavior</li>\n<li>Compliance requires audit trails no-code can&#39;t provide</li>\n</ul>\n<p>You DON&#39;T need a framework when:</p>\n<ul>\n<li>The workflow is mostly linear</li>\n<li>Existing integrations cover your tools</li>\n<li>The team can&#39;t maintain code</li>\n<li>Time-to-value matters more than optimization</li>\n</ul>\n<h3>Debugging Checklist</h3>\n<table>\n<thead>\n<tr>\n<th>Issue</th>\n<th>Check</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Workflow doesn&#39;t trigger</td>\n<td>Is trigger configured correctly? Test data format?</td>\n</tr>\n<tr>\n<td>AI step fails</td>\n<td>Check API key, rate limits, prompt format</td>\n</tr>\n<tr>\n<td>Wrong output</td>\n<td>Verify data mapping, check AI response parsing</td>\n</tr>\n<tr>\n<td>Partial completion</td>\n<td>Look for error in specific step, check conditions</td>\n</tr>\n<tr>\n<td>Performance slow</td>\n<td>Too many steps? Optimize AI calls, batch where possible</td>\n</tr>\n</tbody></table>\n<h3>Self-Check</h3>\n<hr>\n<h2>Up Next</h2>\n<p>In <strong>Module 3.4: Testing and Deployment</strong>, you&#39;ll learn how to validate your AI applications work correctly and deploy them for real use.</p>\n",
      "sections": [
        {
          "id": "why-this-matters",
          "title": "WHY This Matters",
          "type": "why",
          "content": "AI is most powerful when it's connected to your other tools. Automation platforms are the glue—they let you:\n\n- **Trigger AI workflows** from events (new email, form submission, schedule)\n- **Connect AI outputs** to other systems (CRM, email, spreadsheets)\n- **Build complex logic** without coding\n- **Scale operations** beyond what manual work allows\n\nThe combination of AI + automation creates systems that work while you sleep.\n\n---",
          "htmlContent": "<p>AI is most powerful when it&#39;s connected to your other tools. Automation platforms are the glue—they let you:</p>\n<ul>\n<li><strong>Trigger AI workflows</strong> from events (new email, form submission, schedule)</li>\n<li><strong>Connect AI outputs</strong> to other systems (CRM, email, spreadsheets)</li>\n<li><strong>Build complex logic</strong> without coding</li>\n<li><strong>Scale operations</strong> beyond what manual work allows</li>\n</ul>\n<p>The combination of AI + automation creates systems that work while you sleep.</p>\n<hr>\n"
        },
        {
          "id": "what-you-need-to-know",
          "title": "WHAT You Need to Know",
          "type": "what",
          "content": "### The Automation Platform Landscape\n\n| Platform | Strengths | Best For |\n|----------|-----------|----------|\n| **Zapier** | Easiest, huge app library | Simple workflows, beginners |\n| **Make (Integromat)** | Visual, flexible, powerful | Complex logic, branching |\n| **n8n** | Open source, self-hostable | Technical teams, privacy needs |\n| **Power Automate** | Microsoft ecosystem | Microsoft-heavy organizations |\n| **Workato** | Enterprise features | Large organizations, governance |\n\n### Core Concepts\n\n**Workflow structure:**\n\n```\n[TRIGGER] → [ACTION 1] → [ACTION 2] → [ACTION N]\n              │              │\n              ↓              ↓\n        AI Processing    Output to tool\n```\n\n### AI Integration Patterns\n\n**Pattern 1: AI Processing Step**\n```\nTrigger → Get Data → Send to AI → Use AI Output → End\n```\nExample: New support email → Extract email content → AI categorizes & drafts response → Create ticket with draft\n\n**Pattern 2: AI Decision Branch**\n```\nTrigger → AI Analyzes → Branch based on AI output\n                        ├─> Path A (if condition 1)\n                        ├─> Path B (if condition 2)\n                        └─> Path C (default)\n```\nExample: New lead form → AI scores lead quality → High score → Sales team | Low score → Nurture sequence\n\n**Pattern 3: AI Enhancement Loop**\n```\nTrigger → Get records → For each record:\n                        ├─> Send to AI\n                        ├─> Process response\n                        └─> Update record\n          → End\n```\nExample: Daily schedule → Get all pending reviews → AI analyzes each → Update with insights\n\n### Building Blocks\n\n**Data manipulation:**\n- Parse JSON\n- Format text\n- Transform dates\n- Filter arrays\n- Map fields\n\n**Logic:**\n- If/then conditions\n- Loops (for each)\n- Delays\n- Error paths\n\n**AI-specific:**\n- HTTP request (to call AI APIs)\n- Built-in AI steps (Zapier AI, Make AI)\n- Custom AI actions (GPT, Claude integrations)\n\n### Configuration Best Practices\n\n---",
          "htmlContent": "<h3>The Automation Platform Landscape</h3>\n<table>\n<thead>\n<tr>\n<th>Platform</th>\n<th>Strengths</th>\n<th>Best For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Zapier</strong></td>\n<td>Easiest, huge app library</td>\n<td>Simple workflows, beginners</td>\n</tr>\n<tr>\n<td><strong>Make (Integromat)</strong></td>\n<td>Visual, flexible, powerful</td>\n<td>Complex logic, branching</td>\n</tr>\n<tr>\n<td><strong>n8n</strong></td>\n<td>Open source, self-hostable</td>\n<td>Technical teams, privacy needs</td>\n</tr>\n<tr>\n<td><strong>Power Automate</strong></td>\n<td>Microsoft ecosystem</td>\n<td>Microsoft-heavy organizations</td>\n</tr>\n<tr>\n<td><strong>Workato</strong></td>\n<td>Enterprise features</td>\n<td>Large organizations, governance</td>\n</tr>\n</tbody></table>\n<h3>Core Concepts</h3>\n<p><strong>Workflow structure:</strong></p>\n<pre><code>[TRIGGER] → [ACTION 1] → [ACTION 2] → [ACTION N]\n              │              │\n              ↓              ↓\n        AI Processing    Output to tool\n</code></pre>\n<h3>AI Integration Patterns</h3>\n<p><strong>Pattern 1: AI Processing Step</strong></p>\n<pre><code>Trigger → Get Data → Send to AI → Use AI Output → End\n</code></pre>\n<p>Example: New support email → Extract email content → AI categorizes &amp; drafts response → Create ticket with draft</p>\n<p><strong>Pattern 2: AI Decision Branch</strong></p>\n<pre><code>Trigger → AI Analyzes → Branch based on AI output\n                        ├─&gt; Path A (if condition 1)\n                        ├─&gt; Path B (if condition 2)\n                        └─&gt; Path C (default)\n</code></pre>\n<p>Example: New lead form → AI scores lead quality → High score → Sales team | Low score → Nurture sequence</p>\n<p><strong>Pattern 3: AI Enhancement Loop</strong></p>\n<pre><code>Trigger → Get records → For each record:\n                        ├─&gt; Send to AI\n                        ├─&gt; Process response\n                        └─&gt; Update record\n          → End\n</code></pre>\n<p>Example: Daily schedule → Get all pending reviews → AI analyzes each → Update with insights</p>\n<h3>Building Blocks</h3>\n<p><strong>Data manipulation:</strong></p>\n<ul>\n<li>Parse JSON</li>\n<li>Format text</li>\n<li>Transform dates</li>\n<li>Filter arrays</li>\n<li>Map fields</li>\n</ul>\n<p><strong>Logic:</strong></p>\n<ul>\n<li>If/then conditions</li>\n<li>Loops (for each)</li>\n<li>Delays</li>\n<li>Error paths</li>\n</ul>\n<p><strong>AI-specific:</strong></p>\n<ul>\n<li>HTTP request (to call AI APIs)</li>\n<li>Built-in AI steps (Zapier AI, Make AI)</li>\n<li>Custom AI actions (GPT, Claude integrations)</li>\n</ul>\n<h3>Configuration Best Practices</h3>\n<hr>\n"
        },
        {
          "id": "how-to-apply-this",
          "title": "HOW to Apply This",
          "type": "how",
          "content": "### Exercise: Design an AI Automation\n\n### Workflow Blueprint Template\n\n```\nWORKFLOW: [Name]\nTRIGGER: [What starts it]\n\nSTEPS:\n1. [Step name]\n   - Input: [What this step receives]\n   - Action: [What it does]\n   - Output: [What it produces]\n\n2. [Step name]\n   - Input: [Uses output from step 1]\n   - Action: [AI processing - include prompt]\n   - Output: [Structured AI response]\n\n3. [Step name]\n   - Input: [Uses output from step 2]\n   - Action: [What it does with AI output]\n   - Output: [Final result]\n\nERROR HANDLING:\n- If step [N] fails: [What happens]\n- Notification: [Who gets alerted]\n```\n\n### Common Integration Recipes\n\n| Use Case | Trigger | AI Step | Output |\n|----------|---------|---------|--------|\n| Email triage | New email | Categorize & prioritize | Label + notify |\n| Content creation | Schedule | Generate content | Post to platforms |\n| Data enrichment | New CRM record | Research & summarize | Update record |\n| Support automation | New ticket | Draft response | Queue for review |\n| Feedback analysis | New survey | Analyze sentiment | Dashboard update |\n| Document processing | File upload | Extract data | Spreadsheet row |\n\n---\n\n### Multi-Agent Framework Landscape\n\nWhen workflows exceed what Zapier/Make can handle, dedicated agent frameworks take over.\n\n**72% of enterprise AI projects now use multi-agent architectures** (up from 23% in 2024). Understanding this landscape is essential.\n\n#### The Big Three (2025)\n\n| Framework | Architecture | Best For | Learning Curve |\n|-----------|-------------|----------|----------------|\n| **LangGraph** | Graph-based workflows | Complex branching, state management | High |\n| **CrewAI** | Role-based teams | Defined job roles, delegation | Medium |\n| **AutoGen** | Conversational agents | Dynamic collaboration, research | Medium |\n\n#### LangGraph (LangChain ecosystem)\n\n**Architecture:** Graph nodes = agent steps, edges = conditional transitions\n\n**Best for:**\n- Complex decision trees with many branches\n- Workflows needing precise state management\n- Parallel processing with convergence points\n- Fine-grained control over agent behavior\n\n**Pattern:** \"If customer sentiment is negative AND order > $500, route to manager agent; otherwise, route to support agent\"\n\n```\n         ┌─────────────┐\n         │   START     │\n         └──────┬──────┘\n                │\n         ┌──────▼──────┐\n         │  Analyze    │\n         │  Request    │\n         └──────┬──────┘\n                │\n        ┌───────┼───────┐\n        ▼       ▼       ▼\n    [Agent A] [Agent B] [Agent C]\n        │       │       │\n        └───────┼───────┘\n                ▼\n         ┌──────────────┐\n         │   Combine    │\n         │   Results    │\n         └──────────────┘\n```\n\n#### CrewAI\n\n**Architecture:** Crew = team of agents with defined roles, Tasks assigned to specific agents\n\n**Best for:**\n- Mimicking human team structures\n- Clear handoffs between specialists\n- Workflows where roles are well-defined\n- Business processes with established responsibilities\n\n**Pattern:** \"Researcher gathers information → Analyst evaluates → Writer drafts → Editor reviews\"\n\n```\nCREW: Content Production Team\n├── Researcher Agent (role: find information)\n├── Writer Agent (role: create drafts)\n├── Editor Agent (role: review and polish)\n└── Manager (coordinates, handles escalations)\n```\n\n#### AutoGen (Microsoft)\n\n**Architecture:** Agents converse to solve problems, dynamic role-taking based on context\n\n**Best for:**\n- Exploratory tasks where the path isn't clear\n- Research and brainstorming\n- Problems requiring back-and-forth reasoning\n- Collaborative problem-solving\n\n**Pattern:** \"Agents discuss the problem until they reach consensus or need human input\"\n\n#### Framework Selection Decision Tree\n\n#### The No-Code Alternative\n\n**Before reaching for frameworks, consider:**\n\n| Platform | Agent Capability | Best For |\n|----------|-----------------|----------|\n| **Zapier Central** | Natural language agent instructions | Business users, quick prototypes |\n| **Make AI** | Visual agent building with tools | Complex flows without code |\n| **n8n AI nodes** | Self-hosted agent workflows | Privacy-conscious, technical teams |\n\n**Reality check:** Most business problems don't need custom frameworks. Start with no-code, graduate to frameworks only when you hit limits.\n\n#### When to Graduate to Frameworks\n\nYou need a framework when:\n- No-code tools can't express your logic\n- You need custom tool integrations\n- Performance or cost requires optimization\n- You need fine-grained control over agent behavior\n- Compliance requires audit trails no-code can't provide\n\nYou DON'T need a framework when:\n- The workflow is mostly linear\n- Existing integrations cover your tools\n- The team can't maintain code\n- Time-to-value matters more than optimization\n\n### Debugging Checklist\n\n| Issue | Check |\n|-------|-------|\n| Workflow doesn't trigger | Is trigger configured correctly? Test data format? |\n| AI step fails | Check API key, rate limits, prompt format |\n| Wrong output | Verify data mapping, check AI response parsing |\n| Partial completion | Look for error in specific step, check conditions |\n| Performance slow | Too many steps? Optimize AI calls, batch where possible |\n\n### Self-Check\n\n---",
          "htmlContent": "<h3>Exercise: Design an AI Automation</h3>\n<h3>Workflow Blueprint Template</h3>\n<pre><code>WORKFLOW: [Name]\nTRIGGER: [What starts it]\n\nSTEPS:\n1. [Step name]\n   - Input: [What this step receives]\n   - Action: [What it does]\n   - Output: [What it produces]\n\n2. [Step name]\n   - Input: [Uses output from step 1]\n   - Action: [AI processing - include prompt]\n   - Output: [Structured AI response]\n\n3. [Step name]\n   - Input: [Uses output from step 2]\n   - Action: [What it does with AI output]\n   - Output: [Final result]\n\nERROR HANDLING:\n- If step [N] fails: [What happens]\n- Notification: [Who gets alerted]\n</code></pre>\n<h3>Common Integration Recipes</h3>\n<table>\n<thead>\n<tr>\n<th>Use Case</th>\n<th>Trigger</th>\n<th>AI Step</th>\n<th>Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Email triage</td>\n<td>New email</td>\n<td>Categorize &amp; prioritize</td>\n<td>Label + notify</td>\n</tr>\n<tr>\n<td>Content creation</td>\n<td>Schedule</td>\n<td>Generate content</td>\n<td>Post to platforms</td>\n</tr>\n<tr>\n<td>Data enrichment</td>\n<td>New CRM record</td>\n<td>Research &amp; summarize</td>\n<td>Update record</td>\n</tr>\n<tr>\n<td>Support automation</td>\n<td>New ticket</td>\n<td>Draft response</td>\n<td>Queue for review</td>\n</tr>\n<tr>\n<td>Feedback analysis</td>\n<td>New survey</td>\n<td>Analyze sentiment</td>\n<td>Dashboard update</td>\n</tr>\n<tr>\n<td>Document processing</td>\n<td>File upload</td>\n<td>Extract data</td>\n<td>Spreadsheet row</td>\n</tr>\n</tbody></table>\n<hr>\n<h3>Multi-Agent Framework Landscape</h3>\n<p>When workflows exceed what Zapier/Make can handle, dedicated agent frameworks take over.</p>\n<p><strong>72% of enterprise AI projects now use multi-agent architectures</strong> (up from 23% in 2024). Understanding this landscape is essential.</p>\n<h4>The Big Three (2025)</h4>\n<table>\n<thead>\n<tr>\n<th>Framework</th>\n<th>Architecture</th>\n<th>Best For</th>\n<th>Learning Curve</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>LangGraph</strong></td>\n<td>Graph-based workflows</td>\n<td>Complex branching, state management</td>\n<td>High</td>\n</tr>\n<tr>\n<td><strong>CrewAI</strong></td>\n<td>Role-based teams</td>\n<td>Defined job roles, delegation</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td><strong>AutoGen</strong></td>\n<td>Conversational agents</td>\n<td>Dynamic collaboration, research</td>\n<td>Medium</td>\n</tr>\n</tbody></table>\n<h4>LangGraph (LangChain ecosystem)</h4>\n<p><strong>Architecture:</strong> Graph nodes = agent steps, edges = conditional transitions</p>\n<p><strong>Best for:</strong></p>\n<ul>\n<li>Complex decision trees with many branches</li>\n<li>Workflows needing precise state management</li>\n<li>Parallel processing with convergence points</li>\n<li>Fine-grained control over agent behavior</li>\n</ul>\n<p><strong>Pattern:</strong> &quot;If customer sentiment is negative AND order &gt; $500, route to manager agent; otherwise, route to support agent&quot;</p>\n<pre><code>         ┌─────────────┐\n         │   START     │\n         └──────┬──────┘\n                │\n         ┌──────▼──────┐\n         │  Analyze    │\n         │  Request    │\n         └──────┬──────┘\n                │\n        ┌───────┼───────┐\n        ▼       ▼       ▼\n    [Agent A] [Agent B] [Agent C]\n        │       │       │\n        └───────┼───────┘\n                ▼\n         ┌──────────────┐\n         │   Combine    │\n         │   Results    │\n         └──────────────┘\n</code></pre>\n<h4>CrewAI</h4>\n<p><strong>Architecture:</strong> Crew = team of agents with defined roles, Tasks assigned to specific agents</p>\n<p><strong>Best for:</strong></p>\n<ul>\n<li>Mimicking human team structures</li>\n<li>Clear handoffs between specialists</li>\n<li>Workflows where roles are well-defined</li>\n<li>Business processes with established responsibilities</li>\n</ul>\n<p><strong>Pattern:</strong> &quot;Researcher gathers information → Analyst evaluates → Writer drafts → Editor reviews&quot;</p>\n<pre><code>CREW: Content Production Team\n├── Researcher Agent (role: find information)\n├── Writer Agent (role: create drafts)\n├── Editor Agent (role: review and polish)\n└── Manager (coordinates, handles escalations)\n</code></pre>\n<h4>AutoGen (Microsoft)</h4>\n<p><strong>Architecture:</strong> Agents converse to solve problems, dynamic role-taking based on context</p>\n<p><strong>Best for:</strong></p>\n<ul>\n<li>Exploratory tasks where the path isn&#39;t clear</li>\n<li>Research and brainstorming</li>\n<li>Problems requiring back-and-forth reasoning</li>\n<li>Collaborative problem-solving</li>\n</ul>\n<p><strong>Pattern:</strong> &quot;Agents discuss the problem until they reach consensus or need human input&quot;</p>\n<h4>Framework Selection Decision Tree</h4>\n<h4>The No-Code Alternative</h4>\n<p><strong>Before reaching for frameworks, consider:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Platform</th>\n<th>Agent Capability</th>\n<th>Best For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Zapier Central</strong></td>\n<td>Natural language agent instructions</td>\n<td>Business users, quick prototypes</td>\n</tr>\n<tr>\n<td><strong>Make AI</strong></td>\n<td>Visual agent building with tools</td>\n<td>Complex flows without code</td>\n</tr>\n<tr>\n<td><strong>n8n AI nodes</strong></td>\n<td>Self-hosted agent workflows</td>\n<td>Privacy-conscious, technical teams</td>\n</tr>\n</tbody></table>\n<p><strong>Reality check:</strong> Most business problems don&#39;t need custom frameworks. Start with no-code, graduate to frameworks only when you hit limits.</p>\n<h4>When to Graduate to Frameworks</h4>\n<p>You need a framework when:</p>\n<ul>\n<li>No-code tools can&#39;t express your logic</li>\n<li>You need custom tool integrations</li>\n<li>Performance or cost requires optimization</li>\n<li>You need fine-grained control over agent behavior</li>\n<li>Compliance requires audit trails no-code can&#39;t provide</li>\n</ul>\n<p>You DON&#39;T need a framework when:</p>\n<ul>\n<li>The workflow is mostly linear</li>\n<li>Existing integrations cover your tools</li>\n<li>The team can&#39;t maintain code</li>\n<li>Time-to-value matters more than optimization</li>\n</ul>\n<h3>Debugging Checklist</h3>\n<table>\n<thead>\n<tr>\n<th>Issue</th>\n<th>Check</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Workflow doesn&#39;t trigger</td>\n<td>Is trigger configured correctly? Test data format?</td>\n</tr>\n<tr>\n<td>AI step fails</td>\n<td>Check API key, rate limits, prompt format</td>\n</tr>\n<tr>\n<td>Wrong output</td>\n<td>Verify data mapping, check AI response parsing</td>\n</tr>\n<tr>\n<td>Partial completion</td>\n<td>Look for error in specific step, check conditions</td>\n</tr>\n<tr>\n<td>Performance slow</td>\n<td>Too many steps? Optimize AI calls, batch where possible</td>\n</tr>\n</tbody></table>\n<h3>Self-Check</h3>\n<hr>\n"
        },
        {
          "id": "up-next",
          "title": "Up Next",
          "type": "generic",
          "content": "In **Module 3.4: Testing and Deployment**, you'll learn how to validate your AI applications work correctly and deploy them for real use.",
          "htmlContent": "<p>In <strong>Module 3.4: Testing and Deployment</strong>, you&#39;ll learn how to validate your AI applications work correctly and deploy them for real use.</p>\n"
        }
      ],
      "concepts": [
        {
          "id": "triggers-actions",
          "term": "triggers actions",
          "definition": "Automation workflows consist of:\n\n**Trigger**: The event that starts the workflow\n- New email received\n- Form submitted\n- Scheduled time\n- Webhook called\n- File uploaded\n\n**Actions**: Steps that execute in sequence\n- Send data to AI\n- Parse the response\n- Create record in database\n- Send notification\n- Update spreadsheet",
          "htmlDefinition": "<p>Automation workflows consist of:</p>\n<p><strong>Trigger</strong>: The event that starts the workflow</p>\n<ul>\n<li>New email received</li>\n<li>Form submitted</li>\n<li>Scheduled time</li>\n<li>Webhook called</li>\n<li>File uploaded</li>\n</ul>\n<p><strong>Actions</strong>: Steps that execute in sequence</p>\n<ul>\n<li>Send data to AI</li>\n<li>Parse the response</li>\n<li>Create record in database</li>\n<li>Send notification</li>\n<li>Update spreadsheet</li>\n</ul>\n"
        },
        {
          "id": "automation-config",
          "term": "automation config",
          "definition": "**Naming conventions:**\n- Use descriptive names for steps\n- Include the tool name: \"Slack: Send message to #alerts\"\n- Number steps if sequence matters: \"1. Get data, 2. Process, 3. Output\"\n\n**Error handling:**\n- Always add error paths\n- Send alerts when workflows fail\n- Log failures for debugging\n- Build in retry logic\n\n**Testing:**\n- Test with sample data before going live\n- Check edge cases (empty values, long text)\n- Verify AI outputs are properly parsed\n- Monitor first few live executions",
          "htmlDefinition": "<p><strong>Naming conventions:</strong></p>\n<ul>\n<li>Use descriptive names for steps</li>\n<li>Include the tool name: &quot;Slack: Send message to #alerts&quot;</li>\n<li>Number steps if sequence matters: &quot;1. Get data, 2. Process, 3. Output&quot;</li>\n</ul>\n<p><strong>Error handling:</strong></p>\n<ul>\n<li>Always add error paths</li>\n<li>Send alerts when workflows fail</li>\n<li>Log failures for debugging</li>\n<li>Build in retry logic</li>\n</ul>\n<p><strong>Testing:</strong></p>\n<ul>\n<li>Test with sample data before going live</li>\n<li>Check edge cases (empty values, long text)</li>\n<li>Verify AI outputs are properly parsed</li>\n<li>Monitor first few live executions</li>\n</ul>\n"
        },
        {
          "id": "multi-agent-frameworks",
          "term": "multi agent frameworks",
          "definition": "**Multi-agent frameworks** are specialized tools for building AI systems where multiple agents collaborate, delegate, and coordinate. They go beyond simple automation to enable reasoning, dynamic task allocation, and autonomous problem-solving.",
          "htmlDefinition": "<p><strong>Multi-agent frameworks</strong> are specialized tools for building AI systems where multiple agents collaborate, delegate, and coordinate. They go beyond simple automation to enable reasoning, dynamic task allocation, and autonomous problem-solving.</p>\n"
        },
        {
          "id": "framework-selection",
          "term": "framework selection",
          "definition": "Use this decision tree to choose the right framework:\n\n```\nDo you need precise workflow control?\n├─ YES → LangGraph\n│        (You want to define exact paths and conditions)\n│\n└─ NO → Are agent roles clearly defined upfront?\n        │\n        ├─ YES → CrewAI\n        │        (You know who does what)\n        │\n        └─ NO → AutoGen\n                 (Agents figure it out together)\n```",
          "htmlDefinition": "<p>Use this decision tree to choose the right framework:</p>\n<pre><code>Do you need precise workflow control?\n├─ YES → LangGraph\n│        (You want to define exact paths and conditions)\n│\n└─ NO → Are agent roles clearly defined upfront?\n        │\n        ├─ YES → CrewAI\n        │        (You know who does what)\n        │\n        └─ NO → AutoGen\n                 (Agents figure it out together)\n</code></pre>\n"
        }
      ],
      "exercises": [
        {
          "id": "automation-design",
          "title": "Design a workflow for one of these scenarios:",
          "instructions": "**Scenario A: Content Repurposing**\n- Trigger: New blog post published (RSS feed)\n- Process: AI creates social media posts, email summary, and tweet thread\n- Output: Posts created in Buffer, email draft in Gmail, tweet thread saved\n\n**Scenario B: Lead Qualification**\n- Trigger: New form submission\n- Process: AI analyzes response, scores lead, suggests next action\n- Output: Lead created in CRM with score, assigned to appropriate person\n\n**Scenario C: Meeting Follow-up**\n- Trigger: Meeting ends in calendar\n- Process: AI summarizes transcript, identifies action items\n- Output: Summary posted in Slack, tasks created in project management tool\n\n**For your chosen scenario, document:**\n1. Trigger event and source\n2. Each action step with inputs/outputs\n3. What data passes between steps\n4. Error handling approach\n5. Where AI is used and what prompt you'd send",
          "htmlInstructions": "<p><strong>Scenario A: Content Repurposing</strong></p>\n<ul>\n<li>Trigger: New blog post published (RSS feed)</li>\n<li>Process: AI creates social media posts, email summary, and tweet thread</li>\n<li>Output: Posts created in Buffer, email draft in Gmail, tweet thread saved</li>\n</ul>\n<p><strong>Scenario B: Lead Qualification</strong></p>\n<ul>\n<li>Trigger: New form submission</li>\n<li>Process: AI analyzes response, scores lead, suggests next action</li>\n<li>Output: Lead created in CRM with score, assigned to appropriate person</li>\n</ul>\n<p><strong>Scenario C: Meeting Follow-up</strong></p>\n<ul>\n<li>Trigger: Meeting ends in calendar</li>\n<li>Process: AI summarizes transcript, identifies action items</li>\n<li>Output: Summary posted in Slack, tasks created in project management tool</li>\n</ul>\n<p><strong>For your chosen scenario, document:</strong></p>\n<ol>\n<li>Trigger event and source</li>\n<li>Each action step with inputs/outputs</li>\n<li>What data passes between steps</li>\n<li>Error handling approach</li>\n<li>Where AI is used and what prompt you&#39;d send</li>\n</ol>\n"
        }
      ],
      "checklists": [
        {
          "id": "module-3.3-complete",
          "items": [
            {
              "id": "module-3.3-complete-0",
              "text": "I know the major automation platforms and their strengths",
              "completed": false
            },
            {
              "id": "module-3.3-complete-1",
              "text": "I understand triggers, actions, and workflow structure",
              "completed": false
            },
            {
              "id": "module-3.3-complete-2",
              "text": "I can design AI integration patterns",
              "completed": false
            },
            {
              "id": "module-3.3-complete-3",
              "text": "I know how to handle errors in automations",
              "completed": false
            },
            {
              "id": "module-3.3-complete-4",
              "text": "I can create a workflow blueprint for a business scenario",
              "completed": false
            }
          ]
        }
      ]
    },
    {
      "id": "3.4-testing-and-deployment",
      "slug": "3.4-testing-and-deployment",
      "title": "Testing and Deployment",
      "phase": 3,
      "module": 4,
      "phaseId": "phase-3",
      "estimatedMinutes": 18,
      "bloomLevel": "apply",
      "content": "# Testing and Deployment\n\n## WHY This Matters\n\nAI applications fail in surprising ways. Unlike traditional software with predictable bugs, AI can produce plausible-but-wrong outputs that look fine until they cause problems. Rigorous testing and careful deployment protect:\n\n- **Your users** from AI mistakes\n- **Your reputation** from embarrassing failures\n- **Your organization** from liability\n- **Your resources** from costly fixes\n\nShip fast, but ship responsibly.\n\n---\n\n## WHAT You Need to Know\n\n### AI-Specific Testing Challenges\n\n### Testing Strategies\n\n**1. Golden Dataset Testing**\nCreate a set of inputs with known-good outputs:\n\n| Input | Expected Output | Evaluation Criteria |\n|-------|-----------------|---------------------|\n| [Test case 1] | [Expected response] | Matches key points, appropriate tone |\n| [Test case 2] | [Expected response] | Includes required elements |\n| [Edge case 1] | [Expected handling] | Graceful failure, escalation |\n\n**2. Persona Testing**\nTest with different user types:\n- Friendly user with clear request\n- Confused user with vague request\n- Hostile user testing boundaries\n- Technical user with complex needs\n- Non-native English speaker\n\n**3. Failure Mode Testing**\nDeliberately try to break it:\n- Extremely long inputs\n- Empty or minimal inputs\n- Conflicting instructions\n- Off-topic requests\n- Prompt injection attempts\n\n**4. Consistency Testing**\nRun same input multiple times:\n- Are outputs meaningfully similar?\n- Does quality vary unacceptably?\n- Are there occasional failures?\n\n---\n\n### Agent Evaluation Frameworks\n\nWhen testing AI agents (not just prompts), you need specialized evaluation approaches. Agents have additional dimensions of complexity that simple input/output testing doesn't capture.\n\n#### The Agent Evaluation Matrix\n\n| Dimension | What It Tests | Key Metrics | Example Failure |\n|-----------|---------------|-------------|-----------------|\n| **Task Success** | Goal completion | Success rate, completion time | Agent gives up prematurely |\n| **Output Quality** | Result correctness | Accuracy, relevance scores | Correct answer, wrong format |\n| **Tool Use** | Capability selection | Tool accuracy, API efficiency | Used search when calculator needed |\n| **Reasoning** | Decision quality | Logic validity, step coherence | Correct conclusion from wrong logic |\n| **Safety** | Boundary respect | Policy compliance, escalation accuracy | Acted beyond authorized scope |\n| **Reliability** | Consistency | Variance across runs, failure rate | Works 80% of the time |\n\n#### Industry Benchmarks Reference\n\nLeading AI labs evaluate agents against standardized benchmarks. Understanding these helps you design your own evaluations:\n\n#### Building Your Own Evaluation\n\nDon't copy industry benchmarks—design evaluations for YOUR use case:\n\n**Step 1: Define success dimensions**\nWhat does \"good\" mean for your agent? Rank these by importance:\n- Accuracy of final output\n- Efficiency (time, tokens, API calls)\n- User experience (response quality, helpfulness)\n- Safety (staying in bounds, escalating correctly)\n\n**Step 2: Create golden examples**\nFor each critical task, define:\n```\nGOLDEN EXAMPLE\n├── Input: [What the agent receives]\n├── Expected tool calls: [Which tools, in what order]\n├── Expected reasoning: [Key decision points]\n├── Expected output: [What success looks like]\n└── Failure modes: [What would be wrong]\n```\n\n**Step 3: Test systematically**\n```\nHappy path      → Does it work when everything is normal?\nEdge cases      → How does it handle unusual inputs?\nError recovery  → What happens when tools fail?\nAdversarial     → Can users manipulate or confuse it?\nConsistency     → Same input 5 times = same quality?\n```\n\n**Step 4: Measure and iterate**\nTrack metrics over time. Improvements to prompts, tools, or orchestration should show up in your evaluation scores.\n\n#### Evaluation Anti-Patterns\n\n| Anti-Pattern | Problem | Better Approach |\n|--------------|---------|-----------------|\n| **Vibes-based eval** | \"It seems good\" | Define specific metrics |\n| **Demo-only testing** | Cherry-picked examples | Random sampling from real usage |\n| **Single-run scoring** | Ignores variance | Multiple runs per test case |\n| **Output-only focus** | Misses process issues | Evaluate reasoning and tool use |\n| **Static test sets** | Miss evolving issues | Add failing cases to test set |\n| **Human-only review** | Doesn't scale | Combine automated + human eval |\n\n#### LLM-as-Judge Pattern\n\nUse AI to evaluate AI—with careful design:\n\n```\nEVALUATOR PROMPT STRUCTURE\n├── Clear criteria: Exactly what to evaluate\n├── Rubric: Specific scoring guidelines (1-5 scale)\n├── Examples: What each score looks like\n├── Output format: Structured (JSON) for parsing\n└── Reasoning: Ask for explanation before score\n```\n\n**Example rubric prompt:**\n\n> Rate this customer support response on a 1-5 scale:\n>\n> 5: Fully addresses query, accurate, helpful, appropriate tone\n> 4: Addresses query with minor gaps, mostly accurate\n> 3: Partially addresses query, some inaccuracies\n> 2: Misses key points, notable errors\n> 1: Incorrect, unhelpful, or inappropriate\n>\n> First explain your reasoning, then provide score as JSON: {\"score\": N}\n\n**Caution**: LLM evaluators have biases (prefer verbose answers, struggle with domain expertise). Calibrate against human judgment.\n\n### Pre-Launch Checklist\n\n### Deployment Approaches\n\n**Approach 1: Shadow Mode**\n```\nUser request → Current system responds\n            ↘ AI system also runs (not shown to user)\n            → Compare outputs, evaluate AI readiness\n```\n\n**Approach 2: Limited Rollout**\n```\nWeek 1: 5% of requests → AI\nWeek 2: 25% of requests → AI (if Week 1 OK)\nWeek 3: 100% of requests → AI (if Week 2 OK)\n```\n\n**Approach 3: Human-in-the-Loop**\n```\nAI generates → Human reviews → Approved outputs go live\n            → Rejected outputs inform improvements\n```\n\n**Approach 4: Internal First**\n```\nInternal users test → Feedback incorporated → External users\n```\n\n### Monitoring in Production\n\n| What to Monitor | Why | Red Flags |\n|-----------------|-----|-----------|\n| **Response quality** | Catch degradation | Complaints, low ratings |\n| **Latency** | User experience | >5s response times |\n| **Error rate** | System health | >1% API errors |\n| **Token usage** | Cost control | Unexpected spikes |\n| **Escalation rate** | AI capability | Rising escalations |\n| **User satisfaction** | Overall success | Declining feedback |\n\n### Iterative Improvement\n\nAfter launch, establish feedback loops:\n\n```\nProduction usage\n      ↓\nCollect feedback (automated + manual)\n      ↓\nIdentify improvement opportunities\n      ↓\nUpdate prompts / configuration\n      ↓\nTest changes\n      ↓\nDeploy updates\n      ↓\n[Repeat]\n```\n\n---\n\n## HOW to Apply This\n\n### Exercise: Create a Test Plan\n\n### Testing Template\n\n```\nTEST PLAN: [Application Name]\n\n1. GOLDEN DATASET\n| ID | Input | Expected Output | Pass Criteria |\n|----|-------|-----------------|---------------|\n| G1 | ... | ... | ... |\n| G2 | ... | ... | ... |\n\n2. PERSONA TESTS\n| Persona | Scenario | Expected Behavior |\n|---------|----------|-------------------|\n| P1 | ... | ... |\n| P2 | ... | ... |\n\n3. FAILURE MODE TESTS\n| Mode | Input | Expected Handling |\n|------|-------|-------------------|\n| F1 | ... | ... |\n| F2 | ... | ... |\n\n4. SUCCESS METRICS\n| Metric | Target | Measurement |\n|--------|--------|-------------|\n| ... | ... | ... |\n\n5. DEPLOYMENT STAGES\n| Stage | Users | Duration | Success Gate |\n|-------|-------|----------|--------------|\n| ... | ... | ... | ... |\n```\n\n### Common Launch Failures\n\n| Failure | Cause | Prevention |\n|---------|-------|------------|\n| Quality disaster | Insufficient testing | Comprehensive test plan |\n| Cost overrun | Token usage underestimated | Load testing, cost projections |\n| User confusion | Poor UX | Beta testing with real users |\n| Security incident | Prompt injection | Security testing |\n| Performance issues | Scale not considered | Load testing |\n| Rollback chaos | No plan | Document rollback procedure |\n\n### Self-Check\n\n---\n\n## Phase 3 Complete!\n\nYou've built your Implementation skills. Before moving to Phase 4, complete:\n\n**Lab 5: Build an AI Assistant** — Create a functional AI assistant using no-code tools\n\n**Phase 3 Deliverable: Working AI Application** — Build and deploy a functional AI-powered tool that solves a real business problem",
      "htmlContent": "<h1>Testing and Deployment</h1>\n<h2>WHY This Matters</h2>\n<p>AI applications fail in surprising ways. Unlike traditional software with predictable bugs, AI can produce plausible-but-wrong outputs that look fine until they cause problems. Rigorous testing and careful deployment protect:</p>\n<ul>\n<li><strong>Your users</strong> from AI mistakes</li>\n<li><strong>Your reputation</strong> from embarrassing failures</li>\n<li><strong>Your organization</strong> from liability</li>\n<li><strong>Your resources</strong> from costly fixes</li>\n</ul>\n<p>Ship fast, but ship responsibly.</p>\n<hr>\n<h2>WHAT You Need to Know</h2>\n<h3>AI-Specific Testing Challenges</h3>\n<h3>Testing Strategies</h3>\n<p><strong>1. Golden Dataset Testing</strong>\nCreate a set of inputs with known-good outputs:</p>\n<table>\n<thead>\n<tr>\n<th>Input</th>\n<th>Expected Output</th>\n<th>Evaluation Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>[Test case 1]</td>\n<td>[Expected response]</td>\n<td>Matches key points, appropriate tone</td>\n</tr>\n<tr>\n<td>[Test case 2]</td>\n<td>[Expected response]</td>\n<td>Includes required elements</td>\n</tr>\n<tr>\n<td>[Edge case 1]</td>\n<td>[Expected handling]</td>\n<td>Graceful failure, escalation</td>\n</tr>\n</tbody></table>\n<p><strong>2. Persona Testing</strong>\nTest with different user types:</p>\n<ul>\n<li>Friendly user with clear request</li>\n<li>Confused user with vague request</li>\n<li>Hostile user testing boundaries</li>\n<li>Technical user with complex needs</li>\n<li>Non-native English speaker</li>\n</ul>\n<p><strong>3. Failure Mode Testing</strong>\nDeliberately try to break it:</p>\n<ul>\n<li>Extremely long inputs</li>\n<li>Empty or minimal inputs</li>\n<li>Conflicting instructions</li>\n<li>Off-topic requests</li>\n<li>Prompt injection attempts</li>\n</ul>\n<p><strong>4. Consistency Testing</strong>\nRun same input multiple times:</p>\n<ul>\n<li>Are outputs meaningfully similar?</li>\n<li>Does quality vary unacceptably?</li>\n<li>Are there occasional failures?</li>\n</ul>\n<hr>\n<h3>Agent Evaluation Frameworks</h3>\n<p>When testing AI agents (not just prompts), you need specialized evaluation approaches. Agents have additional dimensions of complexity that simple input/output testing doesn&#39;t capture.</p>\n<h4>The Agent Evaluation Matrix</h4>\n<table>\n<thead>\n<tr>\n<th>Dimension</th>\n<th>What It Tests</th>\n<th>Key Metrics</th>\n<th>Example Failure</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Task Success</strong></td>\n<td>Goal completion</td>\n<td>Success rate, completion time</td>\n<td>Agent gives up prematurely</td>\n</tr>\n<tr>\n<td><strong>Output Quality</strong></td>\n<td>Result correctness</td>\n<td>Accuracy, relevance scores</td>\n<td>Correct answer, wrong format</td>\n</tr>\n<tr>\n<td><strong>Tool Use</strong></td>\n<td>Capability selection</td>\n<td>Tool accuracy, API efficiency</td>\n<td>Used search when calculator needed</td>\n</tr>\n<tr>\n<td><strong>Reasoning</strong></td>\n<td>Decision quality</td>\n<td>Logic validity, step coherence</td>\n<td>Correct conclusion from wrong logic</td>\n</tr>\n<tr>\n<td><strong>Safety</strong></td>\n<td>Boundary respect</td>\n<td>Policy compliance, escalation accuracy</td>\n<td>Acted beyond authorized scope</td>\n</tr>\n<tr>\n<td><strong>Reliability</strong></td>\n<td>Consistency</td>\n<td>Variance across runs, failure rate</td>\n<td>Works 80% of the time</td>\n</tr>\n</tbody></table>\n<h4>Industry Benchmarks Reference</h4>\n<p>Leading AI labs evaluate agents against standardized benchmarks. Understanding these helps you design your own evaluations:</p>\n<h4>Building Your Own Evaluation</h4>\n<p>Don&#39;t copy industry benchmarks—design evaluations for YOUR use case:</p>\n<p><strong>Step 1: Define success dimensions</strong>\nWhat does &quot;good&quot; mean for your agent? Rank these by importance:</p>\n<ul>\n<li>Accuracy of final output</li>\n<li>Efficiency (time, tokens, API calls)</li>\n<li>User experience (response quality, helpfulness)</li>\n<li>Safety (staying in bounds, escalating correctly)</li>\n</ul>\n<p><strong>Step 2: Create golden examples</strong>\nFor each critical task, define:</p>\n<pre><code>GOLDEN EXAMPLE\n├── Input: [What the agent receives]\n├── Expected tool calls: [Which tools, in what order]\n├── Expected reasoning: [Key decision points]\n├── Expected output: [What success looks like]\n└── Failure modes: [What would be wrong]\n</code></pre>\n<p><strong>Step 3: Test systematically</strong></p>\n<pre><code>Happy path      → Does it work when everything is normal?\nEdge cases      → How does it handle unusual inputs?\nError recovery  → What happens when tools fail?\nAdversarial     → Can users manipulate or confuse it?\nConsistency     → Same input 5 times = same quality?\n</code></pre>\n<p><strong>Step 4: Measure and iterate</strong>\nTrack metrics over time. Improvements to prompts, tools, or orchestration should show up in your evaluation scores.</p>\n<h4>Evaluation Anti-Patterns</h4>\n<table>\n<thead>\n<tr>\n<th>Anti-Pattern</th>\n<th>Problem</th>\n<th>Better Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Vibes-based eval</strong></td>\n<td>&quot;It seems good&quot;</td>\n<td>Define specific metrics</td>\n</tr>\n<tr>\n<td><strong>Demo-only testing</strong></td>\n<td>Cherry-picked examples</td>\n<td>Random sampling from real usage</td>\n</tr>\n<tr>\n<td><strong>Single-run scoring</strong></td>\n<td>Ignores variance</td>\n<td>Multiple runs per test case</td>\n</tr>\n<tr>\n<td><strong>Output-only focus</strong></td>\n<td>Misses process issues</td>\n<td>Evaluate reasoning and tool use</td>\n</tr>\n<tr>\n<td><strong>Static test sets</strong></td>\n<td>Miss evolving issues</td>\n<td>Add failing cases to test set</td>\n</tr>\n<tr>\n<td><strong>Human-only review</strong></td>\n<td>Doesn&#39;t scale</td>\n<td>Combine automated + human eval</td>\n</tr>\n</tbody></table>\n<h4>LLM-as-Judge Pattern</h4>\n<p>Use AI to evaluate AI—with careful design:</p>\n<pre><code>EVALUATOR PROMPT STRUCTURE\n├── Clear criteria: Exactly what to evaluate\n├── Rubric: Specific scoring guidelines (1-5 scale)\n├── Examples: What each score looks like\n├── Output format: Structured (JSON) for parsing\n└── Reasoning: Ask for explanation before score\n</code></pre>\n<p><strong>Example rubric prompt:</strong></p>\n<blockquote>\n<p>Rate this customer support response on a 1-5 scale:</p>\n<p>5: Fully addresses query, accurate, helpful, appropriate tone\n4: Addresses query with minor gaps, mostly accurate\n3: Partially addresses query, some inaccuracies\n2: Misses key points, notable errors\n1: Incorrect, unhelpful, or inappropriate</p>\n<p>First explain your reasoning, then provide score as JSON: {&quot;score&quot;: N}</p>\n</blockquote>\n<p><strong>Caution</strong>: LLM evaluators have biases (prefer verbose answers, struggle with domain expertise). Calibrate against human judgment.</p>\n<h3>Pre-Launch Checklist</h3>\n<h3>Deployment Approaches</h3>\n<p><strong>Approach 1: Shadow Mode</strong></p>\n<pre><code>User request → Current system responds\n            ↘ AI system also runs (not shown to user)\n            → Compare outputs, evaluate AI readiness\n</code></pre>\n<p><strong>Approach 2: Limited Rollout</strong></p>\n<pre><code>Week 1: 5% of requests → AI\nWeek 2: 25% of requests → AI (if Week 1 OK)\nWeek 3: 100% of requests → AI (if Week 2 OK)\n</code></pre>\n<p><strong>Approach 3: Human-in-the-Loop</strong></p>\n<pre><code>AI generates → Human reviews → Approved outputs go live\n            → Rejected outputs inform improvements\n</code></pre>\n<p><strong>Approach 4: Internal First</strong></p>\n<pre><code>Internal users test → Feedback incorporated → External users\n</code></pre>\n<h3>Monitoring in Production</h3>\n<table>\n<thead>\n<tr>\n<th>What to Monitor</th>\n<th>Why</th>\n<th>Red Flags</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Response quality</strong></td>\n<td>Catch degradation</td>\n<td>Complaints, low ratings</td>\n</tr>\n<tr>\n<td><strong>Latency</strong></td>\n<td>User experience</td>\n<td>&gt;5s response times</td>\n</tr>\n<tr>\n<td><strong>Error rate</strong></td>\n<td>System health</td>\n<td>&gt;1% API errors</td>\n</tr>\n<tr>\n<td><strong>Token usage</strong></td>\n<td>Cost control</td>\n<td>Unexpected spikes</td>\n</tr>\n<tr>\n<td><strong>Escalation rate</strong></td>\n<td>AI capability</td>\n<td>Rising escalations</td>\n</tr>\n<tr>\n<td><strong>User satisfaction</strong></td>\n<td>Overall success</td>\n<td>Declining feedback</td>\n</tr>\n</tbody></table>\n<h3>Iterative Improvement</h3>\n<p>After launch, establish feedback loops:</p>\n<pre><code>Production usage\n      ↓\nCollect feedback (automated + manual)\n      ↓\nIdentify improvement opportunities\n      ↓\nUpdate prompts / configuration\n      ↓\nTest changes\n      ↓\nDeploy updates\n      ↓\n[Repeat]\n</code></pre>\n<hr>\n<h2>HOW to Apply This</h2>\n<h3>Exercise: Create a Test Plan</h3>\n<h3>Testing Template</h3>\n<pre><code>TEST PLAN: [Application Name]\n\n1. GOLDEN DATASET\n| ID | Input | Expected Output | Pass Criteria |\n|----|-------|-----------------|---------------|\n| G1 | ... | ... | ... |\n| G2 | ... | ... | ... |\n\n2. PERSONA TESTS\n| Persona | Scenario | Expected Behavior |\n|---------|----------|-------------------|\n| P1 | ... | ... |\n| P2 | ... | ... |\n\n3. FAILURE MODE TESTS\n| Mode | Input | Expected Handling |\n|------|-------|-------------------|\n| F1 | ... | ... |\n| F2 | ... | ... |\n\n4. SUCCESS METRICS\n| Metric | Target | Measurement |\n|--------|--------|-------------|\n| ... | ... | ... |\n\n5. DEPLOYMENT STAGES\n| Stage | Users | Duration | Success Gate |\n|-------|-------|----------|--------------|\n| ... | ... | ... | ... |\n</code></pre>\n<h3>Common Launch Failures</h3>\n<table>\n<thead>\n<tr>\n<th>Failure</th>\n<th>Cause</th>\n<th>Prevention</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Quality disaster</td>\n<td>Insufficient testing</td>\n<td>Comprehensive test plan</td>\n</tr>\n<tr>\n<td>Cost overrun</td>\n<td>Token usage underestimated</td>\n<td>Load testing, cost projections</td>\n</tr>\n<tr>\n<td>User confusion</td>\n<td>Poor UX</td>\n<td>Beta testing with real users</td>\n</tr>\n<tr>\n<td>Security incident</td>\n<td>Prompt injection</td>\n<td>Security testing</td>\n</tr>\n<tr>\n<td>Performance issues</td>\n<td>Scale not considered</td>\n<td>Load testing</td>\n</tr>\n<tr>\n<td>Rollback chaos</td>\n<td>No plan</td>\n<td>Document rollback procedure</td>\n</tr>\n</tbody></table>\n<h3>Self-Check</h3>\n<hr>\n<h2>Phase 3 Complete!</h2>\n<p>You&#39;ve built your Implementation skills. Before moving to Phase 4, complete:</p>\n<p><strong>Lab 5: Build an AI Assistant</strong> — Create a functional AI assistant using no-code tools</p>\n<p><strong>Phase 3 Deliverable: Working AI Application</strong> — Build and deploy a functional AI-powered tool that solves a real business problem</p>\n",
      "sections": [
        {
          "id": "why-this-matters",
          "title": "WHY This Matters",
          "type": "why",
          "content": "AI applications fail in surprising ways. Unlike traditional software with predictable bugs, AI can produce plausible-but-wrong outputs that look fine until they cause problems. Rigorous testing and careful deployment protect:\n\n- **Your users** from AI mistakes\n- **Your reputation** from embarrassing failures\n- **Your organization** from liability\n- **Your resources** from costly fixes\n\nShip fast, but ship responsibly.\n\n---",
          "htmlContent": "<p>AI applications fail in surprising ways. Unlike traditional software with predictable bugs, AI can produce plausible-but-wrong outputs that look fine until they cause problems. Rigorous testing and careful deployment protect:</p>\n<ul>\n<li><strong>Your users</strong> from AI mistakes</li>\n<li><strong>Your reputation</strong> from embarrassing failures</li>\n<li><strong>Your organization</strong> from liability</li>\n<li><strong>Your resources</strong> from costly fixes</li>\n</ul>\n<p>Ship fast, but ship responsibly.</p>\n<hr>\n"
        },
        {
          "id": "what-you-need-to-know",
          "title": "WHAT You Need to Know",
          "type": "what",
          "content": "### AI-Specific Testing Challenges\n\n### Testing Strategies\n\n**1. Golden Dataset Testing**\nCreate a set of inputs with known-good outputs:\n\n| Input | Expected Output | Evaluation Criteria |\n|-------|-----------------|---------------------|\n| [Test case 1] | [Expected response] | Matches key points, appropriate tone |\n| [Test case 2] | [Expected response] | Includes required elements |\n| [Edge case 1] | [Expected handling] | Graceful failure, escalation |\n\n**2. Persona Testing**\nTest with different user types:\n- Friendly user with clear request\n- Confused user with vague request\n- Hostile user testing boundaries\n- Technical user with complex needs\n- Non-native English speaker\n\n**3. Failure Mode Testing**\nDeliberately try to break it:\n- Extremely long inputs\n- Empty or minimal inputs\n- Conflicting instructions\n- Off-topic requests\n- Prompt injection attempts\n\n**4. Consistency Testing**\nRun same input multiple times:\n- Are outputs meaningfully similar?\n- Does quality vary unacceptably?\n- Are there occasional failures?\n\n---\n\n### Agent Evaluation Frameworks\n\nWhen testing AI agents (not just prompts), you need specialized evaluation approaches. Agents have additional dimensions of complexity that simple input/output testing doesn't capture.\n\n#### The Agent Evaluation Matrix\n\n| Dimension | What It Tests | Key Metrics | Example Failure |\n|-----------|---------------|-------------|-----------------|\n| **Task Success** | Goal completion | Success rate, completion time | Agent gives up prematurely |\n| **Output Quality** | Result correctness | Accuracy, relevance scores | Correct answer, wrong format |\n| **Tool Use** | Capability selection | Tool accuracy, API efficiency | Used search when calculator needed |\n| **Reasoning** | Decision quality | Logic validity, step coherence | Correct conclusion from wrong logic |\n| **Safety** | Boundary respect | Policy compliance, escalation accuracy | Acted beyond authorized scope |\n| **Reliability** | Consistency | Variance across runs, failure rate | Works 80% of the time |\n\n#### Industry Benchmarks Reference\n\nLeading AI labs evaluate agents against standardized benchmarks. Understanding these helps you design your own evaluations:\n\n#### Building Your Own Evaluation\n\nDon't copy industry benchmarks—design evaluations for YOUR use case:\n\n**Step 1: Define success dimensions**\nWhat does \"good\" mean for your agent? Rank these by importance:\n- Accuracy of final output\n- Efficiency (time, tokens, API calls)\n- User experience (response quality, helpfulness)\n- Safety (staying in bounds, escalating correctly)\n\n**Step 2: Create golden examples**\nFor each critical task, define:\n```\nGOLDEN EXAMPLE\n├── Input: [What the agent receives]\n├── Expected tool calls: [Which tools, in what order]\n├── Expected reasoning: [Key decision points]\n├── Expected output: [What success looks like]\n└── Failure modes: [What would be wrong]\n```\n\n**Step 3: Test systematically**\n```\nHappy path      → Does it work when everything is normal?\nEdge cases      → How does it handle unusual inputs?\nError recovery  → What happens when tools fail?\nAdversarial     → Can users manipulate or confuse it?\nConsistency     → Same input 5 times = same quality?\n```\n\n**Step 4: Measure and iterate**\nTrack metrics over time. Improvements to prompts, tools, or orchestration should show up in your evaluation scores.\n\n#### Evaluation Anti-Patterns\n\n| Anti-Pattern | Problem | Better Approach |\n|--------------|---------|-----------------|\n| **Vibes-based eval** | \"It seems good\" | Define specific metrics |\n| **Demo-only testing** | Cherry-picked examples | Random sampling from real usage |\n| **Single-run scoring** | Ignores variance | Multiple runs per test case |\n| **Output-only focus** | Misses process issues | Evaluate reasoning and tool use |\n| **Static test sets** | Miss evolving issues | Add failing cases to test set |\n| **Human-only review** | Doesn't scale | Combine automated + human eval |\n\n#### LLM-as-Judge Pattern\n\nUse AI to evaluate AI—with careful design:\n\n```\nEVALUATOR PROMPT STRUCTURE\n├── Clear criteria: Exactly what to evaluate\n├── Rubric: Specific scoring guidelines (1-5 scale)\n├── Examples: What each score looks like\n├── Output format: Structured (JSON) for parsing\n└── Reasoning: Ask for explanation before score\n```\n\n**Example rubric prompt:**\n\n> Rate this customer support response on a 1-5 scale:\n>\n> 5: Fully addresses query, accurate, helpful, appropriate tone\n> 4: Addresses query with minor gaps, mostly accurate\n> 3: Partially addresses query, some inaccuracies\n> 2: Misses key points, notable errors\n> 1: Incorrect, unhelpful, or inappropriate\n>\n> First explain your reasoning, then provide score as JSON: {\"score\": N}\n\n**Caution**: LLM evaluators have biases (prefer verbose answers, struggle with domain expertise). Calibrate against human judgment.\n\n### Pre-Launch Checklist\n\n### Deployment Approaches\n\n**Approach 1: Shadow Mode**\n```\nUser request → Current system responds\n            ↘ AI system also runs (not shown to user)\n            → Compare outputs, evaluate AI readiness\n```\n\n**Approach 2: Limited Rollout**\n```\nWeek 1: 5% of requests → AI\nWeek 2: 25% of requests → AI (if Week 1 OK)\nWeek 3: 100% of requests → AI (if Week 2 OK)\n```\n\n**Approach 3: Human-in-the-Loop**\n```\nAI generates → Human reviews → Approved outputs go live\n            → Rejected outputs inform improvements\n```\n\n**Approach 4: Internal First**\n```\nInternal users test → Feedback incorporated → External users\n```\n\n### Monitoring in Production\n\n| What to Monitor | Why | Red Flags |\n|-----------------|-----|-----------|\n| **Response quality** | Catch degradation | Complaints, low ratings |\n| **Latency** | User experience | >5s response times |\n| **Error rate** | System health | >1% API errors |\n| **Token usage** | Cost control | Unexpected spikes |\n| **Escalation rate** | AI capability | Rising escalations |\n| **User satisfaction** | Overall success | Declining feedback |\n\n### Iterative Improvement\n\nAfter launch, establish feedback loops:\n\n```\nProduction usage\n      ↓\nCollect feedback (automated + manual)\n      ↓\nIdentify improvement opportunities\n      ↓\nUpdate prompts / configuration\n      ↓\nTest changes\n      ↓\nDeploy updates\n      ↓\n[Repeat]\n```\n\n---",
          "htmlContent": "<h3>AI-Specific Testing Challenges</h3>\n<h3>Testing Strategies</h3>\n<p><strong>1. Golden Dataset Testing</strong>\nCreate a set of inputs with known-good outputs:</p>\n<table>\n<thead>\n<tr>\n<th>Input</th>\n<th>Expected Output</th>\n<th>Evaluation Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>[Test case 1]</td>\n<td>[Expected response]</td>\n<td>Matches key points, appropriate tone</td>\n</tr>\n<tr>\n<td>[Test case 2]</td>\n<td>[Expected response]</td>\n<td>Includes required elements</td>\n</tr>\n<tr>\n<td>[Edge case 1]</td>\n<td>[Expected handling]</td>\n<td>Graceful failure, escalation</td>\n</tr>\n</tbody></table>\n<p><strong>2. Persona Testing</strong>\nTest with different user types:</p>\n<ul>\n<li>Friendly user with clear request</li>\n<li>Confused user with vague request</li>\n<li>Hostile user testing boundaries</li>\n<li>Technical user with complex needs</li>\n<li>Non-native English speaker</li>\n</ul>\n<p><strong>3. Failure Mode Testing</strong>\nDeliberately try to break it:</p>\n<ul>\n<li>Extremely long inputs</li>\n<li>Empty or minimal inputs</li>\n<li>Conflicting instructions</li>\n<li>Off-topic requests</li>\n<li>Prompt injection attempts</li>\n</ul>\n<p><strong>4. Consistency Testing</strong>\nRun same input multiple times:</p>\n<ul>\n<li>Are outputs meaningfully similar?</li>\n<li>Does quality vary unacceptably?</li>\n<li>Are there occasional failures?</li>\n</ul>\n<hr>\n<h3>Agent Evaluation Frameworks</h3>\n<p>When testing AI agents (not just prompts), you need specialized evaluation approaches. Agents have additional dimensions of complexity that simple input/output testing doesn&#39;t capture.</p>\n<h4>The Agent Evaluation Matrix</h4>\n<table>\n<thead>\n<tr>\n<th>Dimension</th>\n<th>What It Tests</th>\n<th>Key Metrics</th>\n<th>Example Failure</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Task Success</strong></td>\n<td>Goal completion</td>\n<td>Success rate, completion time</td>\n<td>Agent gives up prematurely</td>\n</tr>\n<tr>\n<td><strong>Output Quality</strong></td>\n<td>Result correctness</td>\n<td>Accuracy, relevance scores</td>\n<td>Correct answer, wrong format</td>\n</tr>\n<tr>\n<td><strong>Tool Use</strong></td>\n<td>Capability selection</td>\n<td>Tool accuracy, API efficiency</td>\n<td>Used search when calculator needed</td>\n</tr>\n<tr>\n<td><strong>Reasoning</strong></td>\n<td>Decision quality</td>\n<td>Logic validity, step coherence</td>\n<td>Correct conclusion from wrong logic</td>\n</tr>\n<tr>\n<td><strong>Safety</strong></td>\n<td>Boundary respect</td>\n<td>Policy compliance, escalation accuracy</td>\n<td>Acted beyond authorized scope</td>\n</tr>\n<tr>\n<td><strong>Reliability</strong></td>\n<td>Consistency</td>\n<td>Variance across runs, failure rate</td>\n<td>Works 80% of the time</td>\n</tr>\n</tbody></table>\n<h4>Industry Benchmarks Reference</h4>\n<p>Leading AI labs evaluate agents against standardized benchmarks. Understanding these helps you design your own evaluations:</p>\n<h4>Building Your Own Evaluation</h4>\n<p>Don&#39;t copy industry benchmarks—design evaluations for YOUR use case:</p>\n<p><strong>Step 1: Define success dimensions</strong>\nWhat does &quot;good&quot; mean for your agent? Rank these by importance:</p>\n<ul>\n<li>Accuracy of final output</li>\n<li>Efficiency (time, tokens, API calls)</li>\n<li>User experience (response quality, helpfulness)</li>\n<li>Safety (staying in bounds, escalating correctly)</li>\n</ul>\n<p><strong>Step 2: Create golden examples</strong>\nFor each critical task, define:</p>\n<pre><code>GOLDEN EXAMPLE\n├── Input: [What the agent receives]\n├── Expected tool calls: [Which tools, in what order]\n├── Expected reasoning: [Key decision points]\n├── Expected output: [What success looks like]\n└── Failure modes: [What would be wrong]\n</code></pre>\n<p><strong>Step 3: Test systematically</strong></p>\n<pre><code>Happy path      → Does it work when everything is normal?\nEdge cases      → How does it handle unusual inputs?\nError recovery  → What happens when tools fail?\nAdversarial     → Can users manipulate or confuse it?\nConsistency     → Same input 5 times = same quality?\n</code></pre>\n<p><strong>Step 4: Measure and iterate</strong>\nTrack metrics over time. Improvements to prompts, tools, or orchestration should show up in your evaluation scores.</p>\n<h4>Evaluation Anti-Patterns</h4>\n<table>\n<thead>\n<tr>\n<th>Anti-Pattern</th>\n<th>Problem</th>\n<th>Better Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Vibes-based eval</strong></td>\n<td>&quot;It seems good&quot;</td>\n<td>Define specific metrics</td>\n</tr>\n<tr>\n<td><strong>Demo-only testing</strong></td>\n<td>Cherry-picked examples</td>\n<td>Random sampling from real usage</td>\n</tr>\n<tr>\n<td><strong>Single-run scoring</strong></td>\n<td>Ignores variance</td>\n<td>Multiple runs per test case</td>\n</tr>\n<tr>\n<td><strong>Output-only focus</strong></td>\n<td>Misses process issues</td>\n<td>Evaluate reasoning and tool use</td>\n</tr>\n<tr>\n<td><strong>Static test sets</strong></td>\n<td>Miss evolving issues</td>\n<td>Add failing cases to test set</td>\n</tr>\n<tr>\n<td><strong>Human-only review</strong></td>\n<td>Doesn&#39;t scale</td>\n<td>Combine automated + human eval</td>\n</tr>\n</tbody></table>\n<h4>LLM-as-Judge Pattern</h4>\n<p>Use AI to evaluate AI—with careful design:</p>\n<pre><code>EVALUATOR PROMPT STRUCTURE\n├── Clear criteria: Exactly what to evaluate\n├── Rubric: Specific scoring guidelines (1-5 scale)\n├── Examples: What each score looks like\n├── Output format: Structured (JSON) for parsing\n└── Reasoning: Ask for explanation before score\n</code></pre>\n<p><strong>Example rubric prompt:</strong></p>\n<blockquote>\n<p>Rate this customer support response on a 1-5 scale:</p>\n<p>5: Fully addresses query, accurate, helpful, appropriate tone\n4: Addresses query with minor gaps, mostly accurate\n3: Partially addresses query, some inaccuracies\n2: Misses key points, notable errors\n1: Incorrect, unhelpful, or inappropriate</p>\n<p>First explain your reasoning, then provide score as JSON: {&quot;score&quot;: N}</p>\n</blockquote>\n<p><strong>Caution</strong>: LLM evaluators have biases (prefer verbose answers, struggle with domain expertise). Calibrate against human judgment.</p>\n<h3>Pre-Launch Checklist</h3>\n<h3>Deployment Approaches</h3>\n<p><strong>Approach 1: Shadow Mode</strong></p>\n<pre><code>User request → Current system responds\n            ↘ AI system also runs (not shown to user)\n            → Compare outputs, evaluate AI readiness\n</code></pre>\n<p><strong>Approach 2: Limited Rollout</strong></p>\n<pre><code>Week 1: 5% of requests → AI\nWeek 2: 25% of requests → AI (if Week 1 OK)\nWeek 3: 100% of requests → AI (if Week 2 OK)\n</code></pre>\n<p><strong>Approach 3: Human-in-the-Loop</strong></p>\n<pre><code>AI generates → Human reviews → Approved outputs go live\n            → Rejected outputs inform improvements\n</code></pre>\n<p><strong>Approach 4: Internal First</strong></p>\n<pre><code>Internal users test → Feedback incorporated → External users\n</code></pre>\n<h3>Monitoring in Production</h3>\n<table>\n<thead>\n<tr>\n<th>What to Monitor</th>\n<th>Why</th>\n<th>Red Flags</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Response quality</strong></td>\n<td>Catch degradation</td>\n<td>Complaints, low ratings</td>\n</tr>\n<tr>\n<td><strong>Latency</strong></td>\n<td>User experience</td>\n<td>&gt;5s response times</td>\n</tr>\n<tr>\n<td><strong>Error rate</strong></td>\n<td>System health</td>\n<td>&gt;1% API errors</td>\n</tr>\n<tr>\n<td><strong>Token usage</strong></td>\n<td>Cost control</td>\n<td>Unexpected spikes</td>\n</tr>\n<tr>\n<td><strong>Escalation rate</strong></td>\n<td>AI capability</td>\n<td>Rising escalations</td>\n</tr>\n<tr>\n<td><strong>User satisfaction</strong></td>\n<td>Overall success</td>\n<td>Declining feedback</td>\n</tr>\n</tbody></table>\n<h3>Iterative Improvement</h3>\n<p>After launch, establish feedback loops:</p>\n<pre><code>Production usage\n      ↓\nCollect feedback (automated + manual)\n      ↓\nIdentify improvement opportunities\n      ↓\nUpdate prompts / configuration\n      ↓\nTest changes\n      ↓\nDeploy updates\n      ↓\n[Repeat]\n</code></pre>\n<hr>\n"
        },
        {
          "id": "how-to-apply-this",
          "title": "HOW to Apply This",
          "type": "how",
          "content": "### Exercise: Create a Test Plan\n\n### Testing Template\n\n```\nTEST PLAN: [Application Name]\n\n1. GOLDEN DATASET\n| ID | Input | Expected Output | Pass Criteria |\n|----|-------|-----------------|---------------|\n| G1 | ... | ... | ... |\n| G2 | ... | ... | ... |\n\n2. PERSONA TESTS\n| Persona | Scenario | Expected Behavior |\n|---------|----------|-------------------|\n| P1 | ... | ... |\n| P2 | ... | ... |\n\n3. FAILURE MODE TESTS\n| Mode | Input | Expected Handling |\n|------|-------|-------------------|\n| F1 | ... | ... |\n| F2 | ... | ... |\n\n4. SUCCESS METRICS\n| Metric | Target | Measurement |\n|--------|--------|-------------|\n| ... | ... | ... |\n\n5. DEPLOYMENT STAGES\n| Stage | Users | Duration | Success Gate |\n|-------|-------|----------|--------------|\n| ... | ... | ... | ... |\n```\n\n### Common Launch Failures\n\n| Failure | Cause | Prevention |\n|---------|-------|------------|\n| Quality disaster | Insufficient testing | Comprehensive test plan |\n| Cost overrun | Token usage underestimated | Load testing, cost projections |\n| User confusion | Poor UX | Beta testing with real users |\n| Security incident | Prompt injection | Security testing |\n| Performance issues | Scale not considered | Load testing |\n| Rollback chaos | No plan | Document rollback procedure |\n\n### Self-Check\n\n---",
          "htmlContent": "<h3>Exercise: Create a Test Plan</h3>\n<h3>Testing Template</h3>\n<pre><code>TEST PLAN: [Application Name]\n\n1. GOLDEN DATASET\n| ID | Input | Expected Output | Pass Criteria |\n|----|-------|-----------------|---------------|\n| G1 | ... | ... | ... |\n| G2 | ... | ... | ... |\n\n2. PERSONA TESTS\n| Persona | Scenario | Expected Behavior |\n|---------|----------|-------------------|\n| P1 | ... | ... |\n| P2 | ... | ... |\n\n3. FAILURE MODE TESTS\n| Mode | Input | Expected Handling |\n|------|-------|-------------------|\n| F1 | ... | ... |\n| F2 | ... | ... |\n\n4. SUCCESS METRICS\n| Metric | Target | Measurement |\n|--------|--------|-------------|\n| ... | ... | ... |\n\n5. DEPLOYMENT STAGES\n| Stage | Users | Duration | Success Gate |\n|-------|-------|----------|--------------|\n| ... | ... | ... | ... |\n</code></pre>\n<h3>Common Launch Failures</h3>\n<table>\n<thead>\n<tr>\n<th>Failure</th>\n<th>Cause</th>\n<th>Prevention</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Quality disaster</td>\n<td>Insufficient testing</td>\n<td>Comprehensive test plan</td>\n</tr>\n<tr>\n<td>Cost overrun</td>\n<td>Token usage underestimated</td>\n<td>Load testing, cost projections</td>\n</tr>\n<tr>\n<td>User confusion</td>\n<td>Poor UX</td>\n<td>Beta testing with real users</td>\n</tr>\n<tr>\n<td>Security incident</td>\n<td>Prompt injection</td>\n<td>Security testing</td>\n</tr>\n<tr>\n<td>Performance issues</td>\n<td>Scale not considered</td>\n<td>Load testing</td>\n</tr>\n<tr>\n<td>Rollback chaos</td>\n<td>No plan</td>\n<td>Document rollback procedure</td>\n</tr>\n</tbody></table>\n<h3>Self-Check</h3>\n<hr>\n"
        },
        {
          "id": "phase-3-complete!",
          "title": "Phase 3 Complete!",
          "type": "generic",
          "content": "You've built your Implementation skills. Before moving to Phase 4, complete:\n\n**Lab 5: Build an AI Assistant** — Create a functional AI assistant using no-code tools\n\n**Phase 3 Deliverable: Working AI Application** — Build and deploy a functional AI-powered tool that solves a real business problem",
          "htmlContent": "<p>You&#39;ve built your Implementation skills. Before moving to Phase 4, complete:</p>\n<p><strong>Lab 5: Build an AI Assistant</strong> — Create a functional AI assistant using no-code tools</p>\n<p><strong>Phase 3 Deliverable: Working AI Application</strong> — Build and deploy a functional AI-powered tool that solves a real business problem</p>\n"
        }
      ],
      "concepts": [
        {
          "id": "ai-testing",
          "term": "ai testing",
          "definition": "AI testing differs from traditional software testing:\n\n**Traditional software:**\n- Given input X, always produces output Y\n- Bugs are deterministic\n- Edge cases are finite\n\n**AI applications:**\n- Same input may produce different outputs\n- Failures may be subtle (wrong tone, missed nuance)\n- Edge cases are infinite\n- Quality is often subjective",
          "htmlDefinition": "<p>AI testing differs from traditional software testing:</p>\n<p><strong>Traditional software:</strong></p>\n<ul>\n<li>Given input X, always produces output Y</li>\n<li>Bugs are deterministic</li>\n<li>Edge cases are finite</li>\n</ul>\n<p><strong>AI applications:</strong></p>\n<ul>\n<li>Same input may produce different outputs</li>\n<li>Failures may be subtle (wrong tone, missed nuance)</li>\n<li>Edge cases are infinite</li>\n<li>Quality is often subjective</li>\n</ul>\n"
        },
        {
          "id": "agent-evaluation",
          "term": "agent evaluation",
          "definition": "**Agent evaluation** requires testing not just what the agent produces, but HOW it works:\n- Did it choose the right tools?\n- Was its reasoning sound?\n- Did it stay within boundaries?\n- Can it recover from errors?\n- Is it consistent across runs?",
          "htmlDefinition": "<p><strong>Agent evaluation</strong> requires testing not just what the agent produces, but HOW it works:</p>\n<ul>\n<li>Did it choose the right tools?</li>\n<li>Was its reasoning sound?</li>\n<li>Did it stay within boundaries?</li>\n<li>Can it recover from errors?</li>\n<li>Is it consistent across runs?</li>\n</ul>\n"
        },
        {
          "id": "eval-benchmarks",
          "term": "eval benchmarks",
          "definition": "**Industry Benchmarks for AI Agents:**\n\n| Benchmark | Focus | What It Tests | Current Best Performance |\n|-----------|-------|---------------|-------------------------|\n| **AgentBench** | General capability | 8 environments (OS, web, database) | ~45% average (humans: 80%+) |\n| **WebArena** | Web navigation | 812 realistic web tasks | ~35-60% (humans: 78%) |\n| **GAIA** | Real-world reasoning | 466 multi-step problems | Varies by difficulty level |\n| **ToolBench** | Tool calling | 16,000+ tools, 49 categories | ~70% for best models |\n| **BFCL** | Function calling | Correct API invocation | ~90% for top models |\n\n**Key insight**: Even the best models struggle with multi-step, real-world tasks. Set expectations accordingly.",
          "htmlDefinition": "<p><strong>Industry Benchmarks for AI Agents:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>Focus</th>\n<th>What It Tests</th>\n<th>Current Best Performance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>AgentBench</strong></td>\n<td>General capability</td>\n<td>8 environments (OS, web, database)</td>\n<td>~45% average (humans: 80%+)</td>\n</tr>\n<tr>\n<td><strong>WebArena</strong></td>\n<td>Web navigation</td>\n<td>812 realistic web tasks</td>\n<td>~35-60% (humans: 78%)</td>\n</tr>\n<tr>\n<td><strong>GAIA</strong></td>\n<td>Real-world reasoning</td>\n<td>466 multi-step problems</td>\n<td>Varies by difficulty level</td>\n</tr>\n<tr>\n<td><strong>ToolBench</strong></td>\n<td>Tool calling</td>\n<td>16,000+ tools, 49 categories</td>\n<td>~70% for best models</td>\n</tr>\n<tr>\n<td><strong>BFCL</strong></td>\n<td>Function calling</td>\n<td>Correct API invocation</td>\n<td>~90% for top models</td>\n</tr>\n</tbody></table>\n<p><strong>Key insight</strong>: Even the best models struggle with multi-step, real-world tasks. Set expectations accordingly.</p>\n"
        },
        {
          "id": "pre-launch",
          "term": "pre launch",
          "definition": "Before deploying an AI application:\n\n**Functional checks:**\n- [ ] Core use cases work correctly\n- [ ] Edge cases handled gracefully\n- [ ] Error messages are helpful\n- [ ] Performance is acceptable\n\n**Quality checks:**\n- [ ] Output quality meets standards\n- [ ] Tone and voice are appropriate\n- [ ] No hallucinations in test set\n- [ ] Sensitive topics handled correctly\n\n**Safety checks:**\n- [ ] Prompt injection tested\n- [ ] Harmful request handling verified\n- [ ] PII handling appropriate\n- [ ] Escalation paths work\n\n**Operational checks:**\n- [ ] API keys secured\n- [ ] Rate limits understood\n- [ ] Costs projected\n- [ ] Monitoring in place",
          "htmlDefinition": "<p>Before deploying an AI application:</p>\n<p><strong>Functional checks:</strong></p>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> Core use cases work correctly</li>\n<li><input disabled=\"\" type=\"checkbox\"> Edge cases handled gracefully</li>\n<li><input disabled=\"\" type=\"checkbox\"> Error messages are helpful</li>\n<li><input disabled=\"\" type=\"checkbox\"> Performance is acceptable</li>\n</ul>\n<p><strong>Quality checks:</strong></p>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> Output quality meets standards</li>\n<li><input disabled=\"\" type=\"checkbox\"> Tone and voice are appropriate</li>\n<li><input disabled=\"\" type=\"checkbox\"> No hallucinations in test set</li>\n<li><input disabled=\"\" type=\"checkbox\"> Sensitive topics handled correctly</li>\n</ul>\n<p><strong>Safety checks:</strong></p>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> Prompt injection tested</li>\n<li><input disabled=\"\" type=\"checkbox\"> Harmful request handling verified</li>\n<li><input disabled=\"\" type=\"checkbox\"> PII handling appropriate</li>\n<li><input disabled=\"\" type=\"checkbox\"> Escalation paths work</li>\n</ul>\n<p><strong>Operational checks:</strong></p>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> API keys secured</li>\n<li><input disabled=\"\" type=\"checkbox\"> Rate limits understood</li>\n<li><input disabled=\"\" type=\"checkbox\"> Costs projected</li>\n<li><input disabled=\"\" type=\"checkbox\"> Monitoring in place</li>\n</ul>\n"
        }
      ],
      "exercises": [
        {
          "id": "test-plan",
          "title": "Scenario",
          "instructions": "You've built an AI assistant that helps employees draft expense report justifications. Given expense details, it generates professional justifications suitable for finance approval.\n\n**Create a comprehensive test plan:**\n\n**1. Golden dataset (5 cases):**\nWrite 5 test inputs and expected outputs:\n- Routine expense (lunch with client)\n- Large purchase (conference tickets)\n- Unusual expense (thank you gift for vendor)\n- Ambiguous expense (software subscription)\n- Edge case (personal expense mistakenly submitted)\n\n**2. Persona tests:**\nHow should the system respond to:\n- New employee unfamiliar with policies?\n- Executive with vague expense descriptions?\n- Someone trying to justify personal expenses?\n\n**3. Failure mode tests:**\nWhat happens with:\n- Expense over policy limits?\n- Missing required information?\n- Potentially fraudulent patterns?\n\n**4. Success criteria:**\nHow will you measure if the system is working?\n- Quality metrics\n- Operational metrics\n- User satisfaction metrics\n\n**5. Deployment plan:**\nHow would you roll this out?\n- Which users first?\n- What checkpoints?\n- Rollback criteria?",
          "htmlInstructions": "<p>You&#39;ve built an AI assistant that helps employees draft expense report justifications. Given expense details, it generates professional justifications suitable for finance approval.</p>\n<p><strong>Create a comprehensive test plan:</strong></p>\n<p><strong>1. Golden dataset (5 cases):</strong>\nWrite 5 test inputs and expected outputs:</p>\n<ul>\n<li>Routine expense (lunch with client)</li>\n<li>Large purchase (conference tickets)</li>\n<li>Unusual expense (thank you gift for vendor)</li>\n<li>Ambiguous expense (software subscription)</li>\n<li>Edge case (personal expense mistakenly submitted)</li>\n</ul>\n<p><strong>2. Persona tests:</strong>\nHow should the system respond to:</p>\n<ul>\n<li>New employee unfamiliar with policies?</li>\n<li>Executive with vague expense descriptions?</li>\n<li>Someone trying to justify personal expenses?</li>\n</ul>\n<p><strong>3. Failure mode tests:</strong>\nWhat happens with:</p>\n<ul>\n<li>Expense over policy limits?</li>\n<li>Missing required information?</li>\n<li>Potentially fraudulent patterns?</li>\n</ul>\n<p><strong>4. Success criteria:</strong>\nHow will you measure if the system is working?</p>\n<ul>\n<li>Quality metrics</li>\n<li>Operational metrics</li>\n<li>User satisfaction metrics</li>\n</ul>\n<p><strong>5. Deployment plan:</strong>\nHow would you roll this out?</p>\n<ul>\n<li>Which users first?</li>\n<li>What checkpoints?</li>\n<li>Rollback criteria?</li>\n</ul>\n"
        }
      ],
      "checklists": [
        {
          "id": "module-3.4-complete",
          "items": [
            {
              "id": "module-3.4-complete-0",
              "text": "I understand AI-specific testing challenges",
              "completed": false
            },
            {
              "id": "module-3.4-complete-1",
              "text": "I can create golden datasets for testing",
              "completed": false
            },
            {
              "id": "module-3.4-complete-2",
              "text": "I know the six dimensions of agent evaluation",
              "completed": false
            },
            {
              "id": "module-3.4-complete-3",
              "text": "I can design evaluations for my specific use case",
              "completed": false
            },
            {
              "id": "module-3.4-complete-4",
              "text": "I know multiple deployment approaches",
              "completed": false
            },
            {
              "id": "module-3.4-complete-5",
              "text": "I can set up production monitoring",
              "completed": false
            },
            {
              "id": "module-3.4-complete-6",
              "text": "I understand iterative improvement cycles",
              "completed": false
            }
          ]
        }
      ]
    },
    {
      "id": "3.5-customization-fine-tuning",
      "slug": "3.5-customization-fine-tuning",
      "title": "Customization & Fine-Tuning",
      "phase": 3,
      "module": 5,
      "phaseId": "phase-3",
      "estimatedMinutes": 45,
      "bloomLevel": "evaluate",
      "content": "# Customization & Fine-Tuning\n\n## WHY This Matters\n\nOut-of-the-box AI models are generalists. They know a little about everything but may not know *your* business—your terminology, your products, your processes, your voice.\n\nCustomization makes AI speak your language. But it comes at a cost.\n\nThe question isn't *whether* to customize—it's *when* the investment pays off.\n\n**The Business Operator's decision:**\n- When do I need custom AI behavior?\n- What's the most cost-effective way to achieve it?\n- How do I avoid over-engineering a simple problem?\n\n---\n\n## WHAT You Need to Know\n\n### The Customization Spectrum\n\n### RAG: Retrieval-Augmented Generation\n\n### RAG Economics\n\n| Cost Component | Typical Range | Notes |\n|----------------|---------------|-------|\n| **Embedding generation** | $0.0001/1K tokens | One-time per document |\n| **Vector database** | $0-100/month | Pinecone, Weaviate, Supabase |\n| **Retrieval per query** | ~$0.0002 | Minimal compute |\n| **Augmented prompt tokens** | +500-2000 tokens | Added context increases cost |\n\n**Example RAG cost calculation:**\n- 10,000 support documents → $10 to embed (one-time)\n- Vector DB hosting → $25/month\n- 50,000 queries/month → Extra $50 in prompt tokens\n- **Total: ~$75/month + $10 setup**\n\nCompare to: Hiring someone to answer those questions\n\n### Fine-Tuning: Teaching New Behaviors\n\n### Fine-Tuning Economics\n\n| Cost Component | OpenAI GPT-4o-mini | OpenAI GPT-4o |\n|----------------|-------------------|---------------|\n| **Training cost** | $3/million tokens | $25/million tokens |\n| **Inference cost** | 2x base model | 2x base model |\n| **Minimum examples** | 10 (50+ recommended) | 10 (50+ recommended) |\n| **Training time** | Minutes to hours | Hours |\n\n**Example fine-tuning cost calculation:**\n- 500 training examples × 500 tokens each = 250K tokens\n- Training cost: ~$0.75 (GPT-4o-mini)\n- Inference: 2x normal costs ongoing\n- Data preparation: 5-20 hours human time\n- **Total: ~$1 compute + significant human investment**\n\n**The hidden cost:** Creating high-quality training data requires expert time. If you need 500 examples of perfect customer support responses, someone has to write them.\n\n### The Decision Framework\n\n### Hybrid Approaches\n\nMost production systems use multiple approaches:\n\n```\nUser query\n     ↓\n[RAG] Retrieve relevant knowledge from your docs\n     ↓\n[Fine-tuned model] Generate response in your brand voice\n     ↓\n[Prompt guard] Ensure output meets formatting requirements\n     ↓\nResponse to user\n```\n\n**Example: Customer Support Bot**\n- RAG: Product specs, policies, FAQs\n- Fine-tuning: Brand voice and escalation behavior\n- Prompts: Output formatting and safety guardrails\n\n### Build vs. Buy for Customization\n\n| Approach | Build In-House | Use Managed Service |\n|----------|----------------|---------------------|\n| **RAG** | Pinecone + custom pipeline | LangChain, Anthropic Claude Projects |\n| **Fine-tuning** | OpenAI API + data prep | Jasper, Copy.ai (domain-specific) |\n| **Full custom** | Azure OpenAI + enterprise | Specialized vendors |\n\n**When to buy:**\n- Speed to market critical\n- No in-house ML expertise\n- Vendor has domain knowledge you lack\n\n**When to build:**\n- Competitive advantage from customization\n- Data sensitivity requires control\n- Long-term cost optimization at scale\n\n---\n\n## HOW to Apply This\n\n### Exercise: Customization Decision Matrix\n\n### Customization ROI Calculator\n\n```\nCUSTOMIZATION ROI WORKSHEET\n\n1. CURRENT STATE\n   Manual handling time per task: ___ minutes\n   Tasks per month: ___\n   Loaded labor cost per hour: $___\n   Monthly labor cost: $___\n\n2. WITH AI (BASIC PROMPTING)\n   Accuracy rate (usable without edits): ___%\n   Time saved per usable task: ___ minutes\n   Monthly time saved: ___ hours\n   Monthly value of time saved: $___\n\n3. WITH CUSTOMIZATION\n   Expected accuracy improvement: +___%\n   Additional monthly value: $___\n\n4. CUSTOMIZATION COSTS\n   One-time setup:\n   - RAG infrastructure: $___\n   - Fine-tuning training: $___\n   - Human data prep time: $___\n   Ongoing monthly:\n   - Compute increase: $___\n   - Maintenance: $___\n\n5. PAYBACK CALCULATION\n   Incremental monthly value: $___\n   Incremental monthly cost: $___\n   Net monthly benefit: $___\n   One-time investment: $___\n   Payback period: ___ months\n```\n\n### When to Say No\n\n### Self-Check\n\n---\n\n## Phase 3 Complete!\n\nYou've mastered Agentic Orchestration. You can now:\n- Evaluate and select AI tools\n- Work with APIs and automation platforms\n- Test and deploy AI applications responsibly\n- Make informed customization decisions\n\nBefore moving to Phase 4, complete:\n\n**Lab 5: Build an AI Assistant** — Create a functional AI assistant using no-code tools\n\n**Lab 5b: Multi-Agent Orchestration** — Design a system where AI agents collaborate\n\n**Phase 3 Deliverable: Multi-Agent System** — Build and deploy a working multi-agent system that demonstrates autonomous reasoning and collaboration",
      "htmlContent": "<h1>Customization &amp; Fine-Tuning</h1>\n<h2>WHY This Matters</h2>\n<p>Out-of-the-box AI models are generalists. They know a little about everything but may not know <em>your</em> business—your terminology, your products, your processes, your voice.</p>\n<p>Customization makes AI speak your language. But it comes at a cost.</p>\n<p>The question isn&#39;t <em>whether</em> to customize—it&#39;s <em>when</em> the investment pays off.</p>\n<p><strong>The Business Operator&#39;s decision:</strong></p>\n<ul>\n<li>When do I need custom AI behavior?</li>\n<li>What&#39;s the most cost-effective way to achieve it?</li>\n<li>How do I avoid over-engineering a simple problem?</li>\n</ul>\n<hr>\n<h2>WHAT You Need to Know</h2>\n<h3>The Customization Spectrum</h3>\n<h3>RAG: Retrieval-Augmented Generation</h3>\n<h3>RAG Economics</h3>\n<table>\n<thead>\n<tr>\n<th>Cost Component</th>\n<th>Typical Range</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Embedding generation</strong></td>\n<td>$0.0001/1K tokens</td>\n<td>One-time per document</td>\n</tr>\n<tr>\n<td><strong>Vector database</strong></td>\n<td>$0-100/month</td>\n<td>Pinecone, Weaviate, Supabase</td>\n</tr>\n<tr>\n<td><strong>Retrieval per query</strong></td>\n<td>~$0.0002</td>\n<td>Minimal compute</td>\n</tr>\n<tr>\n<td><strong>Augmented prompt tokens</strong></td>\n<td>+500-2000 tokens</td>\n<td>Added context increases cost</td>\n</tr>\n</tbody></table>\n<p><strong>Example RAG cost calculation:</strong></p>\n<ul>\n<li>10,000 support documents → $10 to embed (one-time)</li>\n<li>Vector DB hosting → $25/month</li>\n<li>50,000 queries/month → Extra $50 in prompt tokens</li>\n<li><strong>Total: ~$75/month + $10 setup</strong></li>\n</ul>\n<p>Compare to: Hiring someone to answer those questions</p>\n<h3>Fine-Tuning: Teaching New Behaviors</h3>\n<h3>Fine-Tuning Economics</h3>\n<table>\n<thead>\n<tr>\n<th>Cost Component</th>\n<th>OpenAI GPT-4o-mini</th>\n<th>OpenAI GPT-4o</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Training cost</strong></td>\n<td>$3/million tokens</td>\n<td>$25/million tokens</td>\n</tr>\n<tr>\n<td><strong>Inference cost</strong></td>\n<td>2x base model</td>\n<td>2x base model</td>\n</tr>\n<tr>\n<td><strong>Minimum examples</strong></td>\n<td>10 (50+ recommended)</td>\n<td>10 (50+ recommended)</td>\n</tr>\n<tr>\n<td><strong>Training time</strong></td>\n<td>Minutes to hours</td>\n<td>Hours</td>\n</tr>\n</tbody></table>\n<p><strong>Example fine-tuning cost calculation:</strong></p>\n<ul>\n<li>500 training examples × 500 tokens each = 250K tokens</li>\n<li>Training cost: ~$0.75 (GPT-4o-mini)</li>\n<li>Inference: 2x normal costs ongoing</li>\n<li>Data preparation: 5-20 hours human time</li>\n<li><strong>Total: ~$1 compute + significant human investment</strong></li>\n</ul>\n<p><strong>The hidden cost:</strong> Creating high-quality training data requires expert time. If you need 500 examples of perfect customer support responses, someone has to write them.</p>\n<h3>The Decision Framework</h3>\n<h3>Hybrid Approaches</h3>\n<p>Most production systems use multiple approaches:</p>\n<pre><code>User query\n     ↓\n[RAG] Retrieve relevant knowledge from your docs\n     ↓\n[Fine-tuned model] Generate response in your brand voice\n     ↓\n[Prompt guard] Ensure output meets formatting requirements\n     ↓\nResponse to user\n</code></pre>\n<p><strong>Example: Customer Support Bot</strong></p>\n<ul>\n<li>RAG: Product specs, policies, FAQs</li>\n<li>Fine-tuning: Brand voice and escalation behavior</li>\n<li>Prompts: Output formatting and safety guardrails</li>\n</ul>\n<h3>Build vs. Buy for Customization</h3>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Build In-House</th>\n<th>Use Managed Service</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>RAG</strong></td>\n<td>Pinecone + custom pipeline</td>\n<td>LangChain, Anthropic Claude Projects</td>\n</tr>\n<tr>\n<td><strong>Fine-tuning</strong></td>\n<td>OpenAI API + data prep</td>\n<td>Jasper, Copy.ai (domain-specific)</td>\n</tr>\n<tr>\n<td><strong>Full custom</strong></td>\n<td>Azure OpenAI + enterprise</td>\n<td>Specialized vendors</td>\n</tr>\n</tbody></table>\n<p><strong>When to buy:</strong></p>\n<ul>\n<li>Speed to market critical</li>\n<li>No in-house ML expertise</li>\n<li>Vendor has domain knowledge you lack</li>\n</ul>\n<p><strong>When to build:</strong></p>\n<ul>\n<li>Competitive advantage from customization</li>\n<li>Data sensitivity requires control</li>\n<li>Long-term cost optimization at scale</li>\n</ul>\n<hr>\n<h2>HOW to Apply This</h2>\n<h3>Exercise: Customization Decision Matrix</h3>\n<h3>Customization ROI Calculator</h3>\n<pre><code>CUSTOMIZATION ROI WORKSHEET\n\n1. CURRENT STATE\n   Manual handling time per task: ___ minutes\n   Tasks per month: ___\n   Loaded labor cost per hour: $___\n   Monthly labor cost: $___\n\n2. WITH AI (BASIC PROMPTING)\n   Accuracy rate (usable without edits): ___%\n   Time saved per usable task: ___ minutes\n   Monthly time saved: ___ hours\n   Monthly value of time saved: $___\n\n3. WITH CUSTOMIZATION\n   Expected accuracy improvement: +___%\n   Additional monthly value: $___\n\n4. CUSTOMIZATION COSTS\n   One-time setup:\n   - RAG infrastructure: $___\n   - Fine-tuning training: $___\n   - Human data prep time: $___\n   Ongoing monthly:\n   - Compute increase: $___\n   - Maintenance: $___\n\n5. PAYBACK CALCULATION\n   Incremental monthly value: $___\n   Incremental monthly cost: $___\n   Net monthly benefit: $___\n   One-time investment: $___\n   Payback period: ___ months\n</code></pre>\n<h3>When to Say No</h3>\n<h3>Self-Check</h3>\n<hr>\n<h2>Phase 3 Complete!</h2>\n<p>You&#39;ve mastered Agentic Orchestration. You can now:</p>\n<ul>\n<li>Evaluate and select AI tools</li>\n<li>Work with APIs and automation platforms</li>\n<li>Test and deploy AI applications responsibly</li>\n<li>Make informed customization decisions</li>\n</ul>\n<p>Before moving to Phase 4, complete:</p>\n<p><strong>Lab 5: Build an AI Assistant</strong> — Create a functional AI assistant using no-code tools</p>\n<p><strong>Lab 5b: Multi-Agent Orchestration</strong> — Design a system where AI agents collaborate</p>\n<p><strong>Phase 3 Deliverable: Multi-Agent System</strong> — Build and deploy a working multi-agent system that demonstrates autonomous reasoning and collaboration</p>\n",
      "sections": [
        {
          "id": "why-this-matters",
          "title": "WHY This Matters",
          "type": "why",
          "content": "Out-of-the-box AI models are generalists. They know a little about everything but may not know *your* business—your terminology, your products, your processes, your voice.\n\nCustomization makes AI speak your language. But it comes at a cost.\n\nThe question isn't *whether* to customize—it's *when* the investment pays off.\n\n**The Business Operator's decision:**\n- When do I need custom AI behavior?\n- What's the most cost-effective way to achieve it?\n- How do I avoid over-engineering a simple problem?\n\n---",
          "htmlContent": "<p>Out-of-the-box AI models are generalists. They know a little about everything but may not know <em>your</em> business—your terminology, your products, your processes, your voice.</p>\n<p>Customization makes AI speak your language. But it comes at a cost.</p>\n<p>The question isn&#39;t <em>whether</em> to customize—it&#39;s <em>when</em> the investment pays off.</p>\n<p><strong>The Business Operator&#39;s decision:</strong></p>\n<ul>\n<li>When do I need custom AI behavior?</li>\n<li>What&#39;s the most cost-effective way to achieve it?</li>\n<li>How do I avoid over-engineering a simple problem?</li>\n</ul>\n<hr>\n"
        },
        {
          "id": "what-you-need-to-know",
          "title": "WHAT You Need to Know",
          "type": "what",
          "content": "### The Customization Spectrum\n\n### RAG: Retrieval-Augmented Generation\n\n### RAG Economics\n\n| Cost Component | Typical Range | Notes |\n|----------------|---------------|-------|\n| **Embedding generation** | $0.0001/1K tokens | One-time per document |\n| **Vector database** | $0-100/month | Pinecone, Weaviate, Supabase |\n| **Retrieval per query** | ~$0.0002 | Minimal compute |\n| **Augmented prompt tokens** | +500-2000 tokens | Added context increases cost |\n\n**Example RAG cost calculation:**\n- 10,000 support documents → $10 to embed (one-time)\n- Vector DB hosting → $25/month\n- 50,000 queries/month → Extra $50 in prompt tokens\n- **Total: ~$75/month + $10 setup**\n\nCompare to: Hiring someone to answer those questions\n\n### Fine-Tuning: Teaching New Behaviors\n\n### Fine-Tuning Economics\n\n| Cost Component | OpenAI GPT-4o-mini | OpenAI GPT-4o |\n|----------------|-------------------|---------------|\n| **Training cost** | $3/million tokens | $25/million tokens |\n| **Inference cost** | 2x base model | 2x base model |\n| **Minimum examples** | 10 (50+ recommended) | 10 (50+ recommended) |\n| **Training time** | Minutes to hours | Hours |\n\n**Example fine-tuning cost calculation:**\n- 500 training examples × 500 tokens each = 250K tokens\n- Training cost: ~$0.75 (GPT-4o-mini)\n- Inference: 2x normal costs ongoing\n- Data preparation: 5-20 hours human time\n- **Total: ~$1 compute + significant human investment**\n\n**The hidden cost:** Creating high-quality training data requires expert time. If you need 500 examples of perfect customer support responses, someone has to write them.\n\n### The Decision Framework\n\n### Hybrid Approaches\n\nMost production systems use multiple approaches:\n\n```\nUser query\n     ↓\n[RAG] Retrieve relevant knowledge from your docs\n     ↓\n[Fine-tuned model] Generate response in your brand voice\n     ↓\n[Prompt guard] Ensure output meets formatting requirements\n     ↓\nResponse to user\n```\n\n**Example: Customer Support Bot**\n- RAG: Product specs, policies, FAQs\n- Fine-tuning: Brand voice and escalation behavior\n- Prompts: Output formatting and safety guardrails\n\n### Build vs. Buy for Customization\n\n| Approach | Build In-House | Use Managed Service |\n|----------|----------------|---------------------|\n| **RAG** | Pinecone + custom pipeline | LangChain, Anthropic Claude Projects |\n| **Fine-tuning** | OpenAI API + data prep | Jasper, Copy.ai (domain-specific) |\n| **Full custom** | Azure OpenAI + enterprise | Specialized vendors |\n\n**When to buy:**\n- Speed to market critical\n- No in-house ML expertise\n- Vendor has domain knowledge you lack\n\n**When to build:**\n- Competitive advantage from customization\n- Data sensitivity requires control\n- Long-term cost optimization at scale\n\n---",
          "htmlContent": "<h3>The Customization Spectrum</h3>\n<h3>RAG: Retrieval-Augmented Generation</h3>\n<h3>RAG Economics</h3>\n<table>\n<thead>\n<tr>\n<th>Cost Component</th>\n<th>Typical Range</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Embedding generation</strong></td>\n<td>$0.0001/1K tokens</td>\n<td>One-time per document</td>\n</tr>\n<tr>\n<td><strong>Vector database</strong></td>\n<td>$0-100/month</td>\n<td>Pinecone, Weaviate, Supabase</td>\n</tr>\n<tr>\n<td><strong>Retrieval per query</strong></td>\n<td>~$0.0002</td>\n<td>Minimal compute</td>\n</tr>\n<tr>\n<td><strong>Augmented prompt tokens</strong></td>\n<td>+500-2000 tokens</td>\n<td>Added context increases cost</td>\n</tr>\n</tbody></table>\n<p><strong>Example RAG cost calculation:</strong></p>\n<ul>\n<li>10,000 support documents → $10 to embed (one-time)</li>\n<li>Vector DB hosting → $25/month</li>\n<li>50,000 queries/month → Extra $50 in prompt tokens</li>\n<li><strong>Total: ~$75/month + $10 setup</strong></li>\n</ul>\n<p>Compare to: Hiring someone to answer those questions</p>\n<h3>Fine-Tuning: Teaching New Behaviors</h3>\n<h3>Fine-Tuning Economics</h3>\n<table>\n<thead>\n<tr>\n<th>Cost Component</th>\n<th>OpenAI GPT-4o-mini</th>\n<th>OpenAI GPT-4o</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Training cost</strong></td>\n<td>$3/million tokens</td>\n<td>$25/million tokens</td>\n</tr>\n<tr>\n<td><strong>Inference cost</strong></td>\n<td>2x base model</td>\n<td>2x base model</td>\n</tr>\n<tr>\n<td><strong>Minimum examples</strong></td>\n<td>10 (50+ recommended)</td>\n<td>10 (50+ recommended)</td>\n</tr>\n<tr>\n<td><strong>Training time</strong></td>\n<td>Minutes to hours</td>\n<td>Hours</td>\n</tr>\n</tbody></table>\n<p><strong>Example fine-tuning cost calculation:</strong></p>\n<ul>\n<li>500 training examples × 500 tokens each = 250K tokens</li>\n<li>Training cost: ~$0.75 (GPT-4o-mini)</li>\n<li>Inference: 2x normal costs ongoing</li>\n<li>Data preparation: 5-20 hours human time</li>\n<li><strong>Total: ~$1 compute + significant human investment</strong></li>\n</ul>\n<p><strong>The hidden cost:</strong> Creating high-quality training data requires expert time. If you need 500 examples of perfect customer support responses, someone has to write them.</p>\n<h3>The Decision Framework</h3>\n<h3>Hybrid Approaches</h3>\n<p>Most production systems use multiple approaches:</p>\n<pre><code>User query\n     ↓\n[RAG] Retrieve relevant knowledge from your docs\n     ↓\n[Fine-tuned model] Generate response in your brand voice\n     ↓\n[Prompt guard] Ensure output meets formatting requirements\n     ↓\nResponse to user\n</code></pre>\n<p><strong>Example: Customer Support Bot</strong></p>\n<ul>\n<li>RAG: Product specs, policies, FAQs</li>\n<li>Fine-tuning: Brand voice and escalation behavior</li>\n<li>Prompts: Output formatting and safety guardrails</li>\n</ul>\n<h3>Build vs. Buy for Customization</h3>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Build In-House</th>\n<th>Use Managed Service</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>RAG</strong></td>\n<td>Pinecone + custom pipeline</td>\n<td>LangChain, Anthropic Claude Projects</td>\n</tr>\n<tr>\n<td><strong>Fine-tuning</strong></td>\n<td>OpenAI API + data prep</td>\n<td>Jasper, Copy.ai (domain-specific)</td>\n</tr>\n<tr>\n<td><strong>Full custom</strong></td>\n<td>Azure OpenAI + enterprise</td>\n<td>Specialized vendors</td>\n</tr>\n</tbody></table>\n<p><strong>When to buy:</strong></p>\n<ul>\n<li>Speed to market critical</li>\n<li>No in-house ML expertise</li>\n<li>Vendor has domain knowledge you lack</li>\n</ul>\n<p><strong>When to build:</strong></p>\n<ul>\n<li>Competitive advantage from customization</li>\n<li>Data sensitivity requires control</li>\n<li>Long-term cost optimization at scale</li>\n</ul>\n<hr>\n"
        },
        {
          "id": "how-to-apply-this",
          "title": "HOW to Apply This",
          "type": "how",
          "content": "### Exercise: Customization Decision Matrix\n\n### Customization ROI Calculator\n\n```\nCUSTOMIZATION ROI WORKSHEET\n\n1. CURRENT STATE\n   Manual handling time per task: ___ minutes\n   Tasks per month: ___\n   Loaded labor cost per hour: $___\n   Monthly labor cost: $___\n\n2. WITH AI (BASIC PROMPTING)\n   Accuracy rate (usable without edits): ___%\n   Time saved per usable task: ___ minutes\n   Monthly time saved: ___ hours\n   Monthly value of time saved: $___\n\n3. WITH CUSTOMIZATION\n   Expected accuracy improvement: +___%\n   Additional monthly value: $___\n\n4. CUSTOMIZATION COSTS\n   One-time setup:\n   - RAG infrastructure: $___\n   - Fine-tuning training: $___\n   - Human data prep time: $___\n   Ongoing monthly:\n   - Compute increase: $___\n   - Maintenance: $___\n\n5. PAYBACK CALCULATION\n   Incremental monthly value: $___\n   Incremental monthly cost: $___\n   Net monthly benefit: $___\n   One-time investment: $___\n   Payback period: ___ months\n```\n\n### When to Say No\n\n### Self-Check\n\n---",
          "htmlContent": "<h3>Exercise: Customization Decision Matrix</h3>\n<h3>Customization ROI Calculator</h3>\n<pre><code>CUSTOMIZATION ROI WORKSHEET\n\n1. CURRENT STATE\n   Manual handling time per task: ___ minutes\n   Tasks per month: ___\n   Loaded labor cost per hour: $___\n   Monthly labor cost: $___\n\n2. WITH AI (BASIC PROMPTING)\n   Accuracy rate (usable without edits): ___%\n   Time saved per usable task: ___ minutes\n   Monthly time saved: ___ hours\n   Monthly value of time saved: $___\n\n3. WITH CUSTOMIZATION\n   Expected accuracy improvement: +___%\n   Additional monthly value: $___\n\n4. CUSTOMIZATION COSTS\n   One-time setup:\n   - RAG infrastructure: $___\n   - Fine-tuning training: $___\n   - Human data prep time: $___\n   Ongoing monthly:\n   - Compute increase: $___\n   - Maintenance: $___\n\n5. PAYBACK CALCULATION\n   Incremental monthly value: $___\n   Incremental monthly cost: $___\n   Net monthly benefit: $___\n   One-time investment: $___\n   Payback period: ___ months\n</code></pre>\n<h3>When to Say No</h3>\n<h3>Self-Check</h3>\n<hr>\n"
        },
        {
          "id": "phase-3-complete!",
          "title": "Phase 3 Complete!",
          "type": "generic",
          "content": "You've mastered Agentic Orchestration. You can now:\n- Evaluate and select AI tools\n- Work with APIs and automation platforms\n- Test and deploy AI applications responsibly\n- Make informed customization decisions\n\nBefore moving to Phase 4, complete:\n\n**Lab 5: Build an AI Assistant** — Create a functional AI assistant using no-code tools\n\n**Lab 5b: Multi-Agent Orchestration** — Design a system where AI agents collaborate\n\n**Phase 3 Deliverable: Multi-Agent System** — Build and deploy a working multi-agent system that demonstrates autonomous reasoning and collaboration",
          "htmlContent": "<p>You&#39;ve mastered Agentic Orchestration. You can now:</p>\n<ul>\n<li>Evaluate and select AI tools</li>\n<li>Work with APIs and automation platforms</li>\n<li>Test and deploy AI applications responsibly</li>\n<li>Make informed customization decisions</li>\n</ul>\n<p>Before moving to Phase 4, complete:</p>\n<p><strong>Lab 5: Build an AI Assistant</strong> — Create a functional AI assistant using no-code tools</p>\n<p><strong>Lab 5b: Multi-Agent Orchestration</strong> — Design a system where AI agents collaborate</p>\n<p><strong>Phase 3 Deliverable: Multi-Agent System</strong> — Build and deploy a working multi-agent system that demonstrates autonomous reasoning and collaboration</p>\n"
        }
      ],
      "concepts": [
        {
          "id": "customization-spectrum",
          "term": "customization spectrum",
          "definition": "AI customization exists on a spectrum from cheap-and-quick to expensive-and-permanent:\n\n| Approach | Cost | Time | Flexibility | Best For |\n|----------|------|------|-------------|----------|\n| **Prompt Engineering** | $0 | Minutes | High (change anytime) | Most use cases |\n| **Few-Shot Examples** | $0 | Hours | High | Pattern matching, formatting |\n| **RAG (Retrieval)** | $-$$ | Days | Medium | Knowledge bases, docs |\n| **Fine-Tuning** | $$-$$$ | Weeks | Low (retraining needed) | Style, specialized domains |\n| **Custom Training** | $$$$ | Months | Very Low | Unique capabilities |\n\n**Key insight:** Most projects should start at the top of this spectrum and move down only when necessary.",
          "htmlDefinition": "<p>AI customization exists on a spectrum from cheap-and-quick to expensive-and-permanent:</p>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Cost</th>\n<th>Time</th>\n<th>Flexibility</th>\n<th>Best For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Prompt Engineering</strong></td>\n<td>$0</td>\n<td>Minutes</td>\n<td>High (change anytime)</td>\n<td>Most use cases</td>\n</tr>\n<tr>\n<td><strong>Few-Shot Examples</strong></td>\n<td>$0</td>\n<td>Hours</td>\n<td>High</td>\n<td>Pattern matching, formatting</td>\n</tr>\n<tr>\n<td><strong>RAG (Retrieval)</strong></td>\n<td>$-$$</td>\n<td>Days</td>\n<td>Medium</td>\n<td>Knowledge bases, docs</td>\n</tr>\n<tr>\n<td><strong>Fine-Tuning</strong></td>\n<td>$$-$$$</td>\n<td>Weeks</td>\n<td>Low (retraining needed)</td>\n<td>Style, specialized domains</td>\n</tr>\n<tr>\n<td><strong>Custom Training</strong></td>\n<td>$$$$</td>\n<td>Months</td>\n<td>Very Low</td>\n<td>Unique capabilities</td>\n</tr>\n</tbody></table>\n<p><strong>Key insight:</strong> Most projects should start at the top of this spectrum and move down only when necessary.</p>\n"
        },
        {
          "id": "rag-architecture",
          "term": "rag architecture",
          "definition": "**RAG** = Retrieve relevant context → Augment the prompt → Generate response\n\nInstead of training knowledge into the model, you *look it up* at query time.\n\n**How it works:**\n```\nUser asks: \"What's our refund policy for premium members?\"\n                    ↓\nStep 1: RETRIEVE — Search your knowledge base\n        → Finds: \"Premium members get 30-day refunds...\"\n                    ↓\nStep 2: AUGMENT — Add context to prompt\n        → Prompt: \"Using this policy [context], answer: ...\"\n                    ↓\nStep 3: GENERATE — Model produces answer\n        → \"Premium members are entitled to full refunds within 30 days...\"\n```\n\n**RAG advantages:**\n- Knowledge updates instantly (just update the source)\n- No retraining required\n- Transparent sources (can show citations)\n- Cheaper than fine-tuning for knowledge\n\n**RAG limitations:**\n- Retrieval quality depends on your data structure\n- Adds latency (search step)\n- Can't change how the model *behaves*, only what it *knows*",
          "htmlDefinition": "<p><strong>RAG</strong> = Retrieve relevant context → Augment the prompt → Generate response</p>\n<p>Instead of training knowledge into the model, you <em>look it up</em> at query time.</p>\n<p><strong>How it works:</strong></p>\n<pre><code>User asks: &quot;What&#39;s our refund policy for premium members?&quot;\n                    ↓\nStep 1: RETRIEVE — Search your knowledge base\n        → Finds: &quot;Premium members get 30-day refunds...&quot;\n                    ↓\nStep 2: AUGMENT — Add context to prompt\n        → Prompt: &quot;Using this policy [context], answer: ...&quot;\n                    ↓\nStep 3: GENERATE — Model produces answer\n        → &quot;Premium members are entitled to full refunds within 30 days...&quot;\n</code></pre>\n<p><strong>RAG advantages:</strong></p>\n<ul>\n<li>Knowledge updates instantly (just update the source)</li>\n<li>No retraining required</li>\n<li>Transparent sources (can show citations)</li>\n<li>Cheaper than fine-tuning for knowledge</li>\n</ul>\n<p><strong>RAG limitations:</strong></p>\n<ul>\n<li>Retrieval quality depends on your data structure</li>\n<li>Adds latency (search step)</li>\n<li>Can&#39;t change how the model <em>behaves</em>, only what it <em>knows</em></li>\n</ul>\n"
        },
        {
          "id": "fine-tuning",
          "term": "fine tuning",
          "definition": "**Fine-tuning** = Training a model on your examples to change how it behaves.\n\nNot about what the model knows—about how it responds.\n\n**Good fine-tuning use cases:**\n- Consistent voice/tone (match your brand exactly)\n- Specialized formatting (always output in specific JSON structure)\n- Domain-specific reasoning (medical, legal, financial patterns)\n- Behavior modification (be more/less formal, technical, concise)\n\n**Poor fine-tuning use cases:**\n- Adding factual knowledge (use RAG instead)\n- One-off tasks (just use prompts)\n- Rapidly changing requirements (too slow to iterate)",
          "htmlDefinition": "<p><strong>Fine-tuning</strong> = Training a model on your examples to change how it behaves.</p>\n<p>Not about what the model knows—about how it responds.</p>\n<p><strong>Good fine-tuning use cases:</strong></p>\n<ul>\n<li>Consistent voice/tone (match your brand exactly)</li>\n<li>Specialized formatting (always output in specific JSON structure)</li>\n<li>Domain-specific reasoning (medical, legal, financial patterns)</li>\n<li>Behavior modification (be more/less formal, technical, concise)</li>\n</ul>\n<p><strong>Poor fine-tuning use cases:</strong></p>\n<ul>\n<li>Adding factual knowledge (use RAG instead)</li>\n<li>One-off tasks (just use prompts)</li>\n<li>Rapidly changing requirements (too slow to iterate)</li>\n</ul>\n"
        },
        {
          "id": "customization-decision",
          "term": "customization decision",
          "definition": "**Ask these questions in order:**\n\n**1. Can prompt engineering solve this?**\n- Have you tried detailed system prompts?\n- Have you tested few-shot examples?\n- Have you iterated on prompt structure?\n→ If yes, you're done. Don't over-engineer.\n\n**2. Is the problem *knowledge* or *behavior*?**\n- Knowledge problem = RAG\n  - \"It doesn't know our products\"\n  - \"It can't access our policies\"\n  - \"It gives outdated information\"\n- Behavior problem = Fine-tuning\n  - \"It doesn't sound like our brand\"\n  - \"It won't output in our format consistently\"\n  - \"It reasons incorrectly in our domain\"\n\n**3. What's the volume?**\n- Low volume (<1000 queries/month): Prompt engineering + some manual review\n- Medium volume (1K-100K/month): RAG for knowledge, prompts for behavior\n- High volume (>100K/month): Fine-tuning ROI becomes compelling\n\n**4. How fast do requirements change?**\n- Changing weekly: Prompts only\n- Changing monthly: RAG acceptable\n- Stable for 6+ months: Fine-tuning viable",
          "htmlDefinition": "<p><strong>Ask these questions in order:</strong></p>\n<p><strong>1. Can prompt engineering solve this?</strong></p>\n<ul>\n<li>Have you tried detailed system prompts?</li>\n<li>Have you tested few-shot examples?</li>\n<li>Have you iterated on prompt structure?\n→ If yes, you&#39;re done. Don&#39;t over-engineer.</li>\n</ul>\n<p><strong>2. Is the problem <em>knowledge</em> or <em>behavior</em>?</strong></p>\n<ul>\n<li>Knowledge problem = RAG<ul>\n<li>&quot;It doesn&#39;t know our products&quot;</li>\n<li>&quot;It can&#39;t access our policies&quot;</li>\n<li>&quot;It gives outdated information&quot;</li>\n</ul>\n</li>\n<li>Behavior problem = Fine-tuning<ul>\n<li>&quot;It doesn&#39;t sound like our brand&quot;</li>\n<li>&quot;It won&#39;t output in our format consistently&quot;</li>\n<li>&quot;It reasons incorrectly in our domain&quot;</li>\n</ul>\n</li>\n</ul>\n<p><strong>3. What&#39;s the volume?</strong></p>\n<ul>\n<li>Low volume (&lt;1000 queries/month): Prompt engineering + some manual review</li>\n<li>Medium volume (1K-100K/month): RAG for knowledge, prompts for behavior</li>\n<li>High volume (&gt;100K/month): Fine-tuning ROI becomes compelling</li>\n</ul>\n<p><strong>4. How fast do requirements change?</strong></p>\n<ul>\n<li>Changing weekly: Prompts only</li>\n<li>Changing monthly: RAG acceptable</li>\n<li>Stable for 6+ months: Fine-tuning viable</li>\n</ul>\n"
        },
        {
          "id": "when-not-to-customize",
          "term": "when not to customize",
          "definition": "**Don't customize when:**\n\n- Prompt engineering gets you to 80%+ accuracy\n- Volume doesn't justify the investment\n- Requirements are still changing\n- You lack quality training data\n- The use case is exploratory\n\n**Red flags that you're over-engineering:**\n- \"We might need this capability later\"\n- \"It would be cool if it could...\"\n- \"Other companies are doing fine-tuning\"\n- No clear ROI calculation\n\n**The 80/20 rule applies:**\n80% of AI value comes from basic prompting + good workflow design.\n20% comes from advanced customization.\n\nDon't pursue the 20% until you've captured the 80%.",
          "htmlDefinition": "<p><strong>Don&#39;t customize when:</strong></p>\n<ul>\n<li>Prompt engineering gets you to 80%+ accuracy</li>\n<li>Volume doesn&#39;t justify the investment</li>\n<li>Requirements are still changing</li>\n<li>You lack quality training data</li>\n<li>The use case is exploratory</li>\n</ul>\n<p><strong>Red flags that you&#39;re over-engineering:</strong></p>\n<ul>\n<li>&quot;We might need this capability later&quot;</li>\n<li>&quot;It would be cool if it could...&quot;</li>\n<li>&quot;Other companies are doing fine-tuning&quot;</li>\n<li>No clear ROI calculation</li>\n</ul>\n<p><strong>The 80/20 rule applies:</strong>\n80% of AI value comes from basic prompting + good workflow design.\n20% comes from advanced customization.</p>\n<p>Don&#39;t pursue the 20% until you&#39;ve captured the 80%.</p>\n"
        }
      ],
      "exercises": [
        {
          "id": "customization-decision",
          "title": "Scenario",
          "instructions": "You're the operations lead at a mid-size law firm. Attorneys waste hours drafting routine documents that follow standard templates. You want to deploy AI assistance.\n\n**Analyze these three use cases:**\n\n**Use Case A: Contract summarization**\n- Input: Client contracts (confidential)\n- Output: Plain-English summaries of key terms\n- Volume: ~200 contracts/month\n- Requirements: Must cite specific clauses\n\n**Use Case B: Standard letter drafting**\n- Input: Matter type + key details\n- Output: Client correspondence\n- Volume: ~500 letters/month\n- Requirements: Must match firm's formal voice exactly\n\n**Use Case C: Legal research assistance**\n- Input: Case questions\n- Output: Relevant precedents and analysis\n- Volume: ~100 queries/month\n- Requirements: Must use current case law\n\n**For each use case, determine:**\n\n1. **Primary need**: Knowledge, behavior, or both?\n\n2. **Recommended approach**: Prompt engineering, RAG, fine-tuning, or hybrid?\n\n3. **Cost-benefit analysis**:\n   - Estimated setup cost\n   - Ongoing costs\n   - Value generated (hours saved × hourly rate)\n\n4. **Build vs. buy decision**: In-house or vendor?\n\n5. **Risk assessment**: What could go wrong?",
          "htmlInstructions": "<p>You&#39;re the operations lead at a mid-size law firm. Attorneys waste hours drafting routine documents that follow standard templates. You want to deploy AI assistance.</p>\n<p><strong>Analyze these three use cases:</strong></p>\n<p><strong>Use Case A: Contract summarization</strong></p>\n<ul>\n<li>Input: Client contracts (confidential)</li>\n<li>Output: Plain-English summaries of key terms</li>\n<li>Volume: ~200 contracts/month</li>\n<li>Requirements: Must cite specific clauses</li>\n</ul>\n<p><strong>Use Case B: Standard letter drafting</strong></p>\n<ul>\n<li>Input: Matter type + key details</li>\n<li>Output: Client correspondence</li>\n<li>Volume: ~500 letters/month</li>\n<li>Requirements: Must match firm&#39;s formal voice exactly</li>\n</ul>\n<p><strong>Use Case C: Legal research assistance</strong></p>\n<ul>\n<li>Input: Case questions</li>\n<li>Output: Relevant precedents and analysis</li>\n<li>Volume: ~100 queries/month</li>\n<li>Requirements: Must use current case law</li>\n</ul>\n<p><strong>For each use case, determine:</strong></p>\n<ol>\n<li><p><strong>Primary need</strong>: Knowledge, behavior, or both?</p>\n</li>\n<li><p><strong>Recommended approach</strong>: Prompt engineering, RAG, fine-tuning, or hybrid?</p>\n</li>\n<li><p><strong>Cost-benefit analysis</strong>:</p>\n<ul>\n<li>Estimated setup cost</li>\n<li>Ongoing costs</li>\n<li>Value generated (hours saved × hourly rate)</li>\n</ul>\n</li>\n<li><p><strong>Build vs. buy decision</strong>: In-house or vendor?</p>\n</li>\n<li><p><strong>Risk assessment</strong>: What could go wrong?</p>\n</li>\n</ol>\n"
        }
      ],
      "checklists": [
        {
          "id": "module-3.5-complete",
          "items": [
            {
              "id": "module-3.5-complete-0",
              "text": "I can explain the difference between RAG and fine-tuning",
              "completed": false
            },
            {
              "id": "module-3.5-complete-1",
              "text": "I know when knowledge problems require RAG",
              "completed": false
            },
            {
              "id": "module-3.5-complete-2",
              "text": "I know when behavior problems require fine-tuning",
              "completed": false
            },
            {
              "id": "module-3.5-complete-3",
              "text": "I can calculate customization ROI",
              "completed": false
            },
            {
              "id": "module-3.5-complete-4",
              "text": "I understand hybrid approaches",
              "completed": false
            },
            {
              "id": "module-3.5-complete-5",
              "text": "I can identify when customization is overkill",
              "completed": false
            }
          ]
        }
      ]
    }
  ],
  "labs": [
    {
      "id": "lab-5-build-ai-assistant",
      "slug": "lab-5-build-ai-assistant",
      "title": "Build an AI Assistant",
      "phase": 3,
      "labNumber": 5,
      "estimatedMinutes": 60,
      "objectives": [
        "Build a functional AI assistant using no-code tools",
        "Configure knowledge base and capabilities",
        "Test and iterate on assistant performance"
      ],
      "prerequisites": [
        "3.1-no-code-ai-tools",
        "3.2-api-fundamentals"
      ],
      "content": "# Lab 5: Build an AI Assistant\n\n## Lab Overview\n\nStop theorizing. Build something real. In this lab, you'll create a functional AI assistant that solves a genuine business problem. You'll use no-code tools to go from concept to working product.\n\n**What you'll create:**\n- A custom AI assistant with specialized knowledge\n- Tested and validated for your use case\n- Ready for real users\n\n---\n\n## Part 1: Define Your Assistant (10 minutes)\n\n### Choose Your Use Case\n\nSelect one:\n\n**Option A: Subject Matter Expert Bot**\nAn assistant that answers questions about a specific topic (industry, product, process).\n\n**Option B: Task Automation Assistant**\nAn assistant that helps users complete a specific workflow (onboarding, report generation, etc.).\n\n**Option C: Decision Support Assistant**\nAn assistant that helps users make better decisions by asking good questions and analyzing options.\n\n**Option D: Learning Companion**\nAn assistant that teaches a skill through conversation (explains concepts, provides practice, gives feedback).\n\n**Option E: Your Innovation**\nAn assistant for a use case you've identified.\n\n### Complete the Definition Template\n\n```\nASSISTANT DEFINITION\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nName: [Give it a name]\n\nPurpose:\n[One sentence: What problem does it solve?]\n\nTarget users:\n[Who will use this?]\n\nCore capabilities:\n1. [Primary capability]\n2. [Secondary capability]\n3. [Tertiary capability]\n\nKnowledge requirements:\n[What does it need to know?]\n\nOut of scope:\n[What should it NOT do?]\n\nSuccess metric:\n[How will you know it works?]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n---\n\n## Part 2: Write the Instructions (15 minutes)\n\n### Craft Your System Instructions\n\nUsing the techniques from Module 1.4, write comprehensive instructions:\n\n```\nSYSTEM INSTRUCTIONS TEMPLATE\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nYou are [NAME], a [ROLE] specialized in [DOMAIN].\n\nYOUR PURPOSE:\n[What you help users accomplish]\n\nYOUR EXPERTISE:\n- [Specific knowledge area 1]\n- [Specific knowledge area 2]\n- [Specific knowledge area 3]\n\nHOW YOU COMMUNICATE:\n- Tone: [Describe]\n- Style: [Describe]\n- Length: [Preferences]\n\nWHEN RESPONDING:\n- Always: [Required behavior]\n- Never: [Prohibited behavior]\n- If uncertain: [Fallback behavior]\n\nCONVERSATION FLOW:\n- First, [How to open conversations]\n- Then, [How to gather needed information]\n- Finally, [How to deliver value]\n\nIMPORTANT CONSTRAINTS:\n- [Constraint 1]\n- [Constraint 2]\n- [Constraint 3]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Write Three Versions\n\nCreate three variations of your instructions:\n1. **Concise** (under 200 words)\n2. **Detailed** (300-500 words)\n3. **Comprehensive** (500+ words with examples)\n\nYou'll test which performs best.\n\n---\n\n## Part 3: Prepare Knowledge Base (10 minutes)\n\n### Identify Knowledge Sources\n\nWhat documents or information should your assistant access?\n\n| Source | Type | Purpose |\n|--------|------|---------|\n| [Source 1] | PDF/Doc/Text | [Why needed] |\n| [Source 2] | PDF/Doc/Text | [Why needed] |\n| [Source 3] | PDF/Doc/Text | [Why needed] |\n\n### Prepare Documents\n\nFor each source:\n1. Remove irrelevant content\n2. Ensure clear formatting\n3. Add section headers if missing\n4. Note: Most platforms have file size limits (check yours)\n\n**If you don't have real documents**, create sample content:\n- FAQ document with 10-15 common questions\n- Brief guide or process documentation\n- Reference material relevant to your use case\n\n---\n\n## Part 4: Build the Assistant (15 minutes)\n\n### Select Your Platform\n\nChoose one:\n- **OpenAI GPT Builder** (if you have ChatGPT Plus)\n- **Claude Projects** (if you have Claude Pro)\n- **Poe Bot Creator** (free option)\n- **Alternative** (HuggingChat, other platforms)\n\n### Build Steps\n\n**Step 1: Create new assistant**\n- Name your assistant\n- Add a description\n\n**Step 2: Configure instructions**\n- Start with your \"Detailed\" version\n- Review platform-specific guidelines\n\n**Step 3: Upload knowledge**\n- Add your prepared documents\n- Verify uploads successful\n\n**Step 4: Configure capabilities**\n- Enable/disable web browsing (if available)\n- Enable/disable code interpreter (if available)\n- Adjust other settings as needed\n\n**Step 5: Add conversation starters**\nCreate 4-5 suggested prompts:\n```\n1. \"[Question that showcases core capability 1]\"\n2. \"[Question that showcases core capability 2]\"\n3. \"[Common user question]\"\n4. \"[Edge case question]\"\n5. \"[Getting started question]\"\n```\n\n---\n\n## Part 5: Test and Iterate (15 minutes)\n\n### Run Test Suite\n\nTest your assistant with these scenarios:\n\n**Basic functionality tests:**\n| Test | Input | Expected Behavior | Pass? |\n|------|-------|-------------------|-------|\n| Core use case | [Typical user question] | [What should happen] | |\n| Knowledge retrieval | [Question requiring documents] | [Accurate response] | |\n| Clarification | [Vague question] | [Asks for details] | |\n| Out of scope | [Question it shouldn't answer] | [Politely declines] | |\n\n**Stress tests:**\n| Test | Input | Expected Behavior | Pass? |\n|------|-------|-------------------|-------|\n| Long input | [Very detailed request] | [Handles gracefully] | |\n| Ambiguous request | [Could mean multiple things] | [Seeks clarification] | |\n| Edge case | [Unusual but valid question] | [Reasonable response] | |\n| Adversarial | [Attempt to confuse/break] | [Maintains composure] | |\n\n### Identify Issues\n\nFor each failed test:\n```\nISSUE: [Test name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nExpected: [What should have happened]\nActual: [What happened]\nRoot cause: [Why did it fail?]\nFix: [What to change]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Iteration Cycle\n\n1. Review failed tests\n2. Identify instruction improvements\n3. Update system instructions\n4. Re-test\n5. Document what worked\n\n**Track your iterations:**\n\n| Version | Changes Made | Tests Passed | Notes |\n|---------|--------------|--------------|-------|\n| v1 | Initial build | X/10 | [Issues] |\n| v2 | [Changes] | X/10 | [Improvement] |\n| v3 | [Changes] | X/10 | [Status] |\n\n---\n\n## Part 6: Document Your Assistant (5 minutes)\n\n### Create User Guide\n\n```\n[ASSISTANT NAME] - User Guide\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nWHAT THIS ASSISTANT DOES:\n[Brief description]\n\nHOW TO USE IT:\n1. [Step 1]\n2. [Step 2]\n3. [Step 3]\n\nBEST FOR:\n• [Use case 1]\n• [Use case 2]\n• [Use case 3]\n\nNOT DESIGNED FOR:\n• [Limitation 1]\n• [Limitation 2]\n\nTIPS FOR BEST RESULTS:\n• [Tip 1]\n• [Tip 2]\n• [Tip 3]\n\nTROUBLESHOOTING:\nIf [problem] → Try [solution]\nIf [problem] → Try [solution]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n---\n\n## Deliverable\n\nCreate a complete assistant package:\n\n1. **Assistant Definition Document**\n   - Use case and purpose\n   - Target users\n   - Success criteria\n\n2. **System Instructions**\n   - Final version used\n   - Notes on what worked/didn't\n\n3. **Knowledge Base Inventory**\n   - List of documents uploaded\n   - Why each was included\n\n4. **Test Results**\n   - All test cases and results\n   - Iteration history\n\n5. **User Guide**\n   - Ready to share with potential users\n\n6. **Link to Working Assistant**\n   - Shareable link (if platform supports)\n   - Or screenshots showing it works\n\n---\n\n## Extension Challenge\n\n**Usage Pilot**\n\n1. Share your assistant with 3-5 real users\n2. Collect feedback:\n   - What worked well?\n   - What was confusing?\n   - What's missing?\n3. Make improvements based on feedback\n4. Document the iteration\n\nThis real-world testing is where you'll learn the most.",
      "htmlContent": "<h1>Lab 5: Build an AI Assistant</h1>\n<h2>Lab Overview</h2>\n<p>Stop theorizing. Build something real. In this lab, you&#39;ll create a functional AI assistant that solves a genuine business problem. You&#39;ll use no-code tools to go from concept to working product.</p>\n<p><strong>What you&#39;ll create:</strong></p>\n<ul>\n<li>A custom AI assistant with specialized knowledge</li>\n<li>Tested and validated for your use case</li>\n<li>Ready for real users</li>\n</ul>\n<hr>\n<h2>Part 1: Define Your Assistant (10 minutes)</h2>\n<h3>Choose Your Use Case</h3>\n<p>Select one:</p>\n<p><strong>Option A: Subject Matter Expert Bot</strong>\nAn assistant that answers questions about a specific topic (industry, product, process).</p>\n<p><strong>Option B: Task Automation Assistant</strong>\nAn assistant that helps users complete a specific workflow (onboarding, report generation, etc.).</p>\n<p><strong>Option C: Decision Support Assistant</strong>\nAn assistant that helps users make better decisions by asking good questions and analyzing options.</p>\n<p><strong>Option D: Learning Companion</strong>\nAn assistant that teaches a skill through conversation (explains concepts, provides practice, gives feedback).</p>\n<p><strong>Option E: Your Innovation</strong>\nAn assistant for a use case you&#39;ve identified.</p>\n<h3>Complete the Definition Template</h3>\n<pre><code>ASSISTANT DEFINITION\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nName: [Give it a name]\n\nPurpose:\n[One sentence: What problem does it solve?]\n\nTarget users:\n[Who will use this?]\n\nCore capabilities:\n1. [Primary capability]\n2. [Secondary capability]\n3. [Tertiary capability]\n\nKnowledge requirements:\n[What does it need to know?]\n\nOut of scope:\n[What should it NOT do?]\n\nSuccess metric:\n[How will you know it works?]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<hr>\n<h2>Part 2: Write the Instructions (15 minutes)</h2>\n<h3>Craft Your System Instructions</h3>\n<p>Using the techniques from Module 1.4, write comprehensive instructions:</p>\n<pre><code>SYSTEM INSTRUCTIONS TEMPLATE\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nYou are [NAME], a [ROLE] specialized in [DOMAIN].\n\nYOUR PURPOSE:\n[What you help users accomplish]\n\nYOUR EXPERTISE:\n- [Specific knowledge area 1]\n- [Specific knowledge area 2]\n- [Specific knowledge area 3]\n\nHOW YOU COMMUNICATE:\n- Tone: [Describe]\n- Style: [Describe]\n- Length: [Preferences]\n\nWHEN RESPONDING:\n- Always: [Required behavior]\n- Never: [Prohibited behavior]\n- If uncertain: [Fallback behavior]\n\nCONVERSATION FLOW:\n- First, [How to open conversations]\n- Then, [How to gather needed information]\n- Finally, [How to deliver value]\n\nIMPORTANT CONSTRAINTS:\n- [Constraint 1]\n- [Constraint 2]\n- [Constraint 3]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<h3>Write Three Versions</h3>\n<p>Create three variations of your instructions:</p>\n<ol>\n<li><strong>Concise</strong> (under 200 words)</li>\n<li><strong>Detailed</strong> (300-500 words)</li>\n<li><strong>Comprehensive</strong> (500+ words with examples)</li>\n</ol>\n<p>You&#39;ll test which performs best.</p>\n<hr>\n<h2>Part 3: Prepare Knowledge Base (10 minutes)</h2>\n<h3>Identify Knowledge Sources</h3>\n<p>What documents or information should your assistant access?</p>\n<table>\n<thead>\n<tr>\n<th>Source</th>\n<th>Type</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>[Source 1]</td>\n<td>PDF/Doc/Text</td>\n<td>[Why needed]</td>\n</tr>\n<tr>\n<td>[Source 2]</td>\n<td>PDF/Doc/Text</td>\n<td>[Why needed]</td>\n</tr>\n<tr>\n<td>[Source 3]</td>\n<td>PDF/Doc/Text</td>\n<td>[Why needed]</td>\n</tr>\n</tbody></table>\n<h3>Prepare Documents</h3>\n<p>For each source:</p>\n<ol>\n<li>Remove irrelevant content</li>\n<li>Ensure clear formatting</li>\n<li>Add section headers if missing</li>\n<li>Note: Most platforms have file size limits (check yours)</li>\n</ol>\n<p><strong>If you don&#39;t have real documents</strong>, create sample content:</p>\n<ul>\n<li>FAQ document with 10-15 common questions</li>\n<li>Brief guide or process documentation</li>\n<li>Reference material relevant to your use case</li>\n</ul>\n<hr>\n<h2>Part 4: Build the Assistant (15 minutes)</h2>\n<h3>Select Your Platform</h3>\n<p>Choose one:</p>\n<ul>\n<li><strong>OpenAI GPT Builder</strong> (if you have ChatGPT Plus)</li>\n<li><strong>Claude Projects</strong> (if you have Claude Pro)</li>\n<li><strong>Poe Bot Creator</strong> (free option)</li>\n<li><strong>Alternative</strong> (HuggingChat, other platforms)</li>\n</ul>\n<h3>Build Steps</h3>\n<p><strong>Step 1: Create new assistant</strong></p>\n<ul>\n<li>Name your assistant</li>\n<li>Add a description</li>\n</ul>\n<p><strong>Step 2: Configure instructions</strong></p>\n<ul>\n<li>Start with your &quot;Detailed&quot; version</li>\n<li>Review platform-specific guidelines</li>\n</ul>\n<p><strong>Step 3: Upload knowledge</strong></p>\n<ul>\n<li>Add your prepared documents</li>\n<li>Verify uploads successful</li>\n</ul>\n<p><strong>Step 4: Configure capabilities</strong></p>\n<ul>\n<li>Enable/disable web browsing (if available)</li>\n<li>Enable/disable code interpreter (if available)</li>\n<li>Adjust other settings as needed</li>\n</ul>\n<p><strong>Step 5: Add conversation starters</strong>\nCreate 4-5 suggested prompts:</p>\n<pre><code>1. &quot;[Question that showcases core capability 1]&quot;\n2. &quot;[Question that showcases core capability 2]&quot;\n3. &quot;[Common user question]&quot;\n4. &quot;[Edge case question]&quot;\n5. &quot;[Getting started question]&quot;\n</code></pre>\n<hr>\n<h2>Part 5: Test and Iterate (15 minutes)</h2>\n<h3>Run Test Suite</h3>\n<p>Test your assistant with these scenarios:</p>\n<p><strong>Basic functionality tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test</th>\n<th>Input</th>\n<th>Expected Behavior</th>\n<th>Pass?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Core use case</td>\n<td>[Typical user question]</td>\n<td>[What should happen]</td>\n<td></td>\n</tr>\n<tr>\n<td>Knowledge retrieval</td>\n<td>[Question requiring documents]</td>\n<td>[Accurate response]</td>\n<td></td>\n</tr>\n<tr>\n<td>Clarification</td>\n<td>[Vague question]</td>\n<td>[Asks for details]</td>\n<td></td>\n</tr>\n<tr>\n<td>Out of scope</td>\n<td>[Question it shouldn&#39;t answer]</td>\n<td>[Politely declines]</td>\n<td></td>\n</tr>\n</tbody></table>\n<p><strong>Stress tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test</th>\n<th>Input</th>\n<th>Expected Behavior</th>\n<th>Pass?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Long input</td>\n<td>[Very detailed request]</td>\n<td>[Handles gracefully]</td>\n<td></td>\n</tr>\n<tr>\n<td>Ambiguous request</td>\n<td>[Could mean multiple things]</td>\n<td>[Seeks clarification]</td>\n<td></td>\n</tr>\n<tr>\n<td>Edge case</td>\n<td>[Unusual but valid question]</td>\n<td>[Reasonable response]</td>\n<td></td>\n</tr>\n<tr>\n<td>Adversarial</td>\n<td>[Attempt to confuse/break]</td>\n<td>[Maintains composure]</td>\n<td></td>\n</tr>\n</tbody></table>\n<h3>Identify Issues</h3>\n<p>For each failed test:</p>\n<pre><code>ISSUE: [Test name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nExpected: [What should have happened]\nActual: [What happened]\nRoot cause: [Why did it fail?]\nFix: [What to change]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<h3>Iteration Cycle</h3>\n<ol>\n<li>Review failed tests</li>\n<li>Identify instruction improvements</li>\n<li>Update system instructions</li>\n<li>Re-test</li>\n<li>Document what worked</li>\n</ol>\n<p><strong>Track your iterations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Version</th>\n<th>Changes Made</th>\n<th>Tests Passed</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>v1</td>\n<td>Initial build</td>\n<td>X/10</td>\n<td>[Issues]</td>\n</tr>\n<tr>\n<td>v2</td>\n<td>[Changes]</td>\n<td>X/10</td>\n<td>[Improvement]</td>\n</tr>\n<tr>\n<td>v3</td>\n<td>[Changes]</td>\n<td>X/10</td>\n<td>[Status]</td>\n</tr>\n</tbody></table>\n<hr>\n<h2>Part 6: Document Your Assistant (5 minutes)</h2>\n<h3>Create User Guide</h3>\n<pre><code>[ASSISTANT NAME] - User Guide\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nWHAT THIS ASSISTANT DOES:\n[Brief description]\n\nHOW TO USE IT:\n1. [Step 1]\n2. [Step 2]\n3. [Step 3]\n\nBEST FOR:\n• [Use case 1]\n• [Use case 2]\n• [Use case 3]\n\nNOT DESIGNED FOR:\n• [Limitation 1]\n• [Limitation 2]\n\nTIPS FOR BEST RESULTS:\n• [Tip 1]\n• [Tip 2]\n• [Tip 3]\n\nTROUBLESHOOTING:\nIf [problem] → Try [solution]\nIf [problem] → Try [solution]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<hr>\n<h2>Deliverable</h2>\n<p>Create a complete assistant package:</p>\n<ol>\n<li><p><strong>Assistant Definition Document</strong></p>\n<ul>\n<li>Use case and purpose</li>\n<li>Target users</li>\n<li>Success criteria</li>\n</ul>\n</li>\n<li><p><strong>System Instructions</strong></p>\n<ul>\n<li>Final version used</li>\n<li>Notes on what worked/didn&#39;t</li>\n</ul>\n</li>\n<li><p><strong>Knowledge Base Inventory</strong></p>\n<ul>\n<li>List of documents uploaded</li>\n<li>Why each was included</li>\n</ul>\n</li>\n<li><p><strong>Test Results</strong></p>\n<ul>\n<li>All test cases and results</li>\n<li>Iteration history</li>\n</ul>\n</li>\n<li><p><strong>User Guide</strong></p>\n<ul>\n<li>Ready to share with potential users</li>\n</ul>\n</li>\n<li><p><strong>Link to Working Assistant</strong></p>\n<ul>\n<li>Shareable link (if platform supports)</li>\n<li>Or screenshots showing it works</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h2>Extension Challenge</h2>\n<p><strong>Usage Pilot</strong></p>\n<ol>\n<li>Share your assistant with 3-5 real users</li>\n<li>Collect feedback:<ul>\n<li>What worked well?</li>\n<li>What was confusing?</li>\n<li>What&#39;s missing?</li>\n</ul>\n</li>\n<li>Make improvements based on feedback</li>\n<li>Document the iteration</li>\n</ol>\n<p>This real-world testing is where you&#39;ll learn the most.</p>\n",
      "sections": [
        {
          "id": "lab-overview",
          "title": "Lab Overview",
          "type": "generic",
          "content": "Stop theorizing. Build something real. In this lab, you'll create a functional AI assistant that solves a genuine business problem. You'll use no-code tools to go from concept to working product.\n\n**What you'll create:**\n- A custom AI assistant with specialized knowledge\n- Tested and validated for your use case\n- Ready for real users\n\n---",
          "htmlContent": "<p>Stop theorizing. Build something real. In this lab, you&#39;ll create a functional AI assistant that solves a genuine business problem. You&#39;ll use no-code tools to go from concept to working product.</p>\n<p><strong>What you&#39;ll create:</strong></p>\n<ul>\n<li>A custom AI assistant with specialized knowledge</li>\n<li>Tested and validated for your use case</li>\n<li>Ready for real users</li>\n</ul>\n<hr>\n"
        },
        {
          "id": "part-1:-define-your-assistant-(10-minutes)",
          "title": "Part 1: Define Your Assistant (10 minutes)",
          "type": "generic",
          "content": "### Choose Your Use Case\n\nSelect one:\n\n**Option A: Subject Matter Expert Bot**\nAn assistant that answers questions about a specific topic (industry, product, process).\n\n**Option B: Task Automation Assistant**\nAn assistant that helps users complete a specific workflow (onboarding, report generation, etc.).\n\n**Option C: Decision Support Assistant**\nAn assistant that helps users make better decisions by asking good questions and analyzing options.\n\n**Option D: Learning Companion**\nAn assistant that teaches a skill through conversation (explains concepts, provides practice, gives feedback).\n\n**Option E: Your Innovation**\nAn assistant for a use case you've identified.\n\n### Complete the Definition Template\n\n```\nASSISTANT DEFINITION\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nName: [Give it a name]\n\nPurpose:\n[One sentence: What problem does it solve?]\n\nTarget users:\n[Who will use this?]\n\nCore capabilities:\n1. [Primary capability]\n2. [Secondary capability]\n3. [Tertiary capability]\n\nKnowledge requirements:\n[What does it need to know?]\n\nOut of scope:\n[What should it NOT do?]\n\nSuccess metric:\n[How will you know it works?]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n---",
          "htmlContent": "<h3>Choose Your Use Case</h3>\n<p>Select one:</p>\n<p><strong>Option A: Subject Matter Expert Bot</strong>\nAn assistant that answers questions about a specific topic (industry, product, process).</p>\n<p><strong>Option B: Task Automation Assistant</strong>\nAn assistant that helps users complete a specific workflow (onboarding, report generation, etc.).</p>\n<p><strong>Option C: Decision Support Assistant</strong>\nAn assistant that helps users make better decisions by asking good questions and analyzing options.</p>\n<p><strong>Option D: Learning Companion</strong>\nAn assistant that teaches a skill through conversation (explains concepts, provides practice, gives feedback).</p>\n<p><strong>Option E: Your Innovation</strong>\nAn assistant for a use case you&#39;ve identified.</p>\n<h3>Complete the Definition Template</h3>\n<pre><code>ASSISTANT DEFINITION\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nName: [Give it a name]\n\nPurpose:\n[One sentence: What problem does it solve?]\n\nTarget users:\n[Who will use this?]\n\nCore capabilities:\n1. [Primary capability]\n2. [Secondary capability]\n3. [Tertiary capability]\n\nKnowledge requirements:\n[What does it need to know?]\n\nOut of scope:\n[What should it NOT do?]\n\nSuccess metric:\n[How will you know it works?]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<hr>\n"
        },
        {
          "id": "part-2:-write-the-instructions-(15-minutes)",
          "title": "Part 2: Write the Instructions (15 minutes)",
          "type": "generic",
          "content": "### Craft Your System Instructions\n\nUsing the techniques from Module 1.4, write comprehensive instructions:\n\n```\nSYSTEM INSTRUCTIONS TEMPLATE\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nYou are [NAME], a [ROLE] specialized in [DOMAIN].\n\nYOUR PURPOSE:\n[What you help users accomplish]\n\nYOUR EXPERTISE:\n- [Specific knowledge area 1]\n- [Specific knowledge area 2]\n- [Specific knowledge area 3]\n\nHOW YOU COMMUNICATE:\n- Tone: [Describe]\n- Style: [Describe]\n- Length: [Preferences]\n\nWHEN RESPONDING:\n- Always: [Required behavior]\n- Never: [Prohibited behavior]\n- If uncertain: [Fallback behavior]\n\nCONVERSATION FLOW:\n- First, [How to open conversations]\n- Then, [How to gather needed information]\n- Finally, [How to deliver value]\n\nIMPORTANT CONSTRAINTS:\n- [Constraint 1]\n- [Constraint 2]\n- [Constraint 3]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Write Three Versions\n\nCreate three variations of your instructions:\n1. **Concise** (under 200 words)\n2. **Detailed** (300-500 words)\n3. **Comprehensive** (500+ words with examples)\n\nYou'll test which performs best.\n\n---",
          "htmlContent": "<h3>Craft Your System Instructions</h3>\n<p>Using the techniques from Module 1.4, write comprehensive instructions:</p>\n<pre><code>SYSTEM INSTRUCTIONS TEMPLATE\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nYou are [NAME], a [ROLE] specialized in [DOMAIN].\n\nYOUR PURPOSE:\n[What you help users accomplish]\n\nYOUR EXPERTISE:\n- [Specific knowledge area 1]\n- [Specific knowledge area 2]\n- [Specific knowledge area 3]\n\nHOW YOU COMMUNICATE:\n- Tone: [Describe]\n- Style: [Describe]\n- Length: [Preferences]\n\nWHEN RESPONDING:\n- Always: [Required behavior]\n- Never: [Prohibited behavior]\n- If uncertain: [Fallback behavior]\n\nCONVERSATION FLOW:\n- First, [How to open conversations]\n- Then, [How to gather needed information]\n- Finally, [How to deliver value]\n\nIMPORTANT CONSTRAINTS:\n- [Constraint 1]\n- [Constraint 2]\n- [Constraint 3]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<h3>Write Three Versions</h3>\n<p>Create three variations of your instructions:</p>\n<ol>\n<li><strong>Concise</strong> (under 200 words)</li>\n<li><strong>Detailed</strong> (300-500 words)</li>\n<li><strong>Comprehensive</strong> (500+ words with examples)</li>\n</ol>\n<p>You&#39;ll test which performs best.</p>\n<hr>\n"
        },
        {
          "id": "part-3:-prepare-knowledge-base-(10-minutes)",
          "title": "Part 3: Prepare Knowledge Base (10 minutes)",
          "type": "generic",
          "content": "### Identify Knowledge Sources\n\nWhat documents or information should your assistant access?\n\n| Source | Type | Purpose |\n|--------|------|---------|\n| [Source 1] | PDF/Doc/Text | [Why needed] |\n| [Source 2] | PDF/Doc/Text | [Why needed] |\n| [Source 3] | PDF/Doc/Text | [Why needed] |\n\n### Prepare Documents\n\nFor each source:\n1. Remove irrelevant content\n2. Ensure clear formatting\n3. Add section headers if missing\n4. Note: Most platforms have file size limits (check yours)\n\n**If you don't have real documents**, create sample content:\n- FAQ document with 10-15 common questions\n- Brief guide or process documentation\n- Reference material relevant to your use case\n\n---",
          "htmlContent": "<h3>Identify Knowledge Sources</h3>\n<p>What documents or information should your assistant access?</p>\n<table>\n<thead>\n<tr>\n<th>Source</th>\n<th>Type</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>[Source 1]</td>\n<td>PDF/Doc/Text</td>\n<td>[Why needed]</td>\n</tr>\n<tr>\n<td>[Source 2]</td>\n<td>PDF/Doc/Text</td>\n<td>[Why needed]</td>\n</tr>\n<tr>\n<td>[Source 3]</td>\n<td>PDF/Doc/Text</td>\n<td>[Why needed]</td>\n</tr>\n</tbody></table>\n<h3>Prepare Documents</h3>\n<p>For each source:</p>\n<ol>\n<li>Remove irrelevant content</li>\n<li>Ensure clear formatting</li>\n<li>Add section headers if missing</li>\n<li>Note: Most platforms have file size limits (check yours)</li>\n</ol>\n<p><strong>If you don&#39;t have real documents</strong>, create sample content:</p>\n<ul>\n<li>FAQ document with 10-15 common questions</li>\n<li>Brief guide or process documentation</li>\n<li>Reference material relevant to your use case</li>\n</ul>\n<hr>\n"
        },
        {
          "id": "part-4:-build-the-assistant-(15-minutes)",
          "title": "Part 4: Build the Assistant (15 minutes)",
          "type": "generic",
          "content": "### Select Your Platform\n\nChoose one:\n- **OpenAI GPT Builder** (if you have ChatGPT Plus)\n- **Claude Projects** (if you have Claude Pro)\n- **Poe Bot Creator** (free option)\n- **Alternative** (HuggingChat, other platforms)\n\n### Build Steps\n\n**Step 1: Create new assistant**\n- Name your assistant\n- Add a description\n\n**Step 2: Configure instructions**\n- Start with your \"Detailed\" version\n- Review platform-specific guidelines\n\n**Step 3: Upload knowledge**\n- Add your prepared documents\n- Verify uploads successful\n\n**Step 4: Configure capabilities**\n- Enable/disable web browsing (if available)\n- Enable/disable code interpreter (if available)\n- Adjust other settings as needed\n\n**Step 5: Add conversation starters**\nCreate 4-5 suggested prompts:\n```\n1. \"[Question that showcases core capability 1]\"\n2. \"[Question that showcases core capability 2]\"\n3. \"[Common user question]\"\n4. \"[Edge case question]\"\n5. \"[Getting started question]\"\n```\n\n---",
          "htmlContent": "<h3>Select Your Platform</h3>\n<p>Choose one:</p>\n<ul>\n<li><strong>OpenAI GPT Builder</strong> (if you have ChatGPT Plus)</li>\n<li><strong>Claude Projects</strong> (if you have Claude Pro)</li>\n<li><strong>Poe Bot Creator</strong> (free option)</li>\n<li><strong>Alternative</strong> (HuggingChat, other platforms)</li>\n</ul>\n<h3>Build Steps</h3>\n<p><strong>Step 1: Create new assistant</strong></p>\n<ul>\n<li>Name your assistant</li>\n<li>Add a description</li>\n</ul>\n<p><strong>Step 2: Configure instructions</strong></p>\n<ul>\n<li>Start with your &quot;Detailed&quot; version</li>\n<li>Review platform-specific guidelines</li>\n</ul>\n<p><strong>Step 3: Upload knowledge</strong></p>\n<ul>\n<li>Add your prepared documents</li>\n<li>Verify uploads successful</li>\n</ul>\n<p><strong>Step 4: Configure capabilities</strong></p>\n<ul>\n<li>Enable/disable web browsing (if available)</li>\n<li>Enable/disable code interpreter (if available)</li>\n<li>Adjust other settings as needed</li>\n</ul>\n<p><strong>Step 5: Add conversation starters</strong>\nCreate 4-5 suggested prompts:</p>\n<pre><code>1. &quot;[Question that showcases core capability 1]&quot;\n2. &quot;[Question that showcases core capability 2]&quot;\n3. &quot;[Common user question]&quot;\n4. &quot;[Edge case question]&quot;\n5. &quot;[Getting started question]&quot;\n</code></pre>\n<hr>\n"
        },
        {
          "id": "part-5:-test-and-iterate-(15-minutes)",
          "title": "Part 5: Test and Iterate (15 minutes)",
          "type": "generic",
          "content": "### Run Test Suite\n\nTest your assistant with these scenarios:\n\n**Basic functionality tests:**\n| Test | Input | Expected Behavior | Pass? |\n|------|-------|-------------------|-------|\n| Core use case | [Typical user question] | [What should happen] | |\n| Knowledge retrieval | [Question requiring documents] | [Accurate response] | |\n| Clarification | [Vague question] | [Asks for details] | |\n| Out of scope | [Question it shouldn't answer] | [Politely declines] | |\n\n**Stress tests:**\n| Test | Input | Expected Behavior | Pass? |\n|------|-------|-------------------|-------|\n| Long input | [Very detailed request] | [Handles gracefully] | |\n| Ambiguous request | [Could mean multiple things] | [Seeks clarification] | |\n| Edge case | [Unusual but valid question] | [Reasonable response] | |\n| Adversarial | [Attempt to confuse/break] | [Maintains composure] | |\n\n### Identify Issues\n\nFor each failed test:\n```\nISSUE: [Test name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nExpected: [What should have happened]\nActual: [What happened]\nRoot cause: [Why did it fail?]\nFix: [What to change]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Iteration Cycle\n\n1. Review failed tests\n2. Identify instruction improvements\n3. Update system instructions\n4. Re-test\n5. Document what worked\n\n**Track your iterations:**\n\n| Version | Changes Made | Tests Passed | Notes |\n|---------|--------------|--------------|-------|\n| v1 | Initial build | X/10 | [Issues] |\n| v2 | [Changes] | X/10 | [Improvement] |\n| v3 | [Changes] | X/10 | [Status] |\n\n---",
          "htmlContent": "<h3>Run Test Suite</h3>\n<p>Test your assistant with these scenarios:</p>\n<p><strong>Basic functionality tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test</th>\n<th>Input</th>\n<th>Expected Behavior</th>\n<th>Pass?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Core use case</td>\n<td>[Typical user question]</td>\n<td>[What should happen]</td>\n<td></td>\n</tr>\n<tr>\n<td>Knowledge retrieval</td>\n<td>[Question requiring documents]</td>\n<td>[Accurate response]</td>\n<td></td>\n</tr>\n<tr>\n<td>Clarification</td>\n<td>[Vague question]</td>\n<td>[Asks for details]</td>\n<td></td>\n</tr>\n<tr>\n<td>Out of scope</td>\n<td>[Question it shouldn&#39;t answer]</td>\n<td>[Politely declines]</td>\n<td></td>\n</tr>\n</tbody></table>\n<p><strong>Stress tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test</th>\n<th>Input</th>\n<th>Expected Behavior</th>\n<th>Pass?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Long input</td>\n<td>[Very detailed request]</td>\n<td>[Handles gracefully]</td>\n<td></td>\n</tr>\n<tr>\n<td>Ambiguous request</td>\n<td>[Could mean multiple things]</td>\n<td>[Seeks clarification]</td>\n<td></td>\n</tr>\n<tr>\n<td>Edge case</td>\n<td>[Unusual but valid question]</td>\n<td>[Reasonable response]</td>\n<td></td>\n</tr>\n<tr>\n<td>Adversarial</td>\n<td>[Attempt to confuse/break]</td>\n<td>[Maintains composure]</td>\n<td></td>\n</tr>\n</tbody></table>\n<h3>Identify Issues</h3>\n<p>For each failed test:</p>\n<pre><code>ISSUE: [Test name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nExpected: [What should have happened]\nActual: [What happened]\nRoot cause: [Why did it fail?]\nFix: [What to change]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<h3>Iteration Cycle</h3>\n<ol>\n<li>Review failed tests</li>\n<li>Identify instruction improvements</li>\n<li>Update system instructions</li>\n<li>Re-test</li>\n<li>Document what worked</li>\n</ol>\n<p><strong>Track your iterations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Version</th>\n<th>Changes Made</th>\n<th>Tests Passed</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>v1</td>\n<td>Initial build</td>\n<td>X/10</td>\n<td>[Issues]</td>\n</tr>\n<tr>\n<td>v2</td>\n<td>[Changes]</td>\n<td>X/10</td>\n<td>[Improvement]</td>\n</tr>\n<tr>\n<td>v3</td>\n<td>[Changes]</td>\n<td>X/10</td>\n<td>[Status]</td>\n</tr>\n</tbody></table>\n<hr>\n"
        },
        {
          "id": "part-6:-document-your-assistant-(5-minutes)",
          "title": "Part 6: Document Your Assistant (5 minutes)",
          "type": "generic",
          "content": "### Create User Guide\n\n```\n[ASSISTANT NAME] - User Guide\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nWHAT THIS ASSISTANT DOES:\n[Brief description]\n\nHOW TO USE IT:\n1. [Step 1]\n2. [Step 2]\n3. [Step 3]\n\nBEST FOR:\n• [Use case 1]\n• [Use case 2]\n• [Use case 3]\n\nNOT DESIGNED FOR:\n• [Limitation 1]\n• [Limitation 2]\n\nTIPS FOR BEST RESULTS:\n• [Tip 1]\n• [Tip 2]\n• [Tip 3]\n\nTROUBLESHOOTING:\nIf [problem] → Try [solution]\nIf [problem] → Try [solution]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n---",
          "htmlContent": "<h3>Create User Guide</h3>\n<pre><code>[ASSISTANT NAME] - User Guide\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nWHAT THIS ASSISTANT DOES:\n[Brief description]\n\nHOW TO USE IT:\n1. [Step 1]\n2. [Step 2]\n3. [Step 3]\n\nBEST FOR:\n• [Use case 1]\n• [Use case 2]\n• [Use case 3]\n\nNOT DESIGNED FOR:\n• [Limitation 1]\n• [Limitation 2]\n\nTIPS FOR BEST RESULTS:\n• [Tip 1]\n• [Tip 2]\n• [Tip 3]\n\nTROUBLESHOOTING:\nIf [problem] → Try [solution]\nIf [problem] → Try [solution]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<hr>\n"
        },
        {
          "id": "deliverable",
          "title": "Deliverable",
          "type": "generic",
          "content": "Create a complete assistant package:\n\n1. **Assistant Definition Document**\n   - Use case and purpose\n   - Target users\n   - Success criteria\n\n2. **System Instructions**\n   - Final version used\n   - Notes on what worked/didn't\n\n3. **Knowledge Base Inventory**\n   - List of documents uploaded\n   - Why each was included\n\n4. **Test Results**\n   - All test cases and results\n   - Iteration history\n\n5. **User Guide**\n   - Ready to share with potential users\n\n6. **Link to Working Assistant**\n   - Shareable link (if platform supports)\n   - Or screenshots showing it works\n\n---",
          "htmlContent": "<p>Create a complete assistant package:</p>\n<ol>\n<li><p><strong>Assistant Definition Document</strong></p>\n<ul>\n<li>Use case and purpose</li>\n<li>Target users</li>\n<li>Success criteria</li>\n</ul>\n</li>\n<li><p><strong>System Instructions</strong></p>\n<ul>\n<li>Final version used</li>\n<li>Notes on what worked/didn&#39;t</li>\n</ul>\n</li>\n<li><p><strong>Knowledge Base Inventory</strong></p>\n<ul>\n<li>List of documents uploaded</li>\n<li>Why each was included</li>\n</ul>\n</li>\n<li><p><strong>Test Results</strong></p>\n<ul>\n<li>All test cases and results</li>\n<li>Iteration history</li>\n</ul>\n</li>\n<li><p><strong>User Guide</strong></p>\n<ul>\n<li>Ready to share with potential users</li>\n</ul>\n</li>\n<li><p><strong>Link to Working Assistant</strong></p>\n<ul>\n<li>Shareable link (if platform supports)</li>\n<li>Or screenshots showing it works</li>\n</ul>\n</li>\n</ol>\n<hr>\n"
        },
        {
          "id": "extension-challenge",
          "title": "Extension Challenge",
          "type": "generic",
          "content": "**Usage Pilot**\n\n1. Share your assistant with 3-5 real users\n2. Collect feedback:\n   - What worked well?\n   - What was confusing?\n   - What's missing?\n3. Make improvements based on feedback\n4. Document the iteration\n\nThis real-world testing is where you'll learn the most.",
          "htmlContent": "<p><strong>Usage Pilot</strong></p>\n<ol>\n<li>Share your assistant with 3-5 real users</li>\n<li>Collect feedback:<ul>\n<li>What worked well?</li>\n<li>What was confusing?</li>\n<li>What&#39;s missing?</li>\n</ul>\n</li>\n<li>Make improvements based on feedback</li>\n<li>Document the iteration</li>\n</ol>\n<p>This real-world testing is where you&#39;ll learn the most.</p>\n"
        }
      ]
    },
    {
      "id": "lab-5b-multi-agent-orchestration",
      "slug": "lab-5b-multi-agent-orchestration",
      "title": "Multi-Agent Orchestration",
      "phase": 3,
      "labNumber": 5.5,
      "estimatedMinutes": 90,
      "objectives": [
        "Design a multi-agent system with specialized roles",
        "Implement Worker and Manager agent loops",
        "Create inter-agent communication protocols",
        "Build fault-tolerant agent orchestration"
      ],
      "prerequisites": [
        "3.1-no-code-ai-tools",
        "3.2-api-fundamentals",
        "3.3-automation-platforms",
        "lab-5-build-ai-assistant"
      ],
      "content": "# Lab 5b: Multi-Agent Orchestration\n\n## Lab Overview\n\nLab 5 taught you to build a single AI assistant. But real business problems are too complex for one agent.\n\nThis lab teaches you to orchestrate **multiple AI agents** that collaborate, check each other's work, and handle tasks too complex for any single agent.\n\nThis is the difference between:\n- **Automation**: Pre-defined steps executed in sequence\n- **Agentic AI**: Agents that reason, delegate, and adapt\n\n**What you'll create:**\n- A multi-agent system with Worker and Manager agents\n- Defined communication protocols between agents\n- Quality gates and verification loops\n- A fault-tolerant orchestration pattern\n\n---\n\n## The Multi-Agent Mindset\n\n> **One agent is a tool. Multiple agents are a team.**\n\nSingle agents hit walls:\n- Context windows overflow\n- Specialized tasks suffer from generalist prompts\n- No error checking or verification\n- Single point of failure\n\nMulti-agent systems solve these through **division of labor** and **checks and balances**.\n\n### Real-World Multi-Agent Examples\n\n| System | Worker Agents | Manager Agent | Value |\n|--------|--------------|---------------|-------|\n| **Research Pipeline** | Web searcher, paper analyzer, fact checker | Research coordinator | Verified, comprehensive research |\n| **Content Creation** | Writer, editor, fact-checker, SEO analyzer | Content lead | Publication-ready content |\n| **Customer Support** | Intent classifier, knowledge retriever, response drafter, sentiment analyzer | Support orchestrator | Faster, better responses |\n| **Code Review** | Syntax checker, security scanner, style analyzer, documentation verifier | Review coordinator | Comprehensive code review |\n\n---\n\n## Part 1: Understand Agent Patterns (15 minutes)\n\n### Pattern 1: Worker-Manager Hierarchy\n\n```\n                    ┌─────────────┐\n                    │   MANAGER   │\n                    │   AGENT     │\n                    └──────┬──────┘\n                           │ Delegates tasks\n            ┌──────────────┼──────────────┐\n            ▼              ▼              ▼\n     ┌──────────┐   ┌──────────┐   ┌──────────┐\n     │ WORKER 1 │   │ WORKER 2 │   │ WORKER 3 │\n     │ Research │   │ Analysis │   │ Writing  │\n     └──────────┘   └──────────┘   └──────────┘\n            │              │              │\n            └──────────────┴──────────────┘\n                           │ Return results\n                    ┌──────▼──────┐\n                    │   MANAGER   │\n                    │  Synthesizes│\n                    └─────────────┘\n```\n\n**Manager Agent responsibilities:**\n- Breaks complex task into subtasks\n- Assigns work to appropriate workers\n- Tracks progress and handles failures\n- Synthesizes results into final output\n\n**Worker Agent responsibilities:**\n- Specialized at one task type\n- Executes assigned work\n- Reports success, failure, or need for clarification\n- Stays within defined scope\n\n### Pattern 2: Pipeline with Verification\n\n```\nINPUT → [Agent A: Draft] → [Agent B: Review] → [Agent A: Revise] → OUTPUT\n              ↑                                        │\n              └────────────── Feedback loop ───────────┘\n```\n\nEach agent passes work to the next, with potential loops for refinement.\n\n### Pattern 3: Consensus/Voting\n\n```\nQUERY → [Agent A] ─┐\n      → [Agent B] ─┼→ [Aggregator: Compare & Synthesize] → OUTPUT\n      → [Agent C] ─┘\n```\n\nMultiple agents answer the same question; an aggregator combines or selects the best response.\n\n### Pattern 4: Specialist Routing\n\n```\n                         ┌─ [Technical Agent] → Technical Response\nINPUT → [Router Agent] ──┼─ [Sales Agent] → Sales Response\n                         └─ [Support Agent] → Support Response\n```\n\nA router determines which specialist should handle each request.\n\n### Exercise: Identify the Pattern\n\nFor each scenario, identify the best multi-agent pattern:\n\n| Scenario | Best Pattern | Why? |\n|----------|--------------|------|\n| Legal contract review requiring multiple expertise areas | [Your answer] | [Your reasoning] |\n| Customer inquiry that could be billing, technical, or sales | [Your answer] | [Your reasoning] |\n| Research report requiring web search, analysis, and writing | [Your answer] | [Your reasoning] |\n| Medical symptom assessment needing high reliability | [Your answer] | [Your reasoning] |\n\n---\n\n## Part 2: Design Your Multi-Agent System (20 minutes)\n\n### Choose Your Scenario\n\nPick one complex task that genuinely requires multiple agents:\n\n**Scenario A: Research Brief Generator**\n```\nInput: Topic + Research Question\nAgents needed:\n  - Web Researcher: Finds and summarizes sources\n  - Fact Checker: Verifies claims against sources\n  - Synthesizer: Combines findings into brief\n  - Quality Reviewer: Checks completeness and accuracy\nOutput: Research brief with citations\n```\n\n**Scenario B: Content Review Pipeline**\n```\nInput: Draft blog post or article\nAgents needed:\n  - Editor: Grammar, clarity, structure\n  - Fact Checker: Verifies claims\n  - Brand Voice Checker: Alignment with tone guidelines\n  - SEO Analyzer: Keyword and metadata review\nOutput: Publication-ready content with change log\n```\n\n**Scenario C: Customer Query Resolver**\n```\nInput: Customer message\nAgents needed:\n  - Intent Classifier: What does the customer need?\n  - Knowledge Retriever: Find relevant information\n  - Response Drafter: Create initial response\n  - Empathy Reviewer: Check tone and helpfulness\nOutput: Customer-ready response\n```\n\n**Scenario D: Your Business Process**\n```\nInput: [Real input from your work]\nAgents needed:\n  - [Agent 1]: [Specialized task]\n  - [Agent 2]: [Specialized task]\n  - [Agent 3]: [Verification/synthesis]\nOutput: [Business deliverable]\n```\n\n### Document Your Agent Design\n\n```\nMULTI-AGENT SYSTEM DESIGN\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nSYSTEM NAME: [Name]\n\nPROBLEM STATEMENT:\n[What complex task does this system handle?]\n[Why can't a single agent do this effectively?]\n\nPATTERN: [Worker-Manager / Pipeline / Consensus / Routing]\n\nINPUT:\n[What triggers the system?]\n[What data is provided?]\n\nOUTPUT:\n[What deliverable is produced?]\n[What quality standards must it meet?]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nAGENT ROSTER:\n\nAgent 1: [NAME]\n─────────────────\nRole: [Manager / Worker]\nResponsibility: [What this agent does]\nInputs: [What it receives]\nOutputs: [What it produces]\nModel: [Which LLM - consider cost/capability tradeoffs]\nKey prompt elements:\n  - [Instruction 1]\n  - [Instruction 2]\n  - [Output format]\n\nAgent 2: [NAME]\n─────────────────\nRole: [Manager / Worker]\nResponsibility: [What this agent does]\nInputs: [What it receives]\nOutputs: [What it produces]\nModel: [Which LLM]\nKey prompt elements:\n  - [Instruction 1]\n  - [Instruction 2]\n  - [Output format]\n\nAgent 3: [NAME]\n─────────────────\nRole: [Manager / Worker]\nResponsibility: [What this agent does]\nInputs: [What it receives]\nOutputs: [What it produces]\nModel: [Which LLM]\nKey prompt elements:\n  - [Instruction 1]\n  - [Instruction 2]\n  - [Output format]\n\n[Add more agents as needed]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n---\n\n## Part 3: Design Communication Protocols (15 minutes)\n\n### Agent Message Format\n\nAgents need structured communication. Define a standard message format:\n\n```json\n{\n  \"from_agent\": \"researcher\",\n  \"to_agent\": \"manager\",\n  \"message_type\": \"work_complete|needs_input|error|status\",\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"task_id\": \"research-001\",\n  \"status\": \"success|partial|failure\",\n  \"payload\": {\n    \"result\": \"[The work output]\",\n    \"confidence\": \"high|medium|low\",\n    \"sources\": [\"[citations if applicable]\"],\n    \"notes\": \"[Any caveats or observations]\"\n  },\n  \"next_action\": \"proceed|retry|escalate|clarify\"\n}\n```\n\n### Define Your Communication Protocol\n\n```\nINTER-AGENT COMMUNICATION PROTOCOL\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nWORKFLOW SEQUENCE:\n\nStep 1: [Manager] receives initial request\n        ↓ Parses and creates task breakdown\n        ↓ Sends to [Worker A]\n\nStep 2: [Worker A] receives task\n        ↓ Executes specialized work\n        ↓ Returns result to [Manager]\n\nStep 3: [Manager] evaluates result\n        ↓ IF quality OK → Send to [Worker B]\n        ↓ IF quality NOT OK → Return to [Worker A] with feedback\n\nStep 4: [Worker B] receives work + prior results\n        ↓ Executes next phase\n        ↓ Returns to [Manager]\n\nStep 5: [Manager] synthesizes all results\n        ↓ Produces final output\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nMESSAGE TYPES:\n\nTASK_ASSIGNMENT (Manager → Worker)\n{\n  \"task\": \"[What to do]\",\n  \"context\": \"[Relevant background]\",\n  \"constraints\": \"[Time, format, scope limits]\",\n  \"success_criteria\": \"[How to know it's done well]\"\n}\n\nWORK_RESULT (Worker → Manager)\n{\n  \"result\": \"[The output]\",\n  \"quality_self_assessment\": \"[Worker's view of quality]\",\n  \"blockers\": \"[Any issues encountered]\",\n  \"recommendations\": \"[Suggestions for next steps]\"\n}\n\nFEEDBACK (Manager → Worker)\n{\n  \"assessment\": \"[What was good/what needs work]\",\n  \"specific_requests\": \"[Exact changes needed]\",\n  \"priority\": \"[How urgent]\"\n}\n\nFINAL_OUTPUT (Manager → System)\n{\n  \"deliverable\": \"[The final product]\",\n  \"process_summary\": \"[What agents did]\",\n  \"confidence\": \"[Overall quality assessment]\",\n  \"recommendations\": \"[Next steps if any]\"\n}\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Quality Gates\n\nDefine checkpoints where agents verify work:\n\n```\nQUALITY GATES\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nGate 1: After [Agent A] completes\nCriteria:\n  - [ ] Output format correct\n  - [ ] Required fields present\n  - [ ] No obvious errors\n  - [ ] Confidence above threshold\nPass → Continue to next agent\nFail → Retry (max 2) or escalate\n\nGate 2: After [Agent B] completes\nCriteria:\n  - [ ] Builds correctly on prior work\n  - [ ] Quality maintained or improved\n  - [ ] No contradictions introduced\nPass → Continue\nFail → Return to previous agent with feedback\n\nGate 3: Final synthesis\nCriteria:\n  - [ ] All components integrated\n  - [ ] Meets original request requirements\n  - [ ] Quality standards achieved\n  - [ ] Ready for end user\nPass → Deliver output\nFail → Identify weakest component, request revision\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n---\n\n## Part 4: Build the System (25 minutes)\n\n### Implementation Options\n\nChoose your implementation approach:\n\n**Option A: Prompt Chain (Simplest)**\n- Execute agents sequentially with copy/paste between\n- Good for prototyping and testing\n- Manual orchestration\n\n**Option B: Automation Platform**\n- Use Zapier/Make/n8n to orchestrate\n- Each agent is an AI action in the workflow\n- Automatic data passing between agents\n\n**Option C: Claude Projects / GPT Assistants**\n- Create separate assistants for each agent role\n- Programmatically call each in sequence\n- More sophisticated but requires API access\n\n**Option D: Code-Based (Advanced)**\n- Build with LangChain, CrewAI, or AutoGen\n- Full control over orchestration\n- Requires programming knowledge\n\n### Write Agent Prompts\n\nFor each agent in your system, write a complete prompt:\n\n**MANAGER AGENT PROMPT TEMPLATE:**\n```\nYou are the Manager Agent for a [SYSTEM PURPOSE] system.\n\nYOUR ROLE:\nYou coordinate a team of specialized worker agents to accomplish complex tasks.\nYou do NOT do the specialized work yourself—you delegate, evaluate, and synthesize.\n\nWORKER AGENTS AVAILABLE:\n1. [Worker A Name]: [What they do]\n2. [Worker B Name]: [What they do]\n3. [Worker C Name]: [What they do]\n\nYOUR PROCESS:\n1. Analyze the incoming request\n2. Break it into subtasks for your workers\n3. Evaluate each worker's output\n4. Request revisions if quality insufficient\n5. Synthesize final deliverable\n\nWHEN YOU RECEIVE A NEW REQUEST:\n1. Confirm you understand the request\n2. Create a task breakdown:\n   - Task for [Worker A]: [specific instructions]\n   - Task for [Worker B]: [specific instructions]\n   - Task for [Worker C]: [specific instructions]\n3. Define success criteria for each task\n\nWHEN YOU RECEIVE WORKER OUTPUT:\n1. Evaluate against success criteria\n2. If PASS: Acknowledge and proceed to next step\n3. If FAIL: Provide specific feedback and request revision\n\nWHEN ALL WORK IS COMPLETE:\n1. Synthesize outputs into final deliverable\n2. Note any limitations or caveats\n3. Suggest follow-up actions if relevant\n\nOUTPUT FORMAT:\n[Define structured output format for manager communications]\n```\n\n**WORKER AGENT PROMPT TEMPLATE:**\n```\nYou are the [WORKER NAME] Agent, a specialist in [SPECIALTY].\n\nYOUR ROLE:\nYou are part of a multi-agent system managed by a Manager Agent.\nYou receive specific tasks and return high-quality results.\n\nYOUR SPECIALTY:\n[Detailed description of what this agent does well]\n\nYOUR SCOPE:\nDO: [What this agent should do]\nDON'T: [What this agent should NOT attempt]\n\nWHEN YOU RECEIVE A TASK:\n1. Confirm you understand the assignment\n2. Execute your specialized work\n3. Self-assess your output quality\n4. Return results in the specified format\n\nOUTPUT FORMAT:\n{\n  \"status\": \"complete|partial|blocked\",\n  \"result\": \"[Your work output]\",\n  \"confidence\": \"high|medium|low\",\n  \"notes\": \"[Any caveats or observations]\",\n  \"needs_from_manager\": \"[Any clarification or resources needed]\"\n}\n\nQUALITY STANDARDS:\n- [Standard 1]\n- [Standard 2]\n- [Standard 3]\n\nIf you cannot complete the task to quality standards, explain why\nrather than producing subpar work.\n```\n\n### Test Individual Agents\n\nBefore orchestrating, test each agent in isolation:\n\n| Agent | Test Input | Expected Output | Actual Output | Pass? |\n|-------|------------|-----------------|---------------|-------|\n| Manager | Sample request | Task breakdown | [Result] | ☐ |\n| Worker A | Sample task | Specialized output | [Result] | ☐ |\n| Worker B | Sample task | Specialized output | [Result] | ☐ |\n| Worker C | Sample task | Specialized output | [Result] | ☐ |\n\n---\n\n## Part 5: Error Handling & Resilience (10 minutes)\n\n### Failure Modes\n\nMulti-agent systems can fail in new ways:\n\n| Failure Mode | Cause | Mitigation |\n|--------------|-------|------------|\n| **Agent timeout** | Slow or stuck agent | Set timeouts, retry logic |\n| **Quality spiral** | Agents keep rejecting each other's work | Max revision limit, human escalation |\n| **Context loss** | Key info not passed between agents | Structured message formats |\n| **Infinite loops** | Circular dependencies | Loop detection, max iterations |\n| **Cascading failures** | One agent failure breaks chain | Fallback behaviors, partial output |\n\n### Define Your Error Handling\n\n```\nERROR HANDLING PROTOCOL\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nTIMEOUTS:\n- Worker agents: [X] seconds max per task\n- Full pipeline: [Y] seconds max end-to-end\n- On timeout: [Action]\n\nRETRY POLICY:\n- Max retries per agent: [N]\n- Retry triggers: [What causes retry]\n- Retry with changes: [What gets modified]\n\nREVISION LIMITS:\n- Max Manager → Worker revision loops: [N]\n- On limit reached: [Action]\n\nESCALATION:\n- Trigger: [What causes escalation]\n- Escalate to: [Human role or backup system]\n- Context provided: [What info accompanies escalation]\n\nPARTIAL OUTPUT POLICY:\n- When acceptable: [Conditions]\n- When not acceptable: [Conditions]\n- Format: [How to present partial results]\n\nLOGGING:\n- Log all inter-agent messages: [Yes/No]\n- Log final output: [Yes/No]\n- Log errors: [Yes/No]\n- Retention: [How long]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n---\n\n## Part 6: End-to-End Test (10 minutes)\n\n### Run Complete System\n\nExecute your multi-agent system with a realistic input:\n\n```\nEND-TO-END TEST DOCUMENTATION\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nTEST CASE 1: Standard Input\nInput: [Realistic test input]\n\nAgent Execution Log:\n┌─────────────────────────────────────────────────────┐\n│ [Timestamp] Manager: Received request               │\n│ [Timestamp] Manager: Created task breakdown         │\n│ [Timestamp] Manager → Worker A: Assigned task       │\n│ [Timestamp] Worker A: Task complete, returning      │\n│ [Timestamp] Manager: Evaluated, quality PASS        │\n│ [Timestamp] Manager → Worker B: Assigned task       │\n│ [Timestamp] Worker B: Task complete, returning      │\n│ [Timestamp] Manager: Evaluated, quality PASS        │\n│ [Timestamp] Manager: Synthesizing final output      │\n│ [Timestamp] System: Output delivered                │\n└─────────────────────────────────────────────────────┘\n\nOutput Quality Assessment:\n- Completeness: [1-5]\n- Accuracy: [1-5]\n- Format: [1-5]\n- Meets requirements: [Yes/No]\n\nTime to complete: [X seconds/minutes]\nTokens used: [Approximate]\nCost: [Estimated]\n\nTEST CASE 2: Edge Case\nInput: [Unusual or challenging input]\n[Same logging format]\n\nTEST CASE 3: Error Condition\nInput: [Input designed to trigger failure handling]\n[Same logging format]\nError handling worked correctly: [Yes/No]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n---\n\n## Deliverable\n\nCreate a complete multi-agent system package:\n\n### 1. System Architecture Document\n- Pattern used (Worker-Manager, Pipeline, etc.)\n- Agent roster with responsibilities\n- Communication flow diagram\n- Quality gates defined\n\n### 2. Agent Prompts (Complete)\n- Manager agent prompt\n- All worker agent prompts\n- Message format specifications\n\n### 3. Communication Protocol\n- Message types and formats\n- Handoff procedures\n- Quality gate criteria\n\n### 4. Error Handling Plan\n- Failure modes identified\n- Retry and escalation policies\n- Partial output handling\n\n### 5. Test Results\n- Individual agent tests\n- End-to-end test cases\n- Performance metrics (time, tokens, cost)\n\n### 6. Comparison Analysis\n```\nSINGLE AGENT VS. MULTI-AGENT COMPARISON\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nSame task performed by:\nA) Single generalist agent\nB) Your multi-agent system\n\nMetric            | Single Agent | Multi-Agent | Winner\n──────────────────┼──────────────┼─────────────┼────────\nQuality score     │ [1-5]        │ [1-5]       │ [A/B]\nCompleteness      │ [1-5]        │ [1-5]       │ [A/B]\nTime to complete  │ [X sec]      │ [Y sec]     │ [A/B]\nToken usage       │ [X]          │ [Y]         │ [A/B]\nCost              │ [$X]         │ [$Y]        │ [A/B]\nError handling    │ [1-5]        │ [1-5]       │ [A/B]\n\nVERDICT:\n[When is multi-agent worth it for this task?]\n[What task complexity threshold justifies the overhead?]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n---\n\n## Extension Challenge\n\n### Advanced: Self-Improving Agent System\n\nDesign (and optionally build) a system where agents improve over time:\n\n1. **Feedback Collection Agent**\n   - Gathers quality ratings on outputs\n   - Identifies patterns in failures\n   - Tracks which agent contributions are strongest/weakest\n\n2. **Prompt Optimization Agent**\n   - Analyzes feedback patterns\n   - Suggests prompt modifications\n   - A/B tests prompt variations\n\n3. **Performance Dashboard**\n   - Tracks quality over time\n   - Monitors cost efficiency\n   - Alerts on degradation\n\nDocument your design:\n- How would agents learn from feedback?\n- What metrics would you track?\n- How would you prevent drift or degradation?\n- What human oversight would remain?\n\n---\n\n## Key Takeaways\n\n1. **Multi-agent systems solve complexity** that single agents cannot handle\n2. **Clear roles and protocols** prevent chaos and duplication\n3. **Quality gates** catch errors before they propagate\n4. **Error handling is essential** — agents fail in new ways\n5. **Measure the tradeoff** — multi-agent overhead must be justified by quality gains",
      "htmlContent": "<h1>Lab 5b: Multi-Agent Orchestration</h1>\n<h2>Lab Overview</h2>\n<p>Lab 5 taught you to build a single AI assistant. But real business problems are too complex for one agent.</p>\n<p>This lab teaches you to orchestrate <strong>multiple AI agents</strong> that collaborate, check each other&#39;s work, and handle tasks too complex for any single agent.</p>\n<p>This is the difference between:</p>\n<ul>\n<li><strong>Automation</strong>: Pre-defined steps executed in sequence</li>\n<li><strong>Agentic AI</strong>: Agents that reason, delegate, and adapt</li>\n</ul>\n<p><strong>What you&#39;ll create:</strong></p>\n<ul>\n<li>A multi-agent system with Worker and Manager agents</li>\n<li>Defined communication protocols between agents</li>\n<li>Quality gates and verification loops</li>\n<li>A fault-tolerant orchestration pattern</li>\n</ul>\n<hr>\n<h2>The Multi-Agent Mindset</h2>\n<blockquote>\n<p><strong>One agent is a tool. Multiple agents are a team.</strong></p>\n</blockquote>\n<p>Single agents hit walls:</p>\n<ul>\n<li>Context windows overflow</li>\n<li>Specialized tasks suffer from generalist prompts</li>\n<li>No error checking or verification</li>\n<li>Single point of failure</li>\n</ul>\n<p>Multi-agent systems solve these through <strong>division of labor</strong> and <strong>checks and balances</strong>.</p>\n<h3>Real-World Multi-Agent Examples</h3>\n<table>\n<thead>\n<tr>\n<th>System</th>\n<th>Worker Agents</th>\n<th>Manager Agent</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Research Pipeline</strong></td>\n<td>Web searcher, paper analyzer, fact checker</td>\n<td>Research coordinator</td>\n<td>Verified, comprehensive research</td>\n</tr>\n<tr>\n<td><strong>Content Creation</strong></td>\n<td>Writer, editor, fact-checker, SEO analyzer</td>\n<td>Content lead</td>\n<td>Publication-ready content</td>\n</tr>\n<tr>\n<td><strong>Customer Support</strong></td>\n<td>Intent classifier, knowledge retriever, response drafter, sentiment analyzer</td>\n<td>Support orchestrator</td>\n<td>Faster, better responses</td>\n</tr>\n<tr>\n<td><strong>Code Review</strong></td>\n<td>Syntax checker, security scanner, style analyzer, documentation verifier</td>\n<td>Review coordinator</td>\n<td>Comprehensive code review</td>\n</tr>\n</tbody></table>\n<hr>\n<h2>Part 1: Understand Agent Patterns (15 minutes)</h2>\n<h3>Pattern 1: Worker-Manager Hierarchy</h3>\n<pre><code>                    ┌─────────────┐\n                    │   MANAGER   │\n                    │   AGENT     │\n                    └──────┬──────┘\n                           │ Delegates tasks\n            ┌──────────────┼──────────────┐\n            ▼              ▼              ▼\n     ┌──────────┐   ┌──────────┐   ┌──────────┐\n     │ WORKER 1 │   │ WORKER 2 │   │ WORKER 3 │\n     │ Research │   │ Analysis │   │ Writing  │\n     └──────────┘   └──────────┘   └──────────┘\n            │              │              │\n            └──────────────┴──────────────┘\n                           │ Return results\n                    ┌──────▼──────┐\n                    │   MANAGER   │\n                    │  Synthesizes│\n                    └─────────────┘\n</code></pre>\n<p><strong>Manager Agent responsibilities:</strong></p>\n<ul>\n<li>Breaks complex task into subtasks</li>\n<li>Assigns work to appropriate workers</li>\n<li>Tracks progress and handles failures</li>\n<li>Synthesizes results into final output</li>\n</ul>\n<p><strong>Worker Agent responsibilities:</strong></p>\n<ul>\n<li>Specialized at one task type</li>\n<li>Executes assigned work</li>\n<li>Reports success, failure, or need for clarification</li>\n<li>Stays within defined scope</li>\n</ul>\n<h3>Pattern 2: Pipeline with Verification</h3>\n<pre><code>INPUT → [Agent A: Draft] → [Agent B: Review] → [Agent A: Revise] → OUTPUT\n              ↑                                        │\n              └────────────── Feedback loop ───────────┘\n</code></pre>\n<p>Each agent passes work to the next, with potential loops for refinement.</p>\n<h3>Pattern 3: Consensus/Voting</h3>\n<pre><code>QUERY → [Agent A] ─┐\n      → [Agent B] ─┼→ [Aggregator: Compare &amp; Synthesize] → OUTPUT\n      → [Agent C] ─┘\n</code></pre>\n<p>Multiple agents answer the same question; an aggregator combines or selects the best response.</p>\n<h3>Pattern 4: Specialist Routing</h3>\n<pre><code>                         ┌─ [Technical Agent] → Technical Response\nINPUT → [Router Agent] ──┼─ [Sales Agent] → Sales Response\n                         └─ [Support Agent] → Support Response\n</code></pre>\n<p>A router determines which specialist should handle each request.</p>\n<h3>Exercise: Identify the Pattern</h3>\n<p>For each scenario, identify the best multi-agent pattern:</p>\n<table>\n<thead>\n<tr>\n<th>Scenario</th>\n<th>Best Pattern</th>\n<th>Why?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Legal contract review requiring multiple expertise areas</td>\n<td>[Your answer]</td>\n<td>[Your reasoning]</td>\n</tr>\n<tr>\n<td>Customer inquiry that could be billing, technical, or sales</td>\n<td>[Your answer]</td>\n<td>[Your reasoning]</td>\n</tr>\n<tr>\n<td>Research report requiring web search, analysis, and writing</td>\n<td>[Your answer]</td>\n<td>[Your reasoning]</td>\n</tr>\n<tr>\n<td>Medical symptom assessment needing high reliability</td>\n<td>[Your answer]</td>\n<td>[Your reasoning]</td>\n</tr>\n</tbody></table>\n<hr>\n<h2>Part 2: Design Your Multi-Agent System (20 minutes)</h2>\n<h3>Choose Your Scenario</h3>\n<p>Pick one complex task that genuinely requires multiple agents:</p>\n<p><strong>Scenario A: Research Brief Generator</strong></p>\n<pre><code>Input: Topic + Research Question\nAgents needed:\n  - Web Researcher: Finds and summarizes sources\n  - Fact Checker: Verifies claims against sources\n  - Synthesizer: Combines findings into brief\n  - Quality Reviewer: Checks completeness and accuracy\nOutput: Research brief with citations\n</code></pre>\n<p><strong>Scenario B: Content Review Pipeline</strong></p>\n<pre><code>Input: Draft blog post or article\nAgents needed:\n  - Editor: Grammar, clarity, structure\n  - Fact Checker: Verifies claims\n  - Brand Voice Checker: Alignment with tone guidelines\n  - SEO Analyzer: Keyword and metadata review\nOutput: Publication-ready content with change log\n</code></pre>\n<p><strong>Scenario C: Customer Query Resolver</strong></p>\n<pre><code>Input: Customer message\nAgents needed:\n  - Intent Classifier: What does the customer need?\n  - Knowledge Retriever: Find relevant information\n  - Response Drafter: Create initial response\n  - Empathy Reviewer: Check tone and helpfulness\nOutput: Customer-ready response\n</code></pre>\n<p><strong>Scenario D: Your Business Process</strong></p>\n<pre><code>Input: [Real input from your work]\nAgents needed:\n  - [Agent 1]: [Specialized task]\n  - [Agent 2]: [Specialized task]\n  - [Agent 3]: [Verification/synthesis]\nOutput: [Business deliverable]\n</code></pre>\n<h3>Document Your Agent Design</h3>\n<pre><code>MULTI-AGENT SYSTEM DESIGN\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nSYSTEM NAME: [Name]\n\nPROBLEM STATEMENT:\n[What complex task does this system handle?]\n[Why can&#39;t a single agent do this effectively?]\n\nPATTERN: [Worker-Manager / Pipeline / Consensus / Routing]\n\nINPUT:\n[What triggers the system?]\n[What data is provided?]\n\nOUTPUT:\n[What deliverable is produced?]\n[What quality standards must it meet?]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nAGENT ROSTER:\n\nAgent 1: [NAME]\n─────────────────\nRole: [Manager / Worker]\nResponsibility: [What this agent does]\nInputs: [What it receives]\nOutputs: [What it produces]\nModel: [Which LLM - consider cost/capability tradeoffs]\nKey prompt elements:\n  - [Instruction 1]\n  - [Instruction 2]\n  - [Output format]\n\nAgent 2: [NAME]\n─────────────────\nRole: [Manager / Worker]\nResponsibility: [What this agent does]\nInputs: [What it receives]\nOutputs: [What it produces]\nModel: [Which LLM]\nKey prompt elements:\n  - [Instruction 1]\n  - [Instruction 2]\n  - [Output format]\n\nAgent 3: [NAME]\n─────────────────\nRole: [Manager / Worker]\nResponsibility: [What this agent does]\nInputs: [What it receives]\nOutputs: [What it produces]\nModel: [Which LLM]\nKey prompt elements:\n  - [Instruction 1]\n  - [Instruction 2]\n  - [Output format]\n\n[Add more agents as needed]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<hr>\n<h2>Part 3: Design Communication Protocols (15 minutes)</h2>\n<h3>Agent Message Format</h3>\n<p>Agents need structured communication. Define a standard message format:</p>\n<pre><code class=\"language-json\">{\n  &quot;from_agent&quot;: &quot;researcher&quot;,\n  &quot;to_agent&quot;: &quot;manager&quot;,\n  &quot;message_type&quot;: &quot;work_complete|needs_input|error|status&quot;,\n  &quot;timestamp&quot;: &quot;2024-01-15T10:30:00Z&quot;,\n  &quot;task_id&quot;: &quot;research-001&quot;,\n  &quot;status&quot;: &quot;success|partial|failure&quot;,\n  &quot;payload&quot;: {\n    &quot;result&quot;: &quot;[The work output]&quot;,\n    &quot;confidence&quot;: &quot;high|medium|low&quot;,\n    &quot;sources&quot;: [&quot;[citations if applicable]&quot;],\n    &quot;notes&quot;: &quot;[Any caveats or observations]&quot;\n  },\n  &quot;next_action&quot;: &quot;proceed|retry|escalate|clarify&quot;\n}\n</code></pre>\n<h3>Define Your Communication Protocol</h3>\n<pre><code>INTER-AGENT COMMUNICATION PROTOCOL\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nWORKFLOW SEQUENCE:\n\nStep 1: [Manager] receives initial request\n        ↓ Parses and creates task breakdown\n        ↓ Sends to [Worker A]\n\nStep 2: [Worker A] receives task\n        ↓ Executes specialized work\n        ↓ Returns result to [Manager]\n\nStep 3: [Manager] evaluates result\n        ↓ IF quality OK → Send to [Worker B]\n        ↓ IF quality NOT OK → Return to [Worker A] with feedback\n\nStep 4: [Worker B] receives work + prior results\n        ↓ Executes next phase\n        ↓ Returns to [Manager]\n\nStep 5: [Manager] synthesizes all results\n        ↓ Produces final output\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nMESSAGE TYPES:\n\nTASK_ASSIGNMENT (Manager → Worker)\n{\n  &quot;task&quot;: &quot;[What to do]&quot;,\n  &quot;context&quot;: &quot;[Relevant background]&quot;,\n  &quot;constraints&quot;: &quot;[Time, format, scope limits]&quot;,\n  &quot;success_criteria&quot;: &quot;[How to know it&#39;s done well]&quot;\n}\n\nWORK_RESULT (Worker → Manager)\n{\n  &quot;result&quot;: &quot;[The output]&quot;,\n  &quot;quality_self_assessment&quot;: &quot;[Worker&#39;s view of quality]&quot;,\n  &quot;blockers&quot;: &quot;[Any issues encountered]&quot;,\n  &quot;recommendations&quot;: &quot;[Suggestions for next steps]&quot;\n}\n\nFEEDBACK (Manager → Worker)\n{\n  &quot;assessment&quot;: &quot;[What was good/what needs work]&quot;,\n  &quot;specific_requests&quot;: &quot;[Exact changes needed]&quot;,\n  &quot;priority&quot;: &quot;[How urgent]&quot;\n}\n\nFINAL_OUTPUT (Manager → System)\n{\n  &quot;deliverable&quot;: &quot;[The final product]&quot;,\n  &quot;process_summary&quot;: &quot;[What agents did]&quot;,\n  &quot;confidence&quot;: &quot;[Overall quality assessment]&quot;,\n  &quot;recommendations&quot;: &quot;[Next steps if any]&quot;\n}\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<h3>Quality Gates</h3>\n<p>Define checkpoints where agents verify work:</p>\n<pre><code>QUALITY GATES\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nGate 1: After [Agent A] completes\nCriteria:\n  - [ ] Output format correct\n  - [ ] Required fields present\n  - [ ] No obvious errors\n  - [ ] Confidence above threshold\nPass → Continue to next agent\nFail → Retry (max 2) or escalate\n\nGate 2: After [Agent B] completes\nCriteria:\n  - [ ] Builds correctly on prior work\n  - [ ] Quality maintained or improved\n  - [ ] No contradictions introduced\nPass → Continue\nFail → Return to previous agent with feedback\n\nGate 3: Final synthesis\nCriteria:\n  - [ ] All components integrated\n  - [ ] Meets original request requirements\n  - [ ] Quality standards achieved\n  - [ ] Ready for end user\nPass → Deliver output\nFail → Identify weakest component, request revision\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<hr>\n<h2>Part 4: Build the System (25 minutes)</h2>\n<h3>Implementation Options</h3>\n<p>Choose your implementation approach:</p>\n<p><strong>Option A: Prompt Chain (Simplest)</strong></p>\n<ul>\n<li>Execute agents sequentially with copy/paste between</li>\n<li>Good for prototyping and testing</li>\n<li>Manual orchestration</li>\n</ul>\n<p><strong>Option B: Automation Platform</strong></p>\n<ul>\n<li>Use Zapier/Make/n8n to orchestrate</li>\n<li>Each agent is an AI action in the workflow</li>\n<li>Automatic data passing between agents</li>\n</ul>\n<p><strong>Option C: Claude Projects / GPT Assistants</strong></p>\n<ul>\n<li>Create separate assistants for each agent role</li>\n<li>Programmatically call each in sequence</li>\n<li>More sophisticated but requires API access</li>\n</ul>\n<p><strong>Option D: Code-Based (Advanced)</strong></p>\n<ul>\n<li>Build with LangChain, CrewAI, or AutoGen</li>\n<li>Full control over orchestration</li>\n<li>Requires programming knowledge</li>\n</ul>\n<h3>Write Agent Prompts</h3>\n<p>For each agent in your system, write a complete prompt:</p>\n<p><strong>MANAGER AGENT PROMPT TEMPLATE:</strong></p>\n<pre><code>You are the Manager Agent for a [SYSTEM PURPOSE] system.\n\nYOUR ROLE:\nYou coordinate a team of specialized worker agents to accomplish complex tasks.\nYou do NOT do the specialized work yourself—you delegate, evaluate, and synthesize.\n\nWORKER AGENTS AVAILABLE:\n1. [Worker A Name]: [What they do]\n2. [Worker B Name]: [What they do]\n3. [Worker C Name]: [What they do]\n\nYOUR PROCESS:\n1. Analyze the incoming request\n2. Break it into subtasks for your workers\n3. Evaluate each worker&#39;s output\n4. Request revisions if quality insufficient\n5. Synthesize final deliverable\n\nWHEN YOU RECEIVE A NEW REQUEST:\n1. Confirm you understand the request\n2. Create a task breakdown:\n   - Task for [Worker A]: [specific instructions]\n   - Task for [Worker B]: [specific instructions]\n   - Task for [Worker C]: [specific instructions]\n3. Define success criteria for each task\n\nWHEN YOU RECEIVE WORKER OUTPUT:\n1. Evaluate against success criteria\n2. If PASS: Acknowledge and proceed to next step\n3. If FAIL: Provide specific feedback and request revision\n\nWHEN ALL WORK IS COMPLETE:\n1. Synthesize outputs into final deliverable\n2. Note any limitations or caveats\n3. Suggest follow-up actions if relevant\n\nOUTPUT FORMAT:\n[Define structured output format for manager communications]\n</code></pre>\n<p><strong>WORKER AGENT PROMPT TEMPLATE:</strong></p>\n<pre><code>You are the [WORKER NAME] Agent, a specialist in [SPECIALTY].\n\nYOUR ROLE:\nYou are part of a multi-agent system managed by a Manager Agent.\nYou receive specific tasks and return high-quality results.\n\nYOUR SPECIALTY:\n[Detailed description of what this agent does well]\n\nYOUR SCOPE:\nDO: [What this agent should do]\nDON&#39;T: [What this agent should NOT attempt]\n\nWHEN YOU RECEIVE A TASK:\n1. Confirm you understand the assignment\n2. Execute your specialized work\n3. Self-assess your output quality\n4. Return results in the specified format\n\nOUTPUT FORMAT:\n{\n  &quot;status&quot;: &quot;complete|partial|blocked&quot;,\n  &quot;result&quot;: &quot;[Your work output]&quot;,\n  &quot;confidence&quot;: &quot;high|medium|low&quot;,\n  &quot;notes&quot;: &quot;[Any caveats or observations]&quot;,\n  &quot;needs_from_manager&quot;: &quot;[Any clarification or resources needed]&quot;\n}\n\nQUALITY STANDARDS:\n- [Standard 1]\n- [Standard 2]\n- [Standard 3]\n\nIf you cannot complete the task to quality standards, explain why\nrather than producing subpar work.\n</code></pre>\n<h3>Test Individual Agents</h3>\n<p>Before orchestrating, test each agent in isolation:</p>\n<table>\n<thead>\n<tr>\n<th>Agent</th>\n<th>Test Input</th>\n<th>Expected Output</th>\n<th>Actual Output</th>\n<th>Pass?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Manager</td>\n<td>Sample request</td>\n<td>Task breakdown</td>\n<td>[Result]</td>\n<td>☐</td>\n</tr>\n<tr>\n<td>Worker A</td>\n<td>Sample task</td>\n<td>Specialized output</td>\n<td>[Result]</td>\n<td>☐</td>\n</tr>\n<tr>\n<td>Worker B</td>\n<td>Sample task</td>\n<td>Specialized output</td>\n<td>[Result]</td>\n<td>☐</td>\n</tr>\n<tr>\n<td>Worker C</td>\n<td>Sample task</td>\n<td>Specialized output</td>\n<td>[Result]</td>\n<td>☐</td>\n</tr>\n</tbody></table>\n<hr>\n<h2>Part 5: Error Handling &amp; Resilience (10 minutes)</h2>\n<h3>Failure Modes</h3>\n<p>Multi-agent systems can fail in new ways:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Cause</th>\n<th>Mitigation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Agent timeout</strong></td>\n<td>Slow or stuck agent</td>\n<td>Set timeouts, retry logic</td>\n</tr>\n<tr>\n<td><strong>Quality spiral</strong></td>\n<td>Agents keep rejecting each other&#39;s work</td>\n<td>Max revision limit, human escalation</td>\n</tr>\n<tr>\n<td><strong>Context loss</strong></td>\n<td>Key info not passed between agents</td>\n<td>Structured message formats</td>\n</tr>\n<tr>\n<td><strong>Infinite loops</strong></td>\n<td>Circular dependencies</td>\n<td>Loop detection, max iterations</td>\n</tr>\n<tr>\n<td><strong>Cascading failures</strong></td>\n<td>One agent failure breaks chain</td>\n<td>Fallback behaviors, partial output</td>\n</tr>\n</tbody></table>\n<h3>Define Your Error Handling</h3>\n<pre><code>ERROR HANDLING PROTOCOL\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nTIMEOUTS:\n- Worker agents: [X] seconds max per task\n- Full pipeline: [Y] seconds max end-to-end\n- On timeout: [Action]\n\nRETRY POLICY:\n- Max retries per agent: [N]\n- Retry triggers: [What causes retry]\n- Retry with changes: [What gets modified]\n\nREVISION LIMITS:\n- Max Manager → Worker revision loops: [N]\n- On limit reached: [Action]\n\nESCALATION:\n- Trigger: [What causes escalation]\n- Escalate to: [Human role or backup system]\n- Context provided: [What info accompanies escalation]\n\nPARTIAL OUTPUT POLICY:\n- When acceptable: [Conditions]\n- When not acceptable: [Conditions]\n- Format: [How to present partial results]\n\nLOGGING:\n- Log all inter-agent messages: [Yes/No]\n- Log final output: [Yes/No]\n- Log errors: [Yes/No]\n- Retention: [How long]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<hr>\n<h2>Part 6: End-to-End Test (10 minutes)</h2>\n<h3>Run Complete System</h3>\n<p>Execute your multi-agent system with a realistic input:</p>\n<pre><code>END-TO-END TEST DOCUMENTATION\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nTEST CASE 1: Standard Input\nInput: [Realistic test input]\n\nAgent Execution Log:\n┌─────────────────────────────────────────────────────┐\n│ [Timestamp] Manager: Received request               │\n│ [Timestamp] Manager: Created task breakdown         │\n│ [Timestamp] Manager → Worker A: Assigned task       │\n│ [Timestamp] Worker A: Task complete, returning      │\n│ [Timestamp] Manager: Evaluated, quality PASS        │\n│ [Timestamp] Manager → Worker B: Assigned task       │\n│ [Timestamp] Worker B: Task complete, returning      │\n│ [Timestamp] Manager: Evaluated, quality PASS        │\n│ [Timestamp] Manager: Synthesizing final output      │\n│ [Timestamp] System: Output delivered                │\n└─────────────────────────────────────────────────────┘\n\nOutput Quality Assessment:\n- Completeness: [1-5]\n- Accuracy: [1-5]\n- Format: [1-5]\n- Meets requirements: [Yes/No]\n\nTime to complete: [X seconds/minutes]\nTokens used: [Approximate]\nCost: [Estimated]\n\nTEST CASE 2: Edge Case\nInput: [Unusual or challenging input]\n[Same logging format]\n\nTEST CASE 3: Error Condition\nInput: [Input designed to trigger failure handling]\n[Same logging format]\nError handling worked correctly: [Yes/No]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<hr>\n<h2>Deliverable</h2>\n<p>Create a complete multi-agent system package:</p>\n<h3>1. System Architecture Document</h3>\n<ul>\n<li>Pattern used (Worker-Manager, Pipeline, etc.)</li>\n<li>Agent roster with responsibilities</li>\n<li>Communication flow diagram</li>\n<li>Quality gates defined</li>\n</ul>\n<h3>2. Agent Prompts (Complete)</h3>\n<ul>\n<li>Manager agent prompt</li>\n<li>All worker agent prompts</li>\n<li>Message format specifications</li>\n</ul>\n<h3>3. Communication Protocol</h3>\n<ul>\n<li>Message types and formats</li>\n<li>Handoff procedures</li>\n<li>Quality gate criteria</li>\n</ul>\n<h3>4. Error Handling Plan</h3>\n<ul>\n<li>Failure modes identified</li>\n<li>Retry and escalation policies</li>\n<li>Partial output handling</li>\n</ul>\n<h3>5. Test Results</h3>\n<ul>\n<li>Individual agent tests</li>\n<li>End-to-end test cases</li>\n<li>Performance metrics (time, tokens, cost)</li>\n</ul>\n<h3>6. Comparison Analysis</h3>\n<pre><code>SINGLE AGENT VS. MULTI-AGENT COMPARISON\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nSame task performed by:\nA) Single generalist agent\nB) Your multi-agent system\n\nMetric            | Single Agent | Multi-Agent | Winner\n──────────────────┼──────────────┼─────────────┼────────\nQuality score     │ [1-5]        │ [1-5]       │ [A/B]\nCompleteness      │ [1-5]        │ [1-5]       │ [A/B]\nTime to complete  │ [X sec]      │ [Y sec]     │ [A/B]\nToken usage       │ [X]          │ [Y]         │ [A/B]\nCost              │ [$X]         │ [$Y]        │ [A/B]\nError handling    │ [1-5]        │ [1-5]       │ [A/B]\n\nVERDICT:\n[When is multi-agent worth it for this task?]\n[What task complexity threshold justifies the overhead?]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<hr>\n<h2>Extension Challenge</h2>\n<h3>Advanced: Self-Improving Agent System</h3>\n<p>Design (and optionally build) a system where agents improve over time:</p>\n<ol>\n<li><p><strong>Feedback Collection Agent</strong></p>\n<ul>\n<li>Gathers quality ratings on outputs</li>\n<li>Identifies patterns in failures</li>\n<li>Tracks which agent contributions are strongest/weakest</li>\n</ul>\n</li>\n<li><p><strong>Prompt Optimization Agent</strong></p>\n<ul>\n<li>Analyzes feedback patterns</li>\n<li>Suggests prompt modifications</li>\n<li>A/B tests prompt variations</li>\n</ul>\n</li>\n<li><p><strong>Performance Dashboard</strong></p>\n<ul>\n<li>Tracks quality over time</li>\n<li>Monitors cost efficiency</li>\n<li>Alerts on degradation</li>\n</ul>\n</li>\n</ol>\n<p>Document your design:</p>\n<ul>\n<li>How would agents learn from feedback?</li>\n<li>What metrics would you track?</li>\n<li>How would you prevent drift or degradation?</li>\n<li>What human oversight would remain?</li>\n</ul>\n<hr>\n<h2>Key Takeaways</h2>\n<ol>\n<li><strong>Multi-agent systems solve complexity</strong> that single agents cannot handle</li>\n<li><strong>Clear roles and protocols</strong> prevent chaos and duplication</li>\n<li><strong>Quality gates</strong> catch errors before they propagate</li>\n<li><strong>Error handling is essential</strong> — agents fail in new ways</li>\n<li><strong>Measure the tradeoff</strong> — multi-agent overhead must be justified by quality gains</li>\n</ol>\n",
      "sections": [
        {
          "id": "lab-overview",
          "title": "Lab Overview",
          "type": "generic",
          "content": "Lab 5 taught you to build a single AI assistant. But real business problems are too complex for one agent.\n\nThis lab teaches you to orchestrate **multiple AI agents** that collaborate, check each other's work, and handle tasks too complex for any single agent.\n\nThis is the difference between:\n- **Automation**: Pre-defined steps executed in sequence\n- **Agentic AI**: Agents that reason, delegate, and adapt\n\n**What you'll create:**\n- A multi-agent system with Worker and Manager agents\n- Defined communication protocols between agents\n- Quality gates and verification loops\n- A fault-tolerant orchestration pattern\n\n---",
          "htmlContent": "<p>Lab 5 taught you to build a single AI assistant. But real business problems are too complex for one agent.</p>\n<p>This lab teaches you to orchestrate <strong>multiple AI agents</strong> that collaborate, check each other&#39;s work, and handle tasks too complex for any single agent.</p>\n<p>This is the difference between:</p>\n<ul>\n<li><strong>Automation</strong>: Pre-defined steps executed in sequence</li>\n<li><strong>Agentic AI</strong>: Agents that reason, delegate, and adapt</li>\n</ul>\n<p><strong>What you&#39;ll create:</strong></p>\n<ul>\n<li>A multi-agent system with Worker and Manager agents</li>\n<li>Defined communication protocols between agents</li>\n<li>Quality gates and verification loops</li>\n<li>A fault-tolerant orchestration pattern</li>\n</ul>\n<hr>\n"
        },
        {
          "id": "the-multi-agent-mindset",
          "title": "The Multi-Agent Mindset",
          "type": "generic",
          "content": "> **One agent is a tool. Multiple agents are a team.**\n\nSingle agents hit walls:\n- Context windows overflow\n- Specialized tasks suffer from generalist prompts\n- No error checking or verification\n- Single point of failure\n\nMulti-agent systems solve these through **division of labor** and **checks and balances**.\n\n### Real-World Multi-Agent Examples\n\n| System | Worker Agents | Manager Agent | Value |\n|--------|--------------|---------------|-------|\n| **Research Pipeline** | Web searcher, paper analyzer, fact checker | Research coordinator | Verified, comprehensive research |\n| **Content Creation** | Writer, editor, fact-checker, SEO analyzer | Content lead | Publication-ready content |\n| **Customer Support** | Intent classifier, knowledge retriever, response drafter, sentiment analyzer | Support orchestrator | Faster, better responses |\n| **Code Review** | Syntax checker, security scanner, style analyzer, documentation verifier | Review coordinator | Comprehensive code review |\n\n---",
          "htmlContent": "<blockquote>\n<p><strong>One agent is a tool. Multiple agents are a team.</strong></p>\n</blockquote>\n<p>Single agents hit walls:</p>\n<ul>\n<li>Context windows overflow</li>\n<li>Specialized tasks suffer from generalist prompts</li>\n<li>No error checking or verification</li>\n<li>Single point of failure</li>\n</ul>\n<p>Multi-agent systems solve these through <strong>division of labor</strong> and <strong>checks and balances</strong>.</p>\n<h3>Real-World Multi-Agent Examples</h3>\n<table>\n<thead>\n<tr>\n<th>System</th>\n<th>Worker Agents</th>\n<th>Manager Agent</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Research Pipeline</strong></td>\n<td>Web searcher, paper analyzer, fact checker</td>\n<td>Research coordinator</td>\n<td>Verified, comprehensive research</td>\n</tr>\n<tr>\n<td><strong>Content Creation</strong></td>\n<td>Writer, editor, fact-checker, SEO analyzer</td>\n<td>Content lead</td>\n<td>Publication-ready content</td>\n</tr>\n<tr>\n<td><strong>Customer Support</strong></td>\n<td>Intent classifier, knowledge retriever, response drafter, sentiment analyzer</td>\n<td>Support orchestrator</td>\n<td>Faster, better responses</td>\n</tr>\n<tr>\n<td><strong>Code Review</strong></td>\n<td>Syntax checker, security scanner, style analyzer, documentation verifier</td>\n<td>Review coordinator</td>\n<td>Comprehensive code review</td>\n</tr>\n</tbody></table>\n<hr>\n"
        },
        {
          "id": "part-1:-understand-agent-patterns-(15-minutes)",
          "title": "Part 1: Understand Agent Patterns (15 minutes)",
          "type": "generic",
          "content": "### Pattern 1: Worker-Manager Hierarchy\n\n```\n                    ┌─────────────┐\n                    │   MANAGER   │\n                    │   AGENT     │\n                    └──────┬──────┘\n                           │ Delegates tasks\n            ┌──────────────┼──────────────┐\n            ▼              ▼              ▼\n     ┌──────────┐   ┌──────────┐   ┌──────────┐\n     │ WORKER 1 │   │ WORKER 2 │   │ WORKER 3 │\n     │ Research │   │ Analysis │   │ Writing  │\n     └──────────┘   └──────────┘   └──────────┘\n            │              │              │\n            └──────────────┴──────────────┘\n                           │ Return results\n                    ┌──────▼──────┐\n                    │   MANAGER   │\n                    │  Synthesizes│\n                    └─────────────┘\n```\n\n**Manager Agent responsibilities:**\n- Breaks complex task into subtasks\n- Assigns work to appropriate workers\n- Tracks progress and handles failures\n- Synthesizes results into final output\n\n**Worker Agent responsibilities:**\n- Specialized at one task type\n- Executes assigned work\n- Reports success, failure, or need for clarification\n- Stays within defined scope\n\n### Pattern 2: Pipeline with Verification\n\n```\nINPUT → [Agent A: Draft] → [Agent B: Review] → [Agent A: Revise] → OUTPUT\n              ↑                                        │\n              └────────────── Feedback loop ───────────┘\n```\n\nEach agent passes work to the next, with potential loops for refinement.\n\n### Pattern 3: Consensus/Voting\n\n```\nQUERY → [Agent A] ─┐\n      → [Agent B] ─┼→ [Aggregator: Compare & Synthesize] → OUTPUT\n      → [Agent C] ─┘\n```\n\nMultiple agents answer the same question; an aggregator combines or selects the best response.\n\n### Pattern 4: Specialist Routing\n\n```\n                         ┌─ [Technical Agent] → Technical Response\nINPUT → [Router Agent] ──┼─ [Sales Agent] → Sales Response\n                         └─ [Support Agent] → Support Response\n```\n\nA router determines which specialist should handle each request.\n\n### Exercise: Identify the Pattern\n\nFor each scenario, identify the best multi-agent pattern:\n\n| Scenario | Best Pattern | Why? |\n|----------|--------------|------|\n| Legal contract review requiring multiple expertise areas | [Your answer] | [Your reasoning] |\n| Customer inquiry that could be billing, technical, or sales | [Your answer] | [Your reasoning] |\n| Research report requiring web search, analysis, and writing | [Your answer] | [Your reasoning] |\n| Medical symptom assessment needing high reliability | [Your answer] | [Your reasoning] |\n\n---",
          "htmlContent": "<h3>Pattern 1: Worker-Manager Hierarchy</h3>\n<pre><code>                    ┌─────────────┐\n                    │   MANAGER   │\n                    │   AGENT     │\n                    └──────┬──────┘\n                           │ Delegates tasks\n            ┌──────────────┼──────────────┐\n            ▼              ▼              ▼\n     ┌──────────┐   ┌──────────┐   ┌──────────┐\n     │ WORKER 1 │   │ WORKER 2 │   │ WORKER 3 │\n     │ Research │   │ Analysis │   │ Writing  │\n     └──────────┘   └──────────┘   └──────────┘\n            │              │              │\n            └──────────────┴──────────────┘\n                           │ Return results\n                    ┌──────▼──────┐\n                    │   MANAGER   │\n                    │  Synthesizes│\n                    └─────────────┘\n</code></pre>\n<p><strong>Manager Agent responsibilities:</strong></p>\n<ul>\n<li>Breaks complex task into subtasks</li>\n<li>Assigns work to appropriate workers</li>\n<li>Tracks progress and handles failures</li>\n<li>Synthesizes results into final output</li>\n</ul>\n<p><strong>Worker Agent responsibilities:</strong></p>\n<ul>\n<li>Specialized at one task type</li>\n<li>Executes assigned work</li>\n<li>Reports success, failure, or need for clarification</li>\n<li>Stays within defined scope</li>\n</ul>\n<h3>Pattern 2: Pipeline with Verification</h3>\n<pre><code>INPUT → [Agent A: Draft] → [Agent B: Review] → [Agent A: Revise] → OUTPUT\n              ↑                                        │\n              └────────────── Feedback loop ───────────┘\n</code></pre>\n<p>Each agent passes work to the next, with potential loops for refinement.</p>\n<h3>Pattern 3: Consensus/Voting</h3>\n<pre><code>QUERY → [Agent A] ─┐\n      → [Agent B] ─┼→ [Aggregator: Compare &amp; Synthesize] → OUTPUT\n      → [Agent C] ─┘\n</code></pre>\n<p>Multiple agents answer the same question; an aggregator combines or selects the best response.</p>\n<h3>Pattern 4: Specialist Routing</h3>\n<pre><code>                         ┌─ [Technical Agent] → Technical Response\nINPUT → [Router Agent] ──┼─ [Sales Agent] → Sales Response\n                         └─ [Support Agent] → Support Response\n</code></pre>\n<p>A router determines which specialist should handle each request.</p>\n<h3>Exercise: Identify the Pattern</h3>\n<p>For each scenario, identify the best multi-agent pattern:</p>\n<table>\n<thead>\n<tr>\n<th>Scenario</th>\n<th>Best Pattern</th>\n<th>Why?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Legal contract review requiring multiple expertise areas</td>\n<td>[Your answer]</td>\n<td>[Your reasoning]</td>\n</tr>\n<tr>\n<td>Customer inquiry that could be billing, technical, or sales</td>\n<td>[Your answer]</td>\n<td>[Your reasoning]</td>\n</tr>\n<tr>\n<td>Research report requiring web search, analysis, and writing</td>\n<td>[Your answer]</td>\n<td>[Your reasoning]</td>\n</tr>\n<tr>\n<td>Medical symptom assessment needing high reliability</td>\n<td>[Your answer]</td>\n<td>[Your reasoning]</td>\n</tr>\n</tbody></table>\n<hr>\n"
        },
        {
          "id": "part-2:-design-your-multi-agent-system-(20-minutes)",
          "title": "Part 2: Design Your Multi-Agent System (20 minutes)",
          "type": "generic",
          "content": "### Choose Your Scenario\n\nPick one complex task that genuinely requires multiple agents:\n\n**Scenario A: Research Brief Generator**\n```\nInput: Topic + Research Question\nAgents needed:\n  - Web Researcher: Finds and summarizes sources\n  - Fact Checker: Verifies claims against sources\n  - Synthesizer: Combines findings into brief\n  - Quality Reviewer: Checks completeness and accuracy\nOutput: Research brief with citations\n```\n\n**Scenario B: Content Review Pipeline**\n```\nInput: Draft blog post or article\nAgents needed:\n  - Editor: Grammar, clarity, structure\n  - Fact Checker: Verifies claims\n  - Brand Voice Checker: Alignment with tone guidelines\n  - SEO Analyzer: Keyword and metadata review\nOutput: Publication-ready content with change log\n```\n\n**Scenario C: Customer Query Resolver**\n```\nInput: Customer message\nAgents needed:\n  - Intent Classifier: What does the customer need?\n  - Knowledge Retriever: Find relevant information\n  - Response Drafter: Create initial response\n  - Empathy Reviewer: Check tone and helpfulness\nOutput: Customer-ready response\n```\n\n**Scenario D: Your Business Process**\n```\nInput: [Real input from your work]\nAgents needed:\n  - [Agent 1]: [Specialized task]\n  - [Agent 2]: [Specialized task]\n  - [Agent 3]: [Verification/synthesis]\nOutput: [Business deliverable]\n```\n\n### Document Your Agent Design\n\n```\nMULTI-AGENT SYSTEM DESIGN\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nSYSTEM NAME: [Name]\n\nPROBLEM STATEMENT:\n[What complex task does this system handle?]\n[Why can't a single agent do this effectively?]\n\nPATTERN: [Worker-Manager / Pipeline / Consensus / Routing]\n\nINPUT:\n[What triggers the system?]\n[What data is provided?]\n\nOUTPUT:\n[What deliverable is produced?]\n[What quality standards must it meet?]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nAGENT ROSTER:\n\nAgent 1: [NAME]\n─────────────────\nRole: [Manager / Worker]\nResponsibility: [What this agent does]\nInputs: [What it receives]\nOutputs: [What it produces]\nModel: [Which LLM - consider cost/capability tradeoffs]\nKey prompt elements:\n  - [Instruction 1]\n  - [Instruction 2]\n  - [Output format]\n\nAgent 2: [NAME]\n─────────────────\nRole: [Manager / Worker]\nResponsibility: [What this agent does]\nInputs: [What it receives]\nOutputs: [What it produces]\nModel: [Which LLM]\nKey prompt elements:\n  - [Instruction 1]\n  - [Instruction 2]\n  - [Output format]\n\nAgent 3: [NAME]\n─────────────────\nRole: [Manager / Worker]\nResponsibility: [What this agent does]\nInputs: [What it receives]\nOutputs: [What it produces]\nModel: [Which LLM]\nKey prompt elements:\n  - [Instruction 1]\n  - [Instruction 2]\n  - [Output format]\n\n[Add more agents as needed]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n---",
          "htmlContent": "<h3>Choose Your Scenario</h3>\n<p>Pick one complex task that genuinely requires multiple agents:</p>\n<p><strong>Scenario A: Research Brief Generator</strong></p>\n<pre><code>Input: Topic + Research Question\nAgents needed:\n  - Web Researcher: Finds and summarizes sources\n  - Fact Checker: Verifies claims against sources\n  - Synthesizer: Combines findings into brief\n  - Quality Reviewer: Checks completeness and accuracy\nOutput: Research brief with citations\n</code></pre>\n<p><strong>Scenario B: Content Review Pipeline</strong></p>\n<pre><code>Input: Draft blog post or article\nAgents needed:\n  - Editor: Grammar, clarity, structure\n  - Fact Checker: Verifies claims\n  - Brand Voice Checker: Alignment with tone guidelines\n  - SEO Analyzer: Keyword and metadata review\nOutput: Publication-ready content with change log\n</code></pre>\n<p><strong>Scenario C: Customer Query Resolver</strong></p>\n<pre><code>Input: Customer message\nAgents needed:\n  - Intent Classifier: What does the customer need?\n  - Knowledge Retriever: Find relevant information\n  - Response Drafter: Create initial response\n  - Empathy Reviewer: Check tone and helpfulness\nOutput: Customer-ready response\n</code></pre>\n<p><strong>Scenario D: Your Business Process</strong></p>\n<pre><code>Input: [Real input from your work]\nAgents needed:\n  - [Agent 1]: [Specialized task]\n  - [Agent 2]: [Specialized task]\n  - [Agent 3]: [Verification/synthesis]\nOutput: [Business deliverable]\n</code></pre>\n<h3>Document Your Agent Design</h3>\n<pre><code>MULTI-AGENT SYSTEM DESIGN\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nSYSTEM NAME: [Name]\n\nPROBLEM STATEMENT:\n[What complex task does this system handle?]\n[Why can&#39;t a single agent do this effectively?]\n\nPATTERN: [Worker-Manager / Pipeline / Consensus / Routing]\n\nINPUT:\n[What triggers the system?]\n[What data is provided?]\n\nOUTPUT:\n[What deliverable is produced?]\n[What quality standards must it meet?]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nAGENT ROSTER:\n\nAgent 1: [NAME]\n─────────────────\nRole: [Manager / Worker]\nResponsibility: [What this agent does]\nInputs: [What it receives]\nOutputs: [What it produces]\nModel: [Which LLM - consider cost/capability tradeoffs]\nKey prompt elements:\n  - [Instruction 1]\n  - [Instruction 2]\n  - [Output format]\n\nAgent 2: [NAME]\n─────────────────\nRole: [Manager / Worker]\nResponsibility: [What this agent does]\nInputs: [What it receives]\nOutputs: [What it produces]\nModel: [Which LLM]\nKey prompt elements:\n  - [Instruction 1]\n  - [Instruction 2]\n  - [Output format]\n\nAgent 3: [NAME]\n─────────────────\nRole: [Manager / Worker]\nResponsibility: [What this agent does]\nInputs: [What it receives]\nOutputs: [What it produces]\nModel: [Which LLM]\nKey prompt elements:\n  - [Instruction 1]\n  - [Instruction 2]\n  - [Output format]\n\n[Add more agents as needed]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<hr>\n"
        },
        {
          "id": "part-3:-design-communication-protocols-(15-minutes)",
          "title": "Part 3: Design Communication Protocols (15 minutes)",
          "type": "generic",
          "content": "### Agent Message Format\n\nAgents need structured communication. Define a standard message format:\n\n```json\n{\n  \"from_agent\": \"researcher\",\n  \"to_agent\": \"manager\",\n  \"message_type\": \"work_complete|needs_input|error|status\",\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"task_id\": \"research-001\",\n  \"status\": \"success|partial|failure\",\n  \"payload\": {\n    \"result\": \"[The work output]\",\n    \"confidence\": \"high|medium|low\",\n    \"sources\": [\"[citations if applicable]\"],\n    \"notes\": \"[Any caveats or observations]\"\n  },\n  \"next_action\": \"proceed|retry|escalate|clarify\"\n}\n```\n\n### Define Your Communication Protocol\n\n```\nINTER-AGENT COMMUNICATION PROTOCOL\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nWORKFLOW SEQUENCE:\n\nStep 1: [Manager] receives initial request\n        ↓ Parses and creates task breakdown\n        ↓ Sends to [Worker A]\n\nStep 2: [Worker A] receives task\n        ↓ Executes specialized work\n        ↓ Returns result to [Manager]\n\nStep 3: [Manager] evaluates result\n        ↓ IF quality OK → Send to [Worker B]\n        ↓ IF quality NOT OK → Return to [Worker A] with feedback\n\nStep 4: [Worker B] receives work + prior results\n        ↓ Executes next phase\n        ↓ Returns to [Manager]\n\nStep 5: [Manager] synthesizes all results\n        ↓ Produces final output\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nMESSAGE TYPES:\n\nTASK_ASSIGNMENT (Manager → Worker)\n{\n  \"task\": \"[What to do]\",\n  \"context\": \"[Relevant background]\",\n  \"constraints\": \"[Time, format, scope limits]\",\n  \"success_criteria\": \"[How to know it's done well]\"\n}\n\nWORK_RESULT (Worker → Manager)\n{\n  \"result\": \"[The output]\",\n  \"quality_self_assessment\": \"[Worker's view of quality]\",\n  \"blockers\": \"[Any issues encountered]\",\n  \"recommendations\": \"[Suggestions for next steps]\"\n}\n\nFEEDBACK (Manager → Worker)\n{\n  \"assessment\": \"[What was good/what needs work]\",\n  \"specific_requests\": \"[Exact changes needed]\",\n  \"priority\": \"[How urgent]\"\n}\n\nFINAL_OUTPUT (Manager → System)\n{\n  \"deliverable\": \"[The final product]\",\n  \"process_summary\": \"[What agents did]\",\n  \"confidence\": \"[Overall quality assessment]\",\n  \"recommendations\": \"[Next steps if any]\"\n}\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Quality Gates\n\nDefine checkpoints where agents verify work:\n\n```\nQUALITY GATES\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nGate 1: After [Agent A] completes\nCriteria:\n  - [ ] Output format correct\n  - [ ] Required fields present\n  - [ ] No obvious errors\n  - [ ] Confidence above threshold\nPass → Continue to next agent\nFail → Retry (max 2) or escalate\n\nGate 2: After [Agent B] completes\nCriteria:\n  - [ ] Builds correctly on prior work\n  - [ ] Quality maintained or improved\n  - [ ] No contradictions introduced\nPass → Continue\nFail → Return to previous agent with feedback\n\nGate 3: Final synthesis\nCriteria:\n  - [ ] All components integrated\n  - [ ] Meets original request requirements\n  - [ ] Quality standards achieved\n  - [ ] Ready for end user\nPass → Deliver output\nFail → Identify weakest component, request revision\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n---",
          "htmlContent": "<h3>Agent Message Format</h3>\n<p>Agents need structured communication. Define a standard message format:</p>\n<pre><code class=\"language-json\">{\n  &quot;from_agent&quot;: &quot;researcher&quot;,\n  &quot;to_agent&quot;: &quot;manager&quot;,\n  &quot;message_type&quot;: &quot;work_complete|needs_input|error|status&quot;,\n  &quot;timestamp&quot;: &quot;2024-01-15T10:30:00Z&quot;,\n  &quot;task_id&quot;: &quot;research-001&quot;,\n  &quot;status&quot;: &quot;success|partial|failure&quot;,\n  &quot;payload&quot;: {\n    &quot;result&quot;: &quot;[The work output]&quot;,\n    &quot;confidence&quot;: &quot;high|medium|low&quot;,\n    &quot;sources&quot;: [&quot;[citations if applicable]&quot;],\n    &quot;notes&quot;: &quot;[Any caveats or observations]&quot;\n  },\n  &quot;next_action&quot;: &quot;proceed|retry|escalate|clarify&quot;\n}\n</code></pre>\n<h3>Define Your Communication Protocol</h3>\n<pre><code>INTER-AGENT COMMUNICATION PROTOCOL\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nWORKFLOW SEQUENCE:\n\nStep 1: [Manager] receives initial request\n        ↓ Parses and creates task breakdown\n        ↓ Sends to [Worker A]\n\nStep 2: [Worker A] receives task\n        ↓ Executes specialized work\n        ↓ Returns result to [Manager]\n\nStep 3: [Manager] evaluates result\n        ↓ IF quality OK → Send to [Worker B]\n        ↓ IF quality NOT OK → Return to [Worker A] with feedback\n\nStep 4: [Worker B] receives work + prior results\n        ↓ Executes next phase\n        ↓ Returns to [Manager]\n\nStep 5: [Manager] synthesizes all results\n        ↓ Produces final output\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nMESSAGE TYPES:\n\nTASK_ASSIGNMENT (Manager → Worker)\n{\n  &quot;task&quot;: &quot;[What to do]&quot;,\n  &quot;context&quot;: &quot;[Relevant background]&quot;,\n  &quot;constraints&quot;: &quot;[Time, format, scope limits]&quot;,\n  &quot;success_criteria&quot;: &quot;[How to know it&#39;s done well]&quot;\n}\n\nWORK_RESULT (Worker → Manager)\n{\n  &quot;result&quot;: &quot;[The output]&quot;,\n  &quot;quality_self_assessment&quot;: &quot;[Worker&#39;s view of quality]&quot;,\n  &quot;blockers&quot;: &quot;[Any issues encountered]&quot;,\n  &quot;recommendations&quot;: &quot;[Suggestions for next steps]&quot;\n}\n\nFEEDBACK (Manager → Worker)\n{\n  &quot;assessment&quot;: &quot;[What was good/what needs work]&quot;,\n  &quot;specific_requests&quot;: &quot;[Exact changes needed]&quot;,\n  &quot;priority&quot;: &quot;[How urgent]&quot;\n}\n\nFINAL_OUTPUT (Manager → System)\n{\n  &quot;deliverable&quot;: &quot;[The final product]&quot;,\n  &quot;process_summary&quot;: &quot;[What agents did]&quot;,\n  &quot;confidence&quot;: &quot;[Overall quality assessment]&quot;,\n  &quot;recommendations&quot;: &quot;[Next steps if any]&quot;\n}\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<h3>Quality Gates</h3>\n<p>Define checkpoints where agents verify work:</p>\n<pre><code>QUALITY GATES\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nGate 1: After [Agent A] completes\nCriteria:\n  - [ ] Output format correct\n  - [ ] Required fields present\n  - [ ] No obvious errors\n  - [ ] Confidence above threshold\nPass → Continue to next agent\nFail → Retry (max 2) or escalate\n\nGate 2: After [Agent B] completes\nCriteria:\n  - [ ] Builds correctly on prior work\n  - [ ] Quality maintained or improved\n  - [ ] No contradictions introduced\nPass → Continue\nFail → Return to previous agent with feedback\n\nGate 3: Final synthesis\nCriteria:\n  - [ ] All components integrated\n  - [ ] Meets original request requirements\n  - [ ] Quality standards achieved\n  - [ ] Ready for end user\nPass → Deliver output\nFail → Identify weakest component, request revision\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<hr>\n"
        },
        {
          "id": "part-4:-build-the-system-(25-minutes)",
          "title": "Part 4: Build the System (25 minutes)",
          "type": "generic",
          "content": "### Implementation Options\n\nChoose your implementation approach:\n\n**Option A: Prompt Chain (Simplest)**\n- Execute agents sequentially with copy/paste between\n- Good for prototyping and testing\n- Manual orchestration\n\n**Option B: Automation Platform**\n- Use Zapier/Make/n8n to orchestrate\n- Each agent is an AI action in the workflow\n- Automatic data passing between agents\n\n**Option C: Claude Projects / GPT Assistants**\n- Create separate assistants for each agent role\n- Programmatically call each in sequence\n- More sophisticated but requires API access\n\n**Option D: Code-Based (Advanced)**\n- Build with LangChain, CrewAI, or AutoGen\n- Full control over orchestration\n- Requires programming knowledge\n\n### Write Agent Prompts\n\nFor each agent in your system, write a complete prompt:\n\n**MANAGER AGENT PROMPT TEMPLATE:**\n```\nYou are the Manager Agent for a [SYSTEM PURPOSE] system.\n\nYOUR ROLE:\nYou coordinate a team of specialized worker agents to accomplish complex tasks.\nYou do NOT do the specialized work yourself—you delegate, evaluate, and synthesize.\n\nWORKER AGENTS AVAILABLE:\n1. [Worker A Name]: [What they do]\n2. [Worker B Name]: [What they do]\n3. [Worker C Name]: [What they do]\n\nYOUR PROCESS:\n1. Analyze the incoming request\n2. Break it into subtasks for your workers\n3. Evaluate each worker's output\n4. Request revisions if quality insufficient\n5. Synthesize final deliverable\n\nWHEN YOU RECEIVE A NEW REQUEST:\n1. Confirm you understand the request\n2. Create a task breakdown:\n   - Task for [Worker A]: [specific instructions]\n   - Task for [Worker B]: [specific instructions]\n   - Task for [Worker C]: [specific instructions]\n3. Define success criteria for each task\n\nWHEN YOU RECEIVE WORKER OUTPUT:\n1. Evaluate against success criteria\n2. If PASS: Acknowledge and proceed to next step\n3. If FAIL: Provide specific feedback and request revision\n\nWHEN ALL WORK IS COMPLETE:\n1. Synthesize outputs into final deliverable\n2. Note any limitations or caveats\n3. Suggest follow-up actions if relevant\n\nOUTPUT FORMAT:\n[Define structured output format for manager communications]\n```\n\n**WORKER AGENT PROMPT TEMPLATE:**\n```\nYou are the [WORKER NAME] Agent, a specialist in [SPECIALTY].\n\nYOUR ROLE:\nYou are part of a multi-agent system managed by a Manager Agent.\nYou receive specific tasks and return high-quality results.\n\nYOUR SPECIALTY:\n[Detailed description of what this agent does well]\n\nYOUR SCOPE:\nDO: [What this agent should do]\nDON'T: [What this agent should NOT attempt]\n\nWHEN YOU RECEIVE A TASK:\n1. Confirm you understand the assignment\n2. Execute your specialized work\n3. Self-assess your output quality\n4. Return results in the specified format\n\nOUTPUT FORMAT:\n{\n  \"status\": \"complete|partial|blocked\",\n  \"result\": \"[Your work output]\",\n  \"confidence\": \"high|medium|low\",\n  \"notes\": \"[Any caveats or observations]\",\n  \"needs_from_manager\": \"[Any clarification or resources needed]\"\n}\n\nQUALITY STANDARDS:\n- [Standard 1]\n- [Standard 2]\n- [Standard 3]\n\nIf you cannot complete the task to quality standards, explain why\nrather than producing subpar work.\n```\n\n### Test Individual Agents\n\nBefore orchestrating, test each agent in isolation:\n\n| Agent | Test Input | Expected Output | Actual Output | Pass? |\n|-------|------------|-----------------|---------------|-------|\n| Manager | Sample request | Task breakdown | [Result] | ☐ |\n| Worker A | Sample task | Specialized output | [Result] | ☐ |\n| Worker B | Sample task | Specialized output | [Result] | ☐ |\n| Worker C | Sample task | Specialized output | [Result] | ☐ |\n\n---",
          "htmlContent": "<h3>Implementation Options</h3>\n<p>Choose your implementation approach:</p>\n<p><strong>Option A: Prompt Chain (Simplest)</strong></p>\n<ul>\n<li>Execute agents sequentially with copy/paste between</li>\n<li>Good for prototyping and testing</li>\n<li>Manual orchestration</li>\n</ul>\n<p><strong>Option B: Automation Platform</strong></p>\n<ul>\n<li>Use Zapier/Make/n8n to orchestrate</li>\n<li>Each agent is an AI action in the workflow</li>\n<li>Automatic data passing between agents</li>\n</ul>\n<p><strong>Option C: Claude Projects / GPT Assistants</strong></p>\n<ul>\n<li>Create separate assistants for each agent role</li>\n<li>Programmatically call each in sequence</li>\n<li>More sophisticated but requires API access</li>\n</ul>\n<p><strong>Option D: Code-Based (Advanced)</strong></p>\n<ul>\n<li>Build with LangChain, CrewAI, or AutoGen</li>\n<li>Full control over orchestration</li>\n<li>Requires programming knowledge</li>\n</ul>\n<h3>Write Agent Prompts</h3>\n<p>For each agent in your system, write a complete prompt:</p>\n<p><strong>MANAGER AGENT PROMPT TEMPLATE:</strong></p>\n<pre><code>You are the Manager Agent for a [SYSTEM PURPOSE] system.\n\nYOUR ROLE:\nYou coordinate a team of specialized worker agents to accomplish complex tasks.\nYou do NOT do the specialized work yourself—you delegate, evaluate, and synthesize.\n\nWORKER AGENTS AVAILABLE:\n1. [Worker A Name]: [What they do]\n2. [Worker B Name]: [What they do]\n3. [Worker C Name]: [What they do]\n\nYOUR PROCESS:\n1. Analyze the incoming request\n2. Break it into subtasks for your workers\n3. Evaluate each worker&#39;s output\n4. Request revisions if quality insufficient\n5. Synthesize final deliverable\n\nWHEN YOU RECEIVE A NEW REQUEST:\n1. Confirm you understand the request\n2. Create a task breakdown:\n   - Task for [Worker A]: [specific instructions]\n   - Task for [Worker B]: [specific instructions]\n   - Task for [Worker C]: [specific instructions]\n3. Define success criteria for each task\n\nWHEN YOU RECEIVE WORKER OUTPUT:\n1. Evaluate against success criteria\n2. If PASS: Acknowledge and proceed to next step\n3. If FAIL: Provide specific feedback and request revision\n\nWHEN ALL WORK IS COMPLETE:\n1. Synthesize outputs into final deliverable\n2. Note any limitations or caveats\n3. Suggest follow-up actions if relevant\n\nOUTPUT FORMAT:\n[Define structured output format for manager communications]\n</code></pre>\n<p><strong>WORKER AGENT PROMPT TEMPLATE:</strong></p>\n<pre><code>You are the [WORKER NAME] Agent, a specialist in [SPECIALTY].\n\nYOUR ROLE:\nYou are part of a multi-agent system managed by a Manager Agent.\nYou receive specific tasks and return high-quality results.\n\nYOUR SPECIALTY:\n[Detailed description of what this agent does well]\n\nYOUR SCOPE:\nDO: [What this agent should do]\nDON&#39;T: [What this agent should NOT attempt]\n\nWHEN YOU RECEIVE A TASK:\n1. Confirm you understand the assignment\n2. Execute your specialized work\n3. Self-assess your output quality\n4. Return results in the specified format\n\nOUTPUT FORMAT:\n{\n  &quot;status&quot;: &quot;complete|partial|blocked&quot;,\n  &quot;result&quot;: &quot;[Your work output]&quot;,\n  &quot;confidence&quot;: &quot;high|medium|low&quot;,\n  &quot;notes&quot;: &quot;[Any caveats or observations]&quot;,\n  &quot;needs_from_manager&quot;: &quot;[Any clarification or resources needed]&quot;\n}\n\nQUALITY STANDARDS:\n- [Standard 1]\n- [Standard 2]\n- [Standard 3]\n\nIf you cannot complete the task to quality standards, explain why\nrather than producing subpar work.\n</code></pre>\n<h3>Test Individual Agents</h3>\n<p>Before orchestrating, test each agent in isolation:</p>\n<table>\n<thead>\n<tr>\n<th>Agent</th>\n<th>Test Input</th>\n<th>Expected Output</th>\n<th>Actual Output</th>\n<th>Pass?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Manager</td>\n<td>Sample request</td>\n<td>Task breakdown</td>\n<td>[Result]</td>\n<td>☐</td>\n</tr>\n<tr>\n<td>Worker A</td>\n<td>Sample task</td>\n<td>Specialized output</td>\n<td>[Result]</td>\n<td>☐</td>\n</tr>\n<tr>\n<td>Worker B</td>\n<td>Sample task</td>\n<td>Specialized output</td>\n<td>[Result]</td>\n<td>☐</td>\n</tr>\n<tr>\n<td>Worker C</td>\n<td>Sample task</td>\n<td>Specialized output</td>\n<td>[Result]</td>\n<td>☐</td>\n</tr>\n</tbody></table>\n<hr>\n"
        },
        {
          "id": "part-5:-error-handling-&-resilience-(10-minutes)",
          "title": "Part 5: Error Handling & Resilience (10 minutes)",
          "type": "generic",
          "content": "### Failure Modes\n\nMulti-agent systems can fail in new ways:\n\n| Failure Mode | Cause | Mitigation |\n|--------------|-------|------------|\n| **Agent timeout** | Slow or stuck agent | Set timeouts, retry logic |\n| **Quality spiral** | Agents keep rejecting each other's work | Max revision limit, human escalation |\n| **Context loss** | Key info not passed between agents | Structured message formats |\n| **Infinite loops** | Circular dependencies | Loop detection, max iterations |\n| **Cascading failures** | One agent failure breaks chain | Fallback behaviors, partial output |\n\n### Define Your Error Handling\n\n```\nERROR HANDLING PROTOCOL\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nTIMEOUTS:\n- Worker agents: [X] seconds max per task\n- Full pipeline: [Y] seconds max end-to-end\n- On timeout: [Action]\n\nRETRY POLICY:\n- Max retries per agent: [N]\n- Retry triggers: [What causes retry]\n- Retry with changes: [What gets modified]\n\nREVISION LIMITS:\n- Max Manager → Worker revision loops: [N]\n- On limit reached: [Action]\n\nESCALATION:\n- Trigger: [What causes escalation]\n- Escalate to: [Human role or backup system]\n- Context provided: [What info accompanies escalation]\n\nPARTIAL OUTPUT POLICY:\n- When acceptable: [Conditions]\n- When not acceptable: [Conditions]\n- Format: [How to present partial results]\n\nLOGGING:\n- Log all inter-agent messages: [Yes/No]\n- Log final output: [Yes/No]\n- Log errors: [Yes/No]\n- Retention: [How long]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n---",
          "htmlContent": "<h3>Failure Modes</h3>\n<p>Multi-agent systems can fail in new ways:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Cause</th>\n<th>Mitigation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Agent timeout</strong></td>\n<td>Slow or stuck agent</td>\n<td>Set timeouts, retry logic</td>\n</tr>\n<tr>\n<td><strong>Quality spiral</strong></td>\n<td>Agents keep rejecting each other&#39;s work</td>\n<td>Max revision limit, human escalation</td>\n</tr>\n<tr>\n<td><strong>Context loss</strong></td>\n<td>Key info not passed between agents</td>\n<td>Structured message formats</td>\n</tr>\n<tr>\n<td><strong>Infinite loops</strong></td>\n<td>Circular dependencies</td>\n<td>Loop detection, max iterations</td>\n</tr>\n<tr>\n<td><strong>Cascading failures</strong></td>\n<td>One agent failure breaks chain</td>\n<td>Fallback behaviors, partial output</td>\n</tr>\n</tbody></table>\n<h3>Define Your Error Handling</h3>\n<pre><code>ERROR HANDLING PROTOCOL\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nTIMEOUTS:\n- Worker agents: [X] seconds max per task\n- Full pipeline: [Y] seconds max end-to-end\n- On timeout: [Action]\n\nRETRY POLICY:\n- Max retries per agent: [N]\n- Retry triggers: [What causes retry]\n- Retry with changes: [What gets modified]\n\nREVISION LIMITS:\n- Max Manager → Worker revision loops: [N]\n- On limit reached: [Action]\n\nESCALATION:\n- Trigger: [What causes escalation]\n- Escalate to: [Human role or backup system]\n- Context provided: [What info accompanies escalation]\n\nPARTIAL OUTPUT POLICY:\n- When acceptable: [Conditions]\n- When not acceptable: [Conditions]\n- Format: [How to present partial results]\n\nLOGGING:\n- Log all inter-agent messages: [Yes/No]\n- Log final output: [Yes/No]\n- Log errors: [Yes/No]\n- Retention: [How long]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<hr>\n"
        },
        {
          "id": "part-6:-end-to-end-test-(10-minutes)",
          "title": "Part 6: End-to-End Test (10 minutes)",
          "type": "generic",
          "content": "### Run Complete System\n\nExecute your multi-agent system with a realistic input:\n\n```\nEND-TO-END TEST DOCUMENTATION\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nTEST CASE 1: Standard Input\nInput: [Realistic test input]\n\nAgent Execution Log:\n┌─────────────────────────────────────────────────────┐\n│ [Timestamp] Manager: Received request               │\n│ [Timestamp] Manager: Created task breakdown         │\n│ [Timestamp] Manager → Worker A: Assigned task       │\n│ [Timestamp] Worker A: Task complete, returning      │\n│ [Timestamp] Manager: Evaluated, quality PASS        │\n│ [Timestamp] Manager → Worker B: Assigned task       │\n│ [Timestamp] Worker B: Task complete, returning      │\n│ [Timestamp] Manager: Evaluated, quality PASS        │\n│ [Timestamp] Manager: Synthesizing final output      │\n│ [Timestamp] System: Output delivered                │\n└─────────────────────────────────────────────────────┘\n\nOutput Quality Assessment:\n- Completeness: [1-5]\n- Accuracy: [1-5]\n- Format: [1-5]\n- Meets requirements: [Yes/No]\n\nTime to complete: [X seconds/minutes]\nTokens used: [Approximate]\nCost: [Estimated]\n\nTEST CASE 2: Edge Case\nInput: [Unusual or challenging input]\n[Same logging format]\n\nTEST CASE 3: Error Condition\nInput: [Input designed to trigger failure handling]\n[Same logging format]\nError handling worked correctly: [Yes/No]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n---",
          "htmlContent": "<h3>Run Complete System</h3>\n<p>Execute your multi-agent system with a realistic input:</p>\n<pre><code>END-TO-END TEST DOCUMENTATION\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nTEST CASE 1: Standard Input\nInput: [Realistic test input]\n\nAgent Execution Log:\n┌─────────────────────────────────────────────────────┐\n│ [Timestamp] Manager: Received request               │\n│ [Timestamp] Manager: Created task breakdown         │\n│ [Timestamp] Manager → Worker A: Assigned task       │\n│ [Timestamp] Worker A: Task complete, returning      │\n│ [Timestamp] Manager: Evaluated, quality PASS        │\n│ [Timestamp] Manager → Worker B: Assigned task       │\n│ [Timestamp] Worker B: Task complete, returning      │\n│ [Timestamp] Manager: Evaluated, quality PASS        │\n│ [Timestamp] Manager: Synthesizing final output      │\n│ [Timestamp] System: Output delivered                │\n└─────────────────────────────────────────────────────┘\n\nOutput Quality Assessment:\n- Completeness: [1-5]\n- Accuracy: [1-5]\n- Format: [1-5]\n- Meets requirements: [Yes/No]\n\nTime to complete: [X seconds/minutes]\nTokens used: [Approximate]\nCost: [Estimated]\n\nTEST CASE 2: Edge Case\nInput: [Unusual or challenging input]\n[Same logging format]\n\nTEST CASE 3: Error Condition\nInput: [Input designed to trigger failure handling]\n[Same logging format]\nError handling worked correctly: [Yes/No]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<hr>\n"
        },
        {
          "id": "deliverable",
          "title": "Deliverable",
          "type": "generic",
          "content": "Create a complete multi-agent system package:\n\n### 1. System Architecture Document\n- Pattern used (Worker-Manager, Pipeline, etc.)\n- Agent roster with responsibilities\n- Communication flow diagram\n- Quality gates defined\n\n### 2. Agent Prompts (Complete)\n- Manager agent prompt\n- All worker agent prompts\n- Message format specifications\n\n### 3. Communication Protocol\n- Message types and formats\n- Handoff procedures\n- Quality gate criteria\n\n### 4. Error Handling Plan\n- Failure modes identified\n- Retry and escalation policies\n- Partial output handling\n\n### 5. Test Results\n- Individual agent tests\n- End-to-end test cases\n- Performance metrics (time, tokens, cost)\n\n### 6. Comparison Analysis\n```\nSINGLE AGENT VS. MULTI-AGENT COMPARISON\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nSame task performed by:\nA) Single generalist agent\nB) Your multi-agent system\n\nMetric            | Single Agent | Multi-Agent | Winner\n──────────────────┼──────────────┼─────────────┼────────\nQuality score     │ [1-5]        │ [1-5]       │ [A/B]\nCompleteness      │ [1-5]        │ [1-5]       │ [A/B]\nTime to complete  │ [X sec]      │ [Y sec]     │ [A/B]\nToken usage       │ [X]          │ [Y]         │ [A/B]\nCost              │ [$X]         │ [$Y]        │ [A/B]\nError handling    │ [1-5]        │ [1-5]       │ [A/B]\n\nVERDICT:\n[When is multi-agent worth it for this task?]\n[What task complexity threshold justifies the overhead?]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n---",
          "htmlContent": "<p>Create a complete multi-agent system package:</p>\n<h3>1. System Architecture Document</h3>\n<ul>\n<li>Pattern used (Worker-Manager, Pipeline, etc.)</li>\n<li>Agent roster with responsibilities</li>\n<li>Communication flow diagram</li>\n<li>Quality gates defined</li>\n</ul>\n<h3>2. Agent Prompts (Complete)</h3>\n<ul>\n<li>Manager agent prompt</li>\n<li>All worker agent prompts</li>\n<li>Message format specifications</li>\n</ul>\n<h3>3. Communication Protocol</h3>\n<ul>\n<li>Message types and formats</li>\n<li>Handoff procedures</li>\n<li>Quality gate criteria</li>\n</ul>\n<h3>4. Error Handling Plan</h3>\n<ul>\n<li>Failure modes identified</li>\n<li>Retry and escalation policies</li>\n<li>Partial output handling</li>\n</ul>\n<h3>5. Test Results</h3>\n<ul>\n<li>Individual agent tests</li>\n<li>End-to-end test cases</li>\n<li>Performance metrics (time, tokens, cost)</li>\n</ul>\n<h3>6. Comparison Analysis</h3>\n<pre><code>SINGLE AGENT VS. MULTI-AGENT COMPARISON\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nSame task performed by:\nA) Single generalist agent\nB) Your multi-agent system\n\nMetric            | Single Agent | Multi-Agent | Winner\n──────────────────┼──────────────┼─────────────┼────────\nQuality score     │ [1-5]        │ [1-5]       │ [A/B]\nCompleteness      │ [1-5]        │ [1-5]       │ [A/B]\nTime to complete  │ [X sec]      │ [Y sec]     │ [A/B]\nToken usage       │ [X]          │ [Y]         │ [A/B]\nCost              │ [$X]         │ [$Y]        │ [A/B]\nError handling    │ [1-5]        │ [1-5]       │ [A/B]\n\nVERDICT:\n[When is multi-agent worth it for this task?]\n[What task complexity threshold justifies the overhead?]\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<hr>\n"
        },
        {
          "id": "extension-challenge",
          "title": "Extension Challenge",
          "type": "generic",
          "content": "### Advanced: Self-Improving Agent System\n\nDesign (and optionally build) a system where agents improve over time:\n\n1. **Feedback Collection Agent**\n   - Gathers quality ratings on outputs\n   - Identifies patterns in failures\n   - Tracks which agent contributions are strongest/weakest\n\n2. **Prompt Optimization Agent**\n   - Analyzes feedback patterns\n   - Suggests prompt modifications\n   - A/B tests prompt variations\n\n3. **Performance Dashboard**\n   - Tracks quality over time\n   - Monitors cost efficiency\n   - Alerts on degradation\n\nDocument your design:\n- How would agents learn from feedback?\n- What metrics would you track?\n- How would you prevent drift or degradation?\n- What human oversight would remain?\n\n---",
          "htmlContent": "<h3>Advanced: Self-Improving Agent System</h3>\n<p>Design (and optionally build) a system where agents improve over time:</p>\n<ol>\n<li><p><strong>Feedback Collection Agent</strong></p>\n<ul>\n<li>Gathers quality ratings on outputs</li>\n<li>Identifies patterns in failures</li>\n<li>Tracks which agent contributions are strongest/weakest</li>\n</ul>\n</li>\n<li><p><strong>Prompt Optimization Agent</strong></p>\n<ul>\n<li>Analyzes feedback patterns</li>\n<li>Suggests prompt modifications</li>\n<li>A/B tests prompt variations</li>\n</ul>\n</li>\n<li><p><strong>Performance Dashboard</strong></p>\n<ul>\n<li>Tracks quality over time</li>\n<li>Monitors cost efficiency</li>\n<li>Alerts on degradation</li>\n</ul>\n</li>\n</ol>\n<p>Document your design:</p>\n<ul>\n<li>How would agents learn from feedback?</li>\n<li>What metrics would you track?</li>\n<li>How would you prevent drift or degradation?</li>\n<li>What human oversight would remain?</li>\n</ul>\n<hr>\n"
        },
        {
          "id": "key-takeaways",
          "title": "Key Takeaways",
          "type": "generic",
          "content": "1. **Multi-agent systems solve complexity** that single agents cannot handle\n2. **Clear roles and protocols** prevent chaos and duplication\n3. **Quality gates** catch errors before they propagate\n4. **Error handling is essential** — agents fail in new ways\n5. **Measure the tradeoff** — multi-agent overhead must be justified by quality gains",
          "htmlContent": "<ol>\n<li><strong>Multi-agent systems solve complexity</strong> that single agents cannot handle</li>\n<li><strong>Clear roles and protocols</strong> prevent chaos and duplication</li>\n<li><strong>Quality gates</strong> catch errors before they propagate</li>\n<li><strong>Error handling is essential</strong> — agents fail in new ways</li>\n<li><strong>Measure the tradeoff</strong> — multi-agent overhead must be justified by quality gains</li>\n</ol>\n"
        }
      ]
    }
  ]
}