{
  "metadata": {
    "id": "phase-2",
    "title": "Workflow Engineering",
    "description": "Design AI-enhanced processes by identifying automation opportunities and orchestrating multi-step workflows",
    "order": 2,
    "color": "phase-2",
    "icon": "workflow",
    "estimatedHours": 5,
    "modules": [
      "2.1-process-analysis",
      "2.2-task-decomposition",
      "2.3-quality-and-iteration",
      "2.4-human-ai-handoffs"
    ],
    "labs": [
      "workflow-mapping",
      "agent-evaluation",
      "data-hygiene",
      "quality-gate-design"
    ],
    "deliverable": {
      "title": "Workflow Automation Proposal",
      "description": "Document a real workflow with AI integration opportunities, including ROI analysis"
    }
  },
  "modules": [
    {
      "id": "2.1-process-analysis",
      "slug": "2.1-process-analysis",
      "title": "Process Analysis for AI Integration",
      "phase": 2,
      "module": 1,
      "phaseId": "phase-2",
      "estimatedMinutes": 12,
      "bloomLevel": "analyze",
      "content": "# Process Analysis for AI Integration\n\n## WHY This Matters\n\nBefore you can improve a workflow with AI, you need to understand it deeply. Most AI implementation failures come from:\n\n- Automating the wrong tasks\n- Ignoring upstream/downstream dependencies\n- Underestimating human judgment requirements\n- Missing the real bottlenecks\n\nProcess analysis reveals **where AI can add value** versus where it would add friction.\n\n---\n\n## WHAT You Need to Know\n\n### The Process Mapping Framework\n\n**Basic process documentation template:**\n\n```\nStep [N]: [Name]\n├── Input: What triggers/feeds this step\n├── Action: What happens\n├── Output: What's produced\n├── Owner: Who's responsible\n├── Time: How long it takes\n├── AI Potential: None / Assist / Automate\n└── Notes: Pain points, dependencies, variations\n```\n\n### AI Opportunity Classification\n\nNot all tasks are equal candidates for AI. Use this classification:\n\n| Category | Characteristics | AI Role | Examples |\n|----------|-----------------|---------|----------|\n| **Automate** | Repetitive, rule-based, low judgment | Full automation | Data extraction, formatting, routing |\n| **Assist** | Complex but structured, benefits from speed | Human + AI together | Drafting, analysis, suggestions |\n| **Augment** | High judgment, benefits from information | AI provides inputs | Decision support, research, synthesis |\n| **Avoid** | Relationship-critical, highly variable, risky | Keep human | Negotiations, crisis management, final approvals |\n\n### The AI Suitability Scorecard\n\n### Bottleneck Analysis\n\n**Finding bottlenecks:**\n\n| Signal | What to Look For |\n|--------|------------------|\n| Queue length | Where does work pile up? |\n| Wait times | Where are the longest delays? |\n| Overtime | Which steps require extra hours? |\n| Complaints | Where do people express frustration? |\n| Errors | Where do mistakes happen most? |\n| Workarounds | Where have people created unofficial shortcuts? |\n\n---\n\n## HOW to Apply This\n\n### Exercise: Map and Score a Process\n\n### Common Process Patterns and AI Potential\n\n| Pattern | Description | AI Integration |\n|---------|-------------|----------------|\n| **Intake & Triage** | Receiving items and routing them | High: Classification, extraction |\n| **Research & Gather** | Collecting information | High: Search, summarization |\n| **Draft & Create** | Producing content | High: Generation, templates |\n| **Review & Approve** | Quality checking | Medium: Flagging, suggestions |\n| **Communicate & Notify** | Sending updates | High: Personalization, scheduling |\n| **Analyze & Report** | Making sense of data | High: Patterns, visualization |\n| **Decide & Act** | Making choices | Low: Support only |\n\n### Red Flags: When NOT to Automate\n\n| Red Flag | Why It's Risky | Better Approach |\n|----------|----------------|-----------------|\n| \"It depends\" steps | Too much variability | Assist, don't automate |\n| Customer emotions involved | Relationship risk | Human with AI support |\n| Regulatory/legal review | Compliance risk | AI drafts, human approves |\n| Steps that recently changed | Process not stable | Wait for stabilization |\n| Steps no one understands | Can't verify AI output | Document first |\n\n### Self-Check\n\n---\n\n## Up Next\n\nIn **Module 2.2: Task Decomposition**, you'll learn how to break complex tasks into AI-manageable steps—the foundation of effective workflow automation.",
      "htmlContent": "<h1>Process Analysis for AI Integration</h1>\n<h2>WHY This Matters</h2>\n<p>Before you can improve a workflow with AI, you need to understand it deeply. Most AI implementation failures come from:</p>\n<ul>\n<li>Automating the wrong tasks</li>\n<li>Ignoring upstream/downstream dependencies</li>\n<li>Underestimating human judgment requirements</li>\n<li>Missing the real bottlenecks</li>\n</ul>\n<p>Process analysis reveals <strong>where AI can add value</strong> versus where it would add friction.</p>\n<hr>\n<h2>WHAT You Need to Know</h2>\n<h3>The Process Mapping Framework</h3>\n<p><strong>Basic process documentation template:</strong></p>\n<pre><code>Step [N]: [Name]\n├── Input: What triggers/feeds this step\n├── Action: What happens\n├── Output: What&#39;s produced\n├── Owner: Who&#39;s responsible\n├── Time: How long it takes\n├── AI Potential: None / Assist / Automate\n└── Notes: Pain points, dependencies, variations\n</code></pre>\n<h3>AI Opportunity Classification</h3>\n<p>Not all tasks are equal candidates for AI. Use this classification:</p>\n<table>\n<thead>\n<tr>\n<th>Category</th>\n<th>Characteristics</th>\n<th>AI Role</th>\n<th>Examples</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Automate</strong></td>\n<td>Repetitive, rule-based, low judgment</td>\n<td>Full automation</td>\n<td>Data extraction, formatting, routing</td>\n</tr>\n<tr>\n<td><strong>Assist</strong></td>\n<td>Complex but structured, benefits from speed</td>\n<td>Human + AI together</td>\n<td>Drafting, analysis, suggestions</td>\n</tr>\n<tr>\n<td><strong>Augment</strong></td>\n<td>High judgment, benefits from information</td>\n<td>AI provides inputs</td>\n<td>Decision support, research, synthesis</td>\n</tr>\n<tr>\n<td><strong>Avoid</strong></td>\n<td>Relationship-critical, highly variable, risky</td>\n<td>Keep human</td>\n<td>Negotiations, crisis management, final approvals</td>\n</tr>\n</tbody></table>\n<h3>The AI Suitability Scorecard</h3>\n<h3>Bottleneck Analysis</h3>\n<p><strong>Finding bottlenecks:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Signal</th>\n<th>What to Look For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Queue length</td>\n<td>Where does work pile up?</td>\n</tr>\n<tr>\n<td>Wait times</td>\n<td>Where are the longest delays?</td>\n</tr>\n<tr>\n<td>Overtime</td>\n<td>Which steps require extra hours?</td>\n</tr>\n<tr>\n<td>Complaints</td>\n<td>Where do people express frustration?</td>\n</tr>\n<tr>\n<td>Errors</td>\n<td>Where do mistakes happen most?</td>\n</tr>\n<tr>\n<td>Workarounds</td>\n<td>Where have people created unofficial shortcuts?</td>\n</tr>\n</tbody></table>\n<hr>\n<h2>HOW to Apply This</h2>\n<h3>Exercise: Map and Score a Process</h3>\n<h3>Common Process Patterns and AI Potential</h3>\n<table>\n<thead>\n<tr>\n<th>Pattern</th>\n<th>Description</th>\n<th>AI Integration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Intake &amp; Triage</strong></td>\n<td>Receiving items and routing them</td>\n<td>High: Classification, extraction</td>\n</tr>\n<tr>\n<td><strong>Research &amp; Gather</strong></td>\n<td>Collecting information</td>\n<td>High: Search, summarization</td>\n</tr>\n<tr>\n<td><strong>Draft &amp; Create</strong></td>\n<td>Producing content</td>\n<td>High: Generation, templates</td>\n</tr>\n<tr>\n<td><strong>Review &amp; Approve</strong></td>\n<td>Quality checking</td>\n<td>Medium: Flagging, suggestions</td>\n</tr>\n<tr>\n<td><strong>Communicate &amp; Notify</strong></td>\n<td>Sending updates</td>\n<td>High: Personalization, scheduling</td>\n</tr>\n<tr>\n<td><strong>Analyze &amp; Report</strong></td>\n<td>Making sense of data</td>\n<td>High: Patterns, visualization</td>\n</tr>\n<tr>\n<td><strong>Decide &amp; Act</strong></td>\n<td>Making choices</td>\n<td>Low: Support only</td>\n</tr>\n</tbody></table>\n<h3>Red Flags: When NOT to Automate</h3>\n<table>\n<thead>\n<tr>\n<th>Red Flag</th>\n<th>Why It&#39;s Risky</th>\n<th>Better Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&quot;It depends&quot; steps</td>\n<td>Too much variability</td>\n<td>Assist, don&#39;t automate</td>\n</tr>\n<tr>\n<td>Customer emotions involved</td>\n<td>Relationship risk</td>\n<td>Human with AI support</td>\n</tr>\n<tr>\n<td>Regulatory/legal review</td>\n<td>Compliance risk</td>\n<td>AI drafts, human approves</td>\n</tr>\n<tr>\n<td>Steps that recently changed</td>\n<td>Process not stable</td>\n<td>Wait for stabilization</td>\n</tr>\n<tr>\n<td>Steps no one understands</td>\n<td>Can&#39;t verify AI output</td>\n<td>Document first</td>\n</tr>\n</tbody></table>\n<h3>Self-Check</h3>\n<hr>\n<h2>Up Next</h2>\n<p>In <strong>Module 2.2: Task Decomposition</strong>, you&#39;ll learn how to break complex tasks into AI-manageable steps—the foundation of effective workflow automation.</p>\n",
      "sections": [
        {
          "id": "why-this-matters",
          "title": "WHY This Matters",
          "type": "why",
          "content": "Before you can improve a workflow with AI, you need to understand it deeply. Most AI implementation failures come from:\n\n- Automating the wrong tasks\n- Ignoring upstream/downstream dependencies\n- Underestimating human judgment requirements\n- Missing the real bottlenecks\n\nProcess analysis reveals **where AI can add value** versus where it would add friction.\n\n---",
          "htmlContent": "<p>Before you can improve a workflow with AI, you need to understand it deeply. Most AI implementation failures come from:</p>\n<ul>\n<li>Automating the wrong tasks</li>\n<li>Ignoring upstream/downstream dependencies</li>\n<li>Underestimating human judgment requirements</li>\n<li>Missing the real bottlenecks</li>\n</ul>\n<p>Process analysis reveals <strong>where AI can add value</strong> versus where it would add friction.</p>\n<hr>\n"
        },
        {
          "id": "what-you-need-to-know",
          "title": "WHAT You Need to Know",
          "type": "what",
          "content": "### The Process Mapping Framework\n\n**Basic process documentation template:**\n\n```\nStep [N]: [Name]\n├── Input: What triggers/feeds this step\n├── Action: What happens\n├── Output: What's produced\n├── Owner: Who's responsible\n├── Time: How long it takes\n├── AI Potential: None / Assist / Automate\n└── Notes: Pain points, dependencies, variations\n```\n\n### AI Opportunity Classification\n\nNot all tasks are equal candidates for AI. Use this classification:\n\n| Category | Characteristics | AI Role | Examples |\n|----------|-----------------|---------|----------|\n| **Automate** | Repetitive, rule-based, low judgment | Full automation | Data extraction, formatting, routing |\n| **Assist** | Complex but structured, benefits from speed | Human + AI together | Drafting, analysis, suggestions |\n| **Augment** | High judgment, benefits from information | AI provides inputs | Decision support, research, synthesis |\n| **Avoid** | Relationship-critical, highly variable, risky | Keep human | Negotiations, crisis management, final approvals |\n\n### The AI Suitability Scorecard\n\n### Bottleneck Analysis\n\n**Finding bottlenecks:**\n\n| Signal | What to Look For |\n|--------|------------------|\n| Queue length | Where does work pile up? |\n| Wait times | Where are the longest delays? |\n| Overtime | Which steps require extra hours? |\n| Complaints | Where do people express frustration? |\n| Errors | Where do mistakes happen most? |\n| Workarounds | Where have people created unofficial shortcuts? |\n\n---",
          "htmlContent": "<h3>The Process Mapping Framework</h3>\n<p><strong>Basic process documentation template:</strong></p>\n<pre><code>Step [N]: [Name]\n├── Input: What triggers/feeds this step\n├── Action: What happens\n├── Output: What&#39;s produced\n├── Owner: Who&#39;s responsible\n├── Time: How long it takes\n├── AI Potential: None / Assist / Automate\n└── Notes: Pain points, dependencies, variations\n</code></pre>\n<h3>AI Opportunity Classification</h3>\n<p>Not all tasks are equal candidates for AI. Use this classification:</p>\n<table>\n<thead>\n<tr>\n<th>Category</th>\n<th>Characteristics</th>\n<th>AI Role</th>\n<th>Examples</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Automate</strong></td>\n<td>Repetitive, rule-based, low judgment</td>\n<td>Full automation</td>\n<td>Data extraction, formatting, routing</td>\n</tr>\n<tr>\n<td><strong>Assist</strong></td>\n<td>Complex but structured, benefits from speed</td>\n<td>Human + AI together</td>\n<td>Drafting, analysis, suggestions</td>\n</tr>\n<tr>\n<td><strong>Augment</strong></td>\n<td>High judgment, benefits from information</td>\n<td>AI provides inputs</td>\n<td>Decision support, research, synthesis</td>\n</tr>\n<tr>\n<td><strong>Avoid</strong></td>\n<td>Relationship-critical, highly variable, risky</td>\n<td>Keep human</td>\n<td>Negotiations, crisis management, final approvals</td>\n</tr>\n</tbody></table>\n<h3>The AI Suitability Scorecard</h3>\n<h3>Bottleneck Analysis</h3>\n<p><strong>Finding bottlenecks:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Signal</th>\n<th>What to Look For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Queue length</td>\n<td>Where does work pile up?</td>\n</tr>\n<tr>\n<td>Wait times</td>\n<td>Where are the longest delays?</td>\n</tr>\n<tr>\n<td>Overtime</td>\n<td>Which steps require extra hours?</td>\n</tr>\n<tr>\n<td>Complaints</td>\n<td>Where do people express frustration?</td>\n</tr>\n<tr>\n<td>Errors</td>\n<td>Where do mistakes happen most?</td>\n</tr>\n<tr>\n<td>Workarounds</td>\n<td>Where have people created unofficial shortcuts?</td>\n</tr>\n</tbody></table>\n<hr>\n"
        },
        {
          "id": "how-to-apply-this",
          "title": "HOW to Apply This",
          "type": "how",
          "content": "### Exercise: Map and Score a Process\n\n### Common Process Patterns and AI Potential\n\n| Pattern | Description | AI Integration |\n|---------|-------------|----------------|\n| **Intake & Triage** | Receiving items and routing them | High: Classification, extraction |\n| **Research & Gather** | Collecting information | High: Search, summarization |\n| **Draft & Create** | Producing content | High: Generation, templates |\n| **Review & Approve** | Quality checking | Medium: Flagging, suggestions |\n| **Communicate & Notify** | Sending updates | High: Personalization, scheduling |\n| **Analyze & Report** | Making sense of data | High: Patterns, visualization |\n| **Decide & Act** | Making choices | Low: Support only |\n\n### Red Flags: When NOT to Automate\n\n| Red Flag | Why It's Risky | Better Approach |\n|----------|----------------|-----------------|\n| \"It depends\" steps | Too much variability | Assist, don't automate |\n| Customer emotions involved | Relationship risk | Human with AI support |\n| Regulatory/legal review | Compliance risk | AI drafts, human approves |\n| Steps that recently changed | Process not stable | Wait for stabilization |\n| Steps no one understands | Can't verify AI output | Document first |\n\n### Self-Check\n\n---",
          "htmlContent": "<h3>Exercise: Map and Score a Process</h3>\n<h3>Common Process Patterns and AI Potential</h3>\n<table>\n<thead>\n<tr>\n<th>Pattern</th>\n<th>Description</th>\n<th>AI Integration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Intake &amp; Triage</strong></td>\n<td>Receiving items and routing them</td>\n<td>High: Classification, extraction</td>\n</tr>\n<tr>\n<td><strong>Research &amp; Gather</strong></td>\n<td>Collecting information</td>\n<td>High: Search, summarization</td>\n</tr>\n<tr>\n<td><strong>Draft &amp; Create</strong></td>\n<td>Producing content</td>\n<td>High: Generation, templates</td>\n</tr>\n<tr>\n<td><strong>Review &amp; Approve</strong></td>\n<td>Quality checking</td>\n<td>Medium: Flagging, suggestions</td>\n</tr>\n<tr>\n<td><strong>Communicate &amp; Notify</strong></td>\n<td>Sending updates</td>\n<td>High: Personalization, scheduling</td>\n</tr>\n<tr>\n<td><strong>Analyze &amp; Report</strong></td>\n<td>Making sense of data</td>\n<td>High: Patterns, visualization</td>\n</tr>\n<tr>\n<td><strong>Decide &amp; Act</strong></td>\n<td>Making choices</td>\n<td>Low: Support only</td>\n</tr>\n</tbody></table>\n<h3>Red Flags: When NOT to Automate</h3>\n<table>\n<thead>\n<tr>\n<th>Red Flag</th>\n<th>Why It&#39;s Risky</th>\n<th>Better Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&quot;It depends&quot; steps</td>\n<td>Too much variability</td>\n<td>Assist, don&#39;t automate</td>\n</tr>\n<tr>\n<td>Customer emotions involved</td>\n<td>Relationship risk</td>\n<td>Human with AI support</td>\n</tr>\n<tr>\n<td>Regulatory/legal review</td>\n<td>Compliance risk</td>\n<td>AI drafts, human approves</td>\n</tr>\n<tr>\n<td>Steps that recently changed</td>\n<td>Process not stable</td>\n<td>Wait for stabilization</td>\n</tr>\n<tr>\n<td>Steps no one understands</td>\n<td>Can&#39;t verify AI output</td>\n<td>Document first</td>\n</tr>\n</tbody></table>\n<h3>Self-Check</h3>\n<hr>\n"
        },
        {
          "id": "up-next",
          "title": "Up Next",
          "type": "generic",
          "content": "In **Module 2.2: Task Decomposition**, you'll learn how to break complex tasks into AI-manageable steps—the foundation of effective workflow automation.",
          "htmlContent": "<p>In <strong>Module 2.2: Task Decomposition</strong>, you&#39;ll learn how to break complex tasks into AI-manageable steps—the foundation of effective workflow automation.</p>\n"
        }
      ],
      "concepts": [
        {
          "id": "process-mapping",
          "term": "process mapping",
          "definition": "**Process mapping** creates a visual representation of how work flows through an organization. For AI integration, you need to capture:\n\n- **Steps**: Each distinct action in the workflow\n- **Inputs/Outputs**: What each step receives and produces\n- **Decision points**: Where human judgment is required\n- **Handoffs**: Where work transfers between people/systems\n- **Time**: Duration of each step and wait times\n- **Pain points**: Where problems commonly occur",
          "htmlDefinition": "<p><strong>Process mapping</strong> creates a visual representation of how work flows through an organization. For AI integration, you need to capture:</p>\n<ul>\n<li><strong>Steps</strong>: Each distinct action in the workflow</li>\n<li><strong>Inputs/Outputs</strong>: What each step receives and produces</li>\n<li><strong>Decision points</strong>: Where human judgment is required</li>\n<li><strong>Handoffs</strong>: Where work transfers between people/systems</li>\n<li><strong>Time</strong>: Duration of each step and wait times</li>\n<li><strong>Pain points</strong>: Where problems commonly occur</li>\n</ul>\n"
        },
        {
          "id": "ai-suitability",
          "term": "ai suitability",
          "definition": "Rate each process step on these dimensions to identify AI potential:\n\n**High AI Suitability:**\n- ✅ Repetitive (same task many times)\n- ✅ Data-intensive (lots of information to process)\n- ✅ Time-consuming for humans\n- ✅ Quality inconsistent with manual effort\n- ✅ Clear success criteria\n\n**Low AI Suitability:**\n- ❌ Requires physical presence\n- ❌ Needs real-time human relationship\n- ❌ High stakes with no verification possible\n- ❌ Highly variable (every instance unique)\n- ❌ Requires institutional memory/context AI can't access",
          "htmlDefinition": "<p>Rate each process step on these dimensions to identify AI potential:</p>\n<p><strong>High AI Suitability:</strong></p>\n<ul>\n<li>✅ Repetitive (same task many times)</li>\n<li>✅ Data-intensive (lots of information to process)</li>\n<li>✅ Time-consuming for humans</li>\n<li>✅ Quality inconsistent with manual effort</li>\n<li>✅ Clear success criteria</li>\n</ul>\n<p><strong>Low AI Suitability:</strong></p>\n<ul>\n<li>❌ Requires physical presence</li>\n<li>❌ Needs real-time human relationship</li>\n<li>❌ High stakes with no verification possible</li>\n<li>❌ Highly variable (every instance unique)</li>\n<li>❌ Requires institutional memory/context AI can&#39;t access</li>\n</ul>\n"
        },
        {
          "id": "bottleneck",
          "term": "bottleneck",
          "definition": "A **bottleneck** is any point where work accumulates faster than it can be processed. AI often has the most impact at bottlenecks—but only if the bottleneck is due to processing capacity, not other constraints.\n\n**Bottleneck types:**\n- **Capacity bottleneck**: Not enough people/time → AI can help\n- **Skill bottleneck**: Need rare expertise → AI can help\n- **Information bottleneck**: Waiting for data → Maybe AI can help\n- **Approval bottleneck**: Waiting for decisions → Usually can't automate\n- **Dependency bottleneck**: Waiting for others → Requires process change",
          "htmlDefinition": "<p>A <strong>bottleneck</strong> is any point where work accumulates faster than it can be processed. AI often has the most impact at bottlenecks—but only if the bottleneck is due to processing capacity, not other constraints.</p>\n<p><strong>Bottleneck types:</strong></p>\n<ul>\n<li><strong>Capacity bottleneck</strong>: Not enough people/time → AI can help</li>\n<li><strong>Skill bottleneck</strong>: Need rare expertise → AI can help</li>\n<li><strong>Information bottleneck</strong>: Waiting for data → Maybe AI can help</li>\n<li><strong>Approval bottleneck</strong>: Waiting for decisions → Usually can&#39;t automate</li>\n<li><strong>Dependency bottleneck</strong>: Waiting for others → Requires process change</li>\n</ul>\n"
        }
      ],
      "exercises": [
        {
          "id": "process-mapping-exercise",
          "title": "Choose a workflow you're familiar with",
          "instructions": "(work process, personal routine, or hypothetical business scenario).\n\n**Part 1: Map the process**\nDocument 5-10 steps using the template above.\n\n**Part 2: Score AI suitability**\nFor each step, rate 1-5:\n- Repetitiveness\n- Data intensity\n- Time consumption\n- Quality variability\n- Criteria clarity\n\n**Part 3: Identify opportunities**\n- Which steps score highest for AI?\n- What type of AI integration? (Automate/Assist/Augment)\n- Where are the bottlenecks?\n- What dependencies might complicate AI integration?\n\n**Example processes to analyze:**\n- Customer onboarding\n- Invoice processing\n- Content creation pipeline\n- Hiring workflow\n- Customer support ticket handling",
          "htmlInstructions": "<p>(work process, personal routine, or hypothetical business scenario).</p>\n<p><strong>Part 1: Map the process</strong>\nDocument 5-10 steps using the template above.</p>\n<p><strong>Part 2: Score AI suitability</strong>\nFor each step, rate 1-5:</p>\n<ul>\n<li>Repetitiveness</li>\n<li>Data intensity</li>\n<li>Time consumption</li>\n<li>Quality variability</li>\n<li>Criteria clarity</li>\n</ul>\n<p><strong>Part 3: Identify opportunities</strong></p>\n<ul>\n<li>Which steps score highest for AI?</li>\n<li>What type of AI integration? (Automate/Assist/Augment)</li>\n<li>Where are the bottlenecks?</li>\n<li>What dependencies might complicate AI integration?</li>\n</ul>\n<p><strong>Example processes to analyze:</strong></p>\n<ul>\n<li>Customer onboarding</li>\n<li>Invoice processing</li>\n<li>Content creation pipeline</li>\n<li>Hiring workflow</li>\n<li>Customer support ticket handling</li>\n</ul>\n"
        }
      ],
      "checklists": [
        {
          "id": "module-2.1-complete",
          "items": [
            {
              "id": "module-2.1-complete-0",
              "text": "I can document a process with all key elements",
              "completed": false
            },
            {
              "id": "module-2.1-complete-1",
              "text": "I can classify tasks by AI suitability",
              "completed": false
            },
            {
              "id": "module-2.1-complete-2",
              "text": "I understand the different types of AI integration (automate/assist/augment)",
              "completed": false
            },
            {
              "id": "module-2.1-complete-3",
              "text": "I can identify bottlenecks in a workflow",
              "completed": false
            },
            {
              "id": "module-2.1-complete-4",
              "text": "I know the red flags that indicate a step shouldn't be automated",
              "completed": false
            }
          ]
        }
      ]
    },
    {
      "id": "2.2-task-decomposition",
      "slug": "2.2-task-decomposition",
      "title": "Task Decomposition",
      "phase": 2,
      "module": 2,
      "phaseId": "phase-2",
      "estimatedMinutes": 10,
      "bloomLevel": "apply",
      "content": "# Task Decomposition\n\n## WHY This Matters\n\nAI excels at focused, well-defined tasks. It struggles with ambiguous, multi-part requests. The skill of breaking complex work into AI-manageable chunks is what separates effective operators from frustrated users who complain \"AI doesn't understand what I want.\"\n\n**The decomposition advantage:**\n- Higher quality outputs (focused attention)\n- Better error isolation (know which step failed)\n- Reusable components (build a library of prompts)\n- Clearer human oversight (review at checkpoints)\n\n---\n\n## WHAT You Need to Know\n\n### The Atomic Task Principle\n\n**Decomposition example: \"Write a marketing campaign\"**\n\n❌ **Too big (not atomic):**\n> \"Create a complete marketing campaign for our product launch\"\n\n✅ **Properly decomposed:**\n1. Generate 10 campaign theme concepts (brainstorm)\n2. Evaluate themes against brand guidelines (analysis)\n3. Develop messaging pillars for chosen theme (strategy)\n4. Write email subject lines (copy generation)\n5. Write email body copy (copy generation)\n6. Create social media post variants (adaptation)\n7. Generate ad headlines (copy generation)\n8. Review all copy for consistency (quality check)\n\n### Dependency Mapping\n\n**Dependency visualization:**\n\n```\n[Research] → [Outline] → [Draft] → [Edit] → [Final]\n     ↓           ↓\n[Competitor  [Examples\n Analysis]   Search]\n     ↓           ↓\n     └─────→ [Synthesize] ─→ [Insert into Draft]\n```\n\n### The Orchestration Pattern\n\nComplex workflows often follow this pattern:\n\n```\n┌─────────────────────────────────────────────────┐\n│                 ORCHESTRATOR                     │\n│  (coordinates tasks, manages state, handles      │\n│   errors, combines outputs)                      │\n└──────────────┬──────────────────────────────────┘\n               │\n    ┌──────────┼──────────┐\n    ↓          ↓          ↓\n[Task A]   [Task B]   [Task C]\n    ↓          ↓          ↓\n[Output A] [Output B] [Output C]\n    └──────────┼──────────┘\n               ↓\n        [Combined Result]\n```\n\n**Orchestration responsibilities:**\n- Task sequencing and parallel execution\n- Passing outputs between tasks\n- Error handling and retry logic\n- Human checkpoint management\n- Final assembly of outputs\n\n### The Decomposition Method\n\n**Step 1: Start with the end**\nWhat's the final deliverable? Work backwards.\n\n**Step 2: Identify natural breaks**\nWhere would you naturally pause, review, or make a decision?\n\n**Step 3: Define inputs and outputs**\nFor each chunk, what does it need? What does it produce?\n\n**Step 4: Check atomicity**\nCan each chunk be done by AI without asking clarifying questions?\n\n**Step 5: Map dependencies**\nWhich chunks depend on others? Which can run in parallel?\n\n**Step 6: Add checkpoints**\nWhere should humans verify before proceeding?\n\n---\n\n## HOW to Apply This\n\n### Exercise: Decompose Complex Tasks\n\n### Task Decomposition Template\n\n| # | Task Name | Input | Output | Dependencies | Human Checkpoint? |\n|---|-----------|-------|--------|--------------|-------------------|\n| 1 | | | | None | |\n| 2 | | | | Step 1 | |\n| 3 | | | | Steps 1, 2 | |\n| 4 | | | | Step 3 | ✓ |\n\n### Common Decomposition Patterns\n\n| Complex Task | Decomposition Pattern |\n|--------------|----------------------|\n| **Research report** | Gather → Filter → Analyze → Outline → Draft → Cite → Review |\n| **Content creation** | Brief → Research → Outline → Draft → Edit → Format → Review |\n| **Data analysis** | Extract → Clean → Transform → Analyze → Visualize → Narrate |\n| **Decision support** | Frame question → Gather options → Evaluate → Pros/cons → Recommend |\n| **Communication** | Audience analysis → Key messages → Draft → Tone check → Finalize |\n\n### When to Combine vs. Separate\n\n| Combine When | Separate When |\n|--------------|---------------|\n| Tasks share context that's expensive to repeat | Tasks might have different quality needs |\n| Output of one is trivially small | Mid-point review is valuable |\n| Combined takes < 30 seconds | Either task might fail independently |\n| No human judgment needed between | Different expertise/models optimal |\n\n### Self-Check\n\n---\n\n## Up Next\n\nIn **Module 2.3: Quality and Iteration**, you'll learn how to build quality assurance into AI workflows—validation, iteration, and feedback loops.",
      "htmlContent": "<h1>Task Decomposition</h1>\n<h2>WHY This Matters</h2>\n<p>AI excels at focused, well-defined tasks. It struggles with ambiguous, multi-part requests. The skill of breaking complex work into AI-manageable chunks is what separates effective operators from frustrated users who complain &quot;AI doesn&#39;t understand what I want.&quot;</p>\n<p><strong>The decomposition advantage:</strong></p>\n<ul>\n<li>Higher quality outputs (focused attention)</li>\n<li>Better error isolation (know which step failed)</li>\n<li>Reusable components (build a library of prompts)</li>\n<li>Clearer human oversight (review at checkpoints)</li>\n</ul>\n<hr>\n<h2>WHAT You Need to Know</h2>\n<h3>The Atomic Task Principle</h3>\n<p><strong>Decomposition example: &quot;Write a marketing campaign&quot;</strong></p>\n<p>❌ <strong>Too big (not atomic):</strong></p>\n<blockquote>\n<p>&quot;Create a complete marketing campaign for our product launch&quot;</p>\n</blockquote>\n<p>✅ <strong>Properly decomposed:</strong></p>\n<ol>\n<li>Generate 10 campaign theme concepts (brainstorm)</li>\n<li>Evaluate themes against brand guidelines (analysis)</li>\n<li>Develop messaging pillars for chosen theme (strategy)</li>\n<li>Write email subject lines (copy generation)</li>\n<li>Write email body copy (copy generation)</li>\n<li>Create social media post variants (adaptation)</li>\n<li>Generate ad headlines (copy generation)</li>\n<li>Review all copy for consistency (quality check)</li>\n</ol>\n<h3>Dependency Mapping</h3>\n<p><strong>Dependency visualization:</strong></p>\n<pre><code>[Research] → [Outline] → [Draft] → [Edit] → [Final]\n     ↓           ↓\n[Competitor  [Examples\n Analysis]   Search]\n     ↓           ↓\n     └─────→ [Synthesize] ─→ [Insert into Draft]\n</code></pre>\n<h3>The Orchestration Pattern</h3>\n<p>Complex workflows often follow this pattern:</p>\n<pre><code>┌─────────────────────────────────────────────────┐\n│                 ORCHESTRATOR                     │\n│  (coordinates tasks, manages state, handles      │\n│   errors, combines outputs)                      │\n└──────────────┬──────────────────────────────────┘\n               │\n    ┌──────────┼──────────┐\n    ↓          ↓          ↓\n[Task A]   [Task B]   [Task C]\n    ↓          ↓          ↓\n[Output A] [Output B] [Output C]\n    └──────────┼──────────┘\n               ↓\n        [Combined Result]\n</code></pre>\n<p><strong>Orchestration responsibilities:</strong></p>\n<ul>\n<li>Task sequencing and parallel execution</li>\n<li>Passing outputs between tasks</li>\n<li>Error handling and retry logic</li>\n<li>Human checkpoint management</li>\n<li>Final assembly of outputs</li>\n</ul>\n<h3>The Decomposition Method</h3>\n<p><strong>Step 1: Start with the end</strong>\nWhat&#39;s the final deliverable? Work backwards.</p>\n<p><strong>Step 2: Identify natural breaks</strong>\nWhere would you naturally pause, review, or make a decision?</p>\n<p><strong>Step 3: Define inputs and outputs</strong>\nFor each chunk, what does it need? What does it produce?</p>\n<p><strong>Step 4: Check atomicity</strong>\nCan each chunk be done by AI without asking clarifying questions?</p>\n<p><strong>Step 5: Map dependencies</strong>\nWhich chunks depend on others? Which can run in parallel?</p>\n<p><strong>Step 6: Add checkpoints</strong>\nWhere should humans verify before proceeding?</p>\n<hr>\n<h2>HOW to Apply This</h2>\n<h3>Exercise: Decompose Complex Tasks</h3>\n<h3>Task Decomposition Template</h3>\n<table>\n<thead>\n<tr>\n<th>#</th>\n<th>Task Name</th>\n<th>Input</th>\n<th>Output</th>\n<th>Dependencies</th>\n<th>Human Checkpoint?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td></td>\n<td></td>\n<td></td>\n<td>None</td>\n<td></td>\n</tr>\n<tr>\n<td>2</td>\n<td></td>\n<td></td>\n<td></td>\n<td>Step 1</td>\n<td></td>\n</tr>\n<tr>\n<td>3</td>\n<td></td>\n<td></td>\n<td></td>\n<td>Steps 1, 2</td>\n<td></td>\n</tr>\n<tr>\n<td>4</td>\n<td></td>\n<td></td>\n<td></td>\n<td>Step 3</td>\n<td>✓</td>\n</tr>\n</tbody></table>\n<h3>Common Decomposition Patterns</h3>\n<table>\n<thead>\n<tr>\n<th>Complex Task</th>\n<th>Decomposition Pattern</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Research report</strong></td>\n<td>Gather → Filter → Analyze → Outline → Draft → Cite → Review</td>\n</tr>\n<tr>\n<td><strong>Content creation</strong></td>\n<td>Brief → Research → Outline → Draft → Edit → Format → Review</td>\n</tr>\n<tr>\n<td><strong>Data analysis</strong></td>\n<td>Extract → Clean → Transform → Analyze → Visualize → Narrate</td>\n</tr>\n<tr>\n<td><strong>Decision support</strong></td>\n<td>Frame question → Gather options → Evaluate → Pros/cons → Recommend</td>\n</tr>\n<tr>\n<td><strong>Communication</strong></td>\n<td>Audience analysis → Key messages → Draft → Tone check → Finalize</td>\n</tr>\n</tbody></table>\n<h3>When to Combine vs. Separate</h3>\n<table>\n<thead>\n<tr>\n<th>Combine When</th>\n<th>Separate When</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tasks share context that&#39;s expensive to repeat</td>\n<td>Tasks might have different quality needs</td>\n</tr>\n<tr>\n<td>Output of one is trivially small</td>\n<td>Mid-point review is valuable</td>\n</tr>\n<tr>\n<td>Combined takes &lt; 30 seconds</td>\n<td>Either task might fail independently</td>\n</tr>\n<tr>\n<td>No human judgment needed between</td>\n<td>Different expertise/models optimal</td>\n</tr>\n</tbody></table>\n<h3>Self-Check</h3>\n<hr>\n<h2>Up Next</h2>\n<p>In <strong>Module 2.3: Quality and Iteration</strong>, you&#39;ll learn how to build quality assurance into AI workflows—validation, iteration, and feedback loops.</p>\n",
      "sections": [
        {
          "id": "why-this-matters",
          "title": "WHY This Matters",
          "type": "why",
          "content": "AI excels at focused, well-defined tasks. It struggles with ambiguous, multi-part requests. The skill of breaking complex work into AI-manageable chunks is what separates effective operators from frustrated users who complain \"AI doesn't understand what I want.\"\n\n**The decomposition advantage:**\n- Higher quality outputs (focused attention)\n- Better error isolation (know which step failed)\n- Reusable components (build a library of prompts)\n- Clearer human oversight (review at checkpoints)\n\n---",
          "htmlContent": "<p>AI excels at focused, well-defined tasks. It struggles with ambiguous, multi-part requests. The skill of breaking complex work into AI-manageable chunks is what separates effective operators from frustrated users who complain &quot;AI doesn&#39;t understand what I want.&quot;</p>\n<p><strong>The decomposition advantage:</strong></p>\n<ul>\n<li>Higher quality outputs (focused attention)</li>\n<li>Better error isolation (know which step failed)</li>\n<li>Reusable components (build a library of prompts)</li>\n<li>Clearer human oversight (review at checkpoints)</li>\n</ul>\n<hr>\n"
        },
        {
          "id": "what-you-need-to-know",
          "title": "WHAT You Need to Know",
          "type": "what",
          "content": "### The Atomic Task Principle\n\n**Decomposition example: \"Write a marketing campaign\"**\n\n❌ **Too big (not atomic):**\n> \"Create a complete marketing campaign for our product launch\"\n\n✅ **Properly decomposed:**\n1. Generate 10 campaign theme concepts (brainstorm)\n2. Evaluate themes against brand guidelines (analysis)\n3. Develop messaging pillars for chosen theme (strategy)\n4. Write email subject lines (copy generation)\n5. Write email body copy (copy generation)\n6. Create social media post variants (adaptation)\n7. Generate ad headlines (copy generation)\n8. Review all copy for consistency (quality check)\n\n### Dependency Mapping\n\n**Dependency visualization:**\n\n```\n[Research] → [Outline] → [Draft] → [Edit] → [Final]\n     ↓           ↓\n[Competitor  [Examples\n Analysis]   Search]\n     ↓           ↓\n     └─────→ [Synthesize] ─→ [Insert into Draft]\n```\n\n### The Orchestration Pattern\n\nComplex workflows often follow this pattern:\n\n```\n┌─────────────────────────────────────────────────┐\n│                 ORCHESTRATOR                     │\n│  (coordinates tasks, manages state, handles      │\n│   errors, combines outputs)                      │\n└──────────────┬──────────────────────────────────┘\n               │\n    ┌──────────┼──────────┐\n    ↓          ↓          ↓\n[Task A]   [Task B]   [Task C]\n    ↓          ↓          ↓\n[Output A] [Output B] [Output C]\n    └──────────┼──────────┘\n               ↓\n        [Combined Result]\n```\n\n**Orchestration responsibilities:**\n- Task sequencing and parallel execution\n- Passing outputs between tasks\n- Error handling and retry logic\n- Human checkpoint management\n- Final assembly of outputs\n\n### The Decomposition Method\n\n**Step 1: Start with the end**\nWhat's the final deliverable? Work backwards.\n\n**Step 2: Identify natural breaks**\nWhere would you naturally pause, review, or make a decision?\n\n**Step 3: Define inputs and outputs**\nFor each chunk, what does it need? What does it produce?\n\n**Step 4: Check atomicity**\nCan each chunk be done by AI without asking clarifying questions?\n\n**Step 5: Map dependencies**\nWhich chunks depend on others? Which can run in parallel?\n\n**Step 6: Add checkpoints**\nWhere should humans verify before proceeding?\n\n---",
          "htmlContent": "<h3>The Atomic Task Principle</h3>\n<p><strong>Decomposition example: &quot;Write a marketing campaign&quot;</strong></p>\n<p>❌ <strong>Too big (not atomic):</strong></p>\n<blockquote>\n<p>&quot;Create a complete marketing campaign for our product launch&quot;</p>\n</blockquote>\n<p>✅ <strong>Properly decomposed:</strong></p>\n<ol>\n<li>Generate 10 campaign theme concepts (brainstorm)</li>\n<li>Evaluate themes against brand guidelines (analysis)</li>\n<li>Develop messaging pillars for chosen theme (strategy)</li>\n<li>Write email subject lines (copy generation)</li>\n<li>Write email body copy (copy generation)</li>\n<li>Create social media post variants (adaptation)</li>\n<li>Generate ad headlines (copy generation)</li>\n<li>Review all copy for consistency (quality check)</li>\n</ol>\n<h3>Dependency Mapping</h3>\n<p><strong>Dependency visualization:</strong></p>\n<pre><code>[Research] → [Outline] → [Draft] → [Edit] → [Final]\n     ↓           ↓\n[Competitor  [Examples\n Analysis]   Search]\n     ↓           ↓\n     └─────→ [Synthesize] ─→ [Insert into Draft]\n</code></pre>\n<h3>The Orchestration Pattern</h3>\n<p>Complex workflows often follow this pattern:</p>\n<pre><code>┌─────────────────────────────────────────────────┐\n│                 ORCHESTRATOR                     │\n│  (coordinates tasks, manages state, handles      │\n│   errors, combines outputs)                      │\n└──────────────┬──────────────────────────────────┘\n               │\n    ┌──────────┼──────────┐\n    ↓          ↓          ↓\n[Task A]   [Task B]   [Task C]\n    ↓          ↓          ↓\n[Output A] [Output B] [Output C]\n    └──────────┼──────────┘\n               ↓\n        [Combined Result]\n</code></pre>\n<p><strong>Orchestration responsibilities:</strong></p>\n<ul>\n<li>Task sequencing and parallel execution</li>\n<li>Passing outputs between tasks</li>\n<li>Error handling and retry logic</li>\n<li>Human checkpoint management</li>\n<li>Final assembly of outputs</li>\n</ul>\n<h3>The Decomposition Method</h3>\n<p><strong>Step 1: Start with the end</strong>\nWhat&#39;s the final deliverable? Work backwards.</p>\n<p><strong>Step 2: Identify natural breaks</strong>\nWhere would you naturally pause, review, or make a decision?</p>\n<p><strong>Step 3: Define inputs and outputs</strong>\nFor each chunk, what does it need? What does it produce?</p>\n<p><strong>Step 4: Check atomicity</strong>\nCan each chunk be done by AI without asking clarifying questions?</p>\n<p><strong>Step 5: Map dependencies</strong>\nWhich chunks depend on others? Which can run in parallel?</p>\n<p><strong>Step 6: Add checkpoints</strong>\nWhere should humans verify before proceeding?</p>\n<hr>\n"
        },
        {
          "id": "how-to-apply-this",
          "title": "HOW to Apply This",
          "type": "how",
          "content": "### Exercise: Decompose Complex Tasks\n\n### Task Decomposition Template\n\n| # | Task Name | Input | Output | Dependencies | Human Checkpoint? |\n|---|-----------|-------|--------|--------------|-------------------|\n| 1 | | | | None | |\n| 2 | | | | Step 1 | |\n| 3 | | | | Steps 1, 2 | |\n| 4 | | | | Step 3 | ✓ |\n\n### Common Decomposition Patterns\n\n| Complex Task | Decomposition Pattern |\n|--------------|----------------------|\n| **Research report** | Gather → Filter → Analyze → Outline → Draft → Cite → Review |\n| **Content creation** | Brief → Research → Outline → Draft → Edit → Format → Review |\n| **Data analysis** | Extract → Clean → Transform → Analyze → Visualize → Narrate |\n| **Decision support** | Frame question → Gather options → Evaluate → Pros/cons → Recommend |\n| **Communication** | Audience analysis → Key messages → Draft → Tone check → Finalize |\n\n### When to Combine vs. Separate\n\n| Combine When | Separate When |\n|--------------|---------------|\n| Tasks share context that's expensive to repeat | Tasks might have different quality needs |\n| Output of one is trivially small | Mid-point review is valuable |\n| Combined takes < 30 seconds | Either task might fail independently |\n| No human judgment needed between | Different expertise/models optimal |\n\n### Self-Check\n\n---",
          "htmlContent": "<h3>Exercise: Decompose Complex Tasks</h3>\n<h3>Task Decomposition Template</h3>\n<table>\n<thead>\n<tr>\n<th>#</th>\n<th>Task Name</th>\n<th>Input</th>\n<th>Output</th>\n<th>Dependencies</th>\n<th>Human Checkpoint?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td></td>\n<td></td>\n<td></td>\n<td>None</td>\n<td></td>\n</tr>\n<tr>\n<td>2</td>\n<td></td>\n<td></td>\n<td></td>\n<td>Step 1</td>\n<td></td>\n</tr>\n<tr>\n<td>3</td>\n<td></td>\n<td></td>\n<td></td>\n<td>Steps 1, 2</td>\n<td></td>\n</tr>\n<tr>\n<td>4</td>\n<td></td>\n<td></td>\n<td></td>\n<td>Step 3</td>\n<td>✓</td>\n</tr>\n</tbody></table>\n<h3>Common Decomposition Patterns</h3>\n<table>\n<thead>\n<tr>\n<th>Complex Task</th>\n<th>Decomposition Pattern</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Research report</strong></td>\n<td>Gather → Filter → Analyze → Outline → Draft → Cite → Review</td>\n</tr>\n<tr>\n<td><strong>Content creation</strong></td>\n<td>Brief → Research → Outline → Draft → Edit → Format → Review</td>\n</tr>\n<tr>\n<td><strong>Data analysis</strong></td>\n<td>Extract → Clean → Transform → Analyze → Visualize → Narrate</td>\n</tr>\n<tr>\n<td><strong>Decision support</strong></td>\n<td>Frame question → Gather options → Evaluate → Pros/cons → Recommend</td>\n</tr>\n<tr>\n<td><strong>Communication</strong></td>\n<td>Audience analysis → Key messages → Draft → Tone check → Finalize</td>\n</tr>\n</tbody></table>\n<h3>When to Combine vs. Separate</h3>\n<table>\n<thead>\n<tr>\n<th>Combine When</th>\n<th>Separate When</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tasks share context that&#39;s expensive to repeat</td>\n<td>Tasks might have different quality needs</td>\n</tr>\n<tr>\n<td>Output of one is trivially small</td>\n<td>Mid-point review is valuable</td>\n</tr>\n<tr>\n<td>Combined takes &lt; 30 seconds</td>\n<td>Either task might fail independently</td>\n</tr>\n<tr>\n<td>No human judgment needed between</td>\n<td>Different expertise/models optimal</td>\n</tr>\n</tbody></table>\n<h3>Self-Check</h3>\n<hr>\n"
        },
        {
          "id": "up-next",
          "title": "Up Next",
          "type": "generic",
          "content": "In **Module 2.3: Quality and Iteration**, you'll learn how to build quality assurance into AI workflows—validation, iteration, and feedback loops.",
          "htmlContent": "<p>In <strong>Module 2.3: Quality and Iteration</strong>, you&#39;ll learn how to build quality assurance into AI workflows—validation, iteration, and feedback loops.</p>\n"
        }
      ],
      "concepts": [
        {
          "id": "atomic-task",
          "term": "atomic task",
          "definition": "An **atomic task** is the smallest unit of work that produces a meaningful, verifiable output. For AI, atomic tasks should:\n\n- Have a single, clear objective\n- Require no mid-task decisions\n- Produce output that can be verified independently\n- Take seconds to minutes, not hours\n- Be repeatable with consistent results",
          "htmlDefinition": "<p>An <strong>atomic task</strong> is the smallest unit of work that produces a meaningful, verifiable output. For AI, atomic tasks should:</p>\n<ul>\n<li>Have a single, clear objective</li>\n<li>Require no mid-task decisions</li>\n<li>Produce output that can be verified independently</li>\n<li>Take seconds to minutes, not hours</li>\n<li>Be repeatable with consistent results</li>\n</ul>\n"
        },
        {
          "id": "dependencies",
          "term": "dependencies",
          "definition": "**Dependencies** are relationships between tasks where one must complete before another can start. Types include:\n\n- **Sequential**: Task B needs output from Task A\n- **Parallel**: Tasks can run simultaneously\n- **Conditional**: Task B only runs if Task A produces specific result\n- **Iterative**: Task repeats until condition met",
          "htmlDefinition": "<p><strong>Dependencies</strong> are relationships between tasks where one must complete before another can start. Types include:</p>\n<ul>\n<li><strong>Sequential</strong>: Task B needs output from Task A</li>\n<li><strong>Parallel</strong>: Tasks can run simultaneously</li>\n<li><strong>Conditional</strong>: Task B only runs if Task A produces specific result</li>\n<li><strong>Iterative</strong>: Task repeats until condition met</li>\n</ul>\n"
        }
      ],
      "exercises": [
        {
          "id": "decomposition-practice",
          "title": "Decompose each of these tasks into atomic steps:",
          "instructions": "**Task 1: Quarterly Business Review Presentation**\nBreak into steps covering: data gathering, analysis, story creation, slide content, and review.\n\n**Task 2: Customer Complaint Resolution**\nInclude: understanding the issue, researching history, drafting response, and escalation logic.\n\n**Task 3: Job Posting Creation**\nCover: requirements gathering, competitive analysis, writing, compliance check, and formatting.\n\n**For each decomposition, identify:**\n- Input needed for each step\n- Output produced by each step\n- Dependencies between steps\n- Where human checkpoints should be",
          "htmlInstructions": "<p><strong>Task 1: Quarterly Business Review Presentation</strong>\nBreak into steps covering: data gathering, analysis, story creation, slide content, and review.</p>\n<p><strong>Task 2: Customer Complaint Resolution</strong>\nInclude: understanding the issue, researching history, drafting response, and escalation logic.</p>\n<p><strong>Task 3: Job Posting Creation</strong>\nCover: requirements gathering, competitive analysis, writing, compliance check, and formatting.</p>\n<p><strong>For each decomposition, identify:</strong></p>\n<ul>\n<li>Input needed for each step</li>\n<li>Output produced by each step</li>\n<li>Dependencies between steps</li>\n<li>Where human checkpoints should be</li>\n</ul>\n"
        }
      ],
      "checklists": [
        {
          "id": "module-2.2-complete",
          "items": [
            {
              "id": "module-2.2-complete-0",
              "text": "I understand what makes a task \"atomic\"",
              "completed": false
            },
            {
              "id": "module-2.2-complete-1",
              "text": "I can identify dependencies between tasks",
              "completed": false
            },
            {
              "id": "module-2.2-complete-2",
              "text": "I can decompose complex work into AI-manageable steps",
              "completed": false
            },
            {
              "id": "module-2.2-complete-3",
              "text": "I know where to place human checkpoints",
              "completed": false
            },
            {
              "id": "module-2.2-complete-4",
              "text": "I can create a task decomposition template",
              "completed": false
            }
          ]
        }
      ]
    },
    {
      "id": "2.3-quality-and-iteration",
      "slug": "2.3-quality-and-iteration",
      "title": "Quality and Iteration",
      "phase": 2,
      "module": 3,
      "phaseId": "phase-2",
      "estimatedMinutes": 12,
      "bloomLevel": "apply",
      "content": "# Quality and Iteration\n\n## WHY This Matters\n\nAI doesn't know when its output is wrong. It can produce confident, well-formatted nonsense. Without quality controls, you're gambling with every AI output.\n\n**The quality imperative:**\n- AI outputs need verification (it doesn't self-correct)\n- Early feedback is cheaper than late discovery\n- Iteration is often faster than perfect prompts\n- Quality gates protect downstream work\n\nProfessionals don't ship first drafts. AI operators don't ship unvalidated outputs.\n\n---\n\n## WHAT You Need to Know\n\n### Quality Gate Architecture\n\n**Quality gate types:**\n\n| Gate Type | Mechanism | Use When |\n|-----------|-----------|----------|\n| **Automated** | Rule-based checks | Format, length, required elements |\n| **AI-Assisted** | Second AI evaluates first | Subjective quality, consistency |\n| **Human** | Person reviews | High stakes, nuanced judgment |\n| **Statistical** | Sampling approach | High volume, lower stakes |\n\n### Validation Strategies\n\n**1. Checklist Validation**\nDefine explicit criteria and verify each:\n\n```\n✓ Contains executive summary\n✓ All sections from template present\n✓ No placeholder text remaining\n✓ Word count within range (800-1200)\n✓ No competitor mentions\n✓ CTA included\n✗ Links verified (needs review)\n```\n\n**2. AI Cross-Check**\nUse a second prompt to evaluate the first output:\n\n```\nReview this [content type] against these criteria:\n1. [Criterion 1]: Pass/Fail + Explanation\n2. [Criterion 2]: Pass/Fail + Explanation\n...\n\nOutput: JSON with pass/fail for each criterion and overall assessment.\n```\n\n**3. Rubric Scoring**\n\n### The Iteration Loop\n\n**When to iterate vs. when to restart:**\n\n| Situation | Action | Rationale |\n|-----------|--------|-----------|\n| 80%+ acceptable, minor issues | Iterate | Refine what's working |\n| 50-80% acceptable, structural issues | Targeted regenerate | Keep good parts, redo bad |\n| <50% acceptable | Restart with better prompt | Foundation is wrong |\n| Wrong direction entirely | Restart with new approach | Prompt design failed |\n\n**Effective iteration prompts:**\n\n| Issue | Iteration Approach |\n|-------|-------------------|\n| Too long | \"Condense to X words, preserving [key elements]\" |\n| Missing elements | \"Add [specific element] after [location]\" |\n| Wrong tone | \"Rewrite in [desired tone], maintaining content\" |\n| Factual error | \"[X] is incorrect. The correct information is [Y]. Revise.\" |\n| Structural problem | \"Reorganize to follow [structure]: first X, then Y, then Z\" |\n\n### Building Feedback Loops\n\n**Feedback capture template:**\n\n| Field | Purpose |\n|-------|---------|\n| Prompt used | Reference for refinement |\n| Model/provider | Track performance by model |\n| Output quality (1-5) | Quantitative tracking |\n| Issues found | Specific problems to address |\n| Iterations needed | Efficiency metric |\n| Final outcome | Did it ship? Why/why not? |\n| Improvement notes | What would work better? |\n\n---\n\n## HOW to Apply This\n\n### Exercise: Design Quality Gates\n\n### Quality Metrics to Track\n\n| Metric | What It Measures | Target |\n|--------|------------------|--------|\n| **First-pass rate** | % outputs acceptable without iteration | 70%+ |\n| **Iteration count** | Average revisions needed | <2 |\n| **Human intervention rate** | % requiring human correction | <20% |\n| **Quality score trend** | Rubric scores over time | Improving |\n| **Rejection rate** | % outputs not usable | <5% |\n| **Time to acceptable** | Duration from prompt to final | Decreasing |\n\n### Common Quality Failures and Fixes\n\n| Failure Mode | Detection | Prevention |\n|--------------|-----------|------------|\n| Hallucinated facts | Cross-reference, fact-check prompt | Provide source documents |\n| Incomplete response | Checklist validation | Explicit requirements in prompt |\n| Wrong tone | AI tone check, human spot-check | Clear tone examples in prompt |\n| Inconsistent with previous | Compare against prior outputs | Include relevant context |\n| Template broken | Format validation | Structured output formats |\n\n### Self-Check\n\n---\n\n## Up Next\n\nIn **Module 2.4: Human-AI Handoffs**, you'll learn how to design the boundaries between AI work and human work—the critical transition points where quality and control live.",
      "htmlContent": "<h1>Quality and Iteration</h1>\n<h2>WHY This Matters</h2>\n<p>AI doesn&#39;t know when its output is wrong. It can produce confident, well-formatted nonsense. Without quality controls, you&#39;re gambling with every AI output.</p>\n<p><strong>The quality imperative:</strong></p>\n<ul>\n<li>AI outputs need verification (it doesn&#39;t self-correct)</li>\n<li>Early feedback is cheaper than late discovery</li>\n<li>Iteration is often faster than perfect prompts</li>\n<li>Quality gates protect downstream work</li>\n</ul>\n<p>Professionals don&#39;t ship first drafts. AI operators don&#39;t ship unvalidated outputs.</p>\n<hr>\n<h2>WHAT You Need to Know</h2>\n<h3>Quality Gate Architecture</h3>\n<p><strong>Quality gate types:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Gate Type</th>\n<th>Mechanism</th>\n<th>Use When</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Automated</strong></td>\n<td>Rule-based checks</td>\n<td>Format, length, required elements</td>\n</tr>\n<tr>\n<td><strong>AI-Assisted</strong></td>\n<td>Second AI evaluates first</td>\n<td>Subjective quality, consistency</td>\n</tr>\n<tr>\n<td><strong>Human</strong></td>\n<td>Person reviews</td>\n<td>High stakes, nuanced judgment</td>\n</tr>\n<tr>\n<td><strong>Statistical</strong></td>\n<td>Sampling approach</td>\n<td>High volume, lower stakes</td>\n</tr>\n</tbody></table>\n<h3>Validation Strategies</h3>\n<p><strong>1. Checklist Validation</strong>\nDefine explicit criteria and verify each:</p>\n<pre><code>✓ Contains executive summary\n✓ All sections from template present\n✓ No placeholder text remaining\n✓ Word count within range (800-1200)\n✓ No competitor mentions\n✓ CTA included\n✗ Links verified (needs review)\n</code></pre>\n<p><strong>2. AI Cross-Check</strong>\nUse a second prompt to evaluate the first output:</p>\n<pre><code>Review this [content type] against these criteria:\n1. [Criterion 1]: Pass/Fail + Explanation\n2. [Criterion 2]: Pass/Fail + Explanation\n...\n\nOutput: JSON with pass/fail for each criterion and overall assessment.\n</code></pre>\n<p><strong>3. Rubric Scoring</strong></p>\n<h3>The Iteration Loop</h3>\n<p><strong>When to iterate vs. when to restart:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Situation</th>\n<th>Action</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>80%+ acceptable, minor issues</td>\n<td>Iterate</td>\n<td>Refine what&#39;s working</td>\n</tr>\n<tr>\n<td>50-80% acceptable, structural issues</td>\n<td>Targeted regenerate</td>\n<td>Keep good parts, redo bad</td>\n</tr>\n<tr>\n<td>&lt;50% acceptable</td>\n<td>Restart with better prompt</td>\n<td>Foundation is wrong</td>\n</tr>\n<tr>\n<td>Wrong direction entirely</td>\n<td>Restart with new approach</td>\n<td>Prompt design failed</td>\n</tr>\n</tbody></table>\n<p><strong>Effective iteration prompts:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Issue</th>\n<th>Iteration Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Too long</td>\n<td>&quot;Condense to X words, preserving [key elements]&quot;</td>\n</tr>\n<tr>\n<td>Missing elements</td>\n<td>&quot;Add [specific element] after [location]&quot;</td>\n</tr>\n<tr>\n<td>Wrong tone</td>\n<td>&quot;Rewrite in [desired tone], maintaining content&quot;</td>\n</tr>\n<tr>\n<td>Factual error</td>\n<td>&quot;[X] is incorrect. The correct information is [Y]. Revise.&quot;</td>\n</tr>\n<tr>\n<td>Structural problem</td>\n<td>&quot;Reorganize to follow [structure]: first X, then Y, then Z&quot;</td>\n</tr>\n</tbody></table>\n<h3>Building Feedback Loops</h3>\n<p><strong>Feedback capture template:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Prompt used</td>\n<td>Reference for refinement</td>\n</tr>\n<tr>\n<td>Model/provider</td>\n<td>Track performance by model</td>\n</tr>\n<tr>\n<td>Output quality (1-5)</td>\n<td>Quantitative tracking</td>\n</tr>\n<tr>\n<td>Issues found</td>\n<td>Specific problems to address</td>\n</tr>\n<tr>\n<td>Iterations needed</td>\n<td>Efficiency metric</td>\n</tr>\n<tr>\n<td>Final outcome</td>\n<td>Did it ship? Why/why not?</td>\n</tr>\n<tr>\n<td>Improvement notes</td>\n<td>What would work better?</td>\n</tr>\n</tbody></table>\n<hr>\n<h2>HOW to Apply This</h2>\n<h3>Exercise: Design Quality Gates</h3>\n<h3>Quality Metrics to Track</h3>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>What It Measures</th>\n<th>Target</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>First-pass rate</strong></td>\n<td>% outputs acceptable without iteration</td>\n<td>70%+</td>\n</tr>\n<tr>\n<td><strong>Iteration count</strong></td>\n<td>Average revisions needed</td>\n<td>&lt;2</td>\n</tr>\n<tr>\n<td><strong>Human intervention rate</strong></td>\n<td>% requiring human correction</td>\n<td>&lt;20%</td>\n</tr>\n<tr>\n<td><strong>Quality score trend</strong></td>\n<td>Rubric scores over time</td>\n<td>Improving</td>\n</tr>\n<tr>\n<td><strong>Rejection rate</strong></td>\n<td>% outputs not usable</td>\n<td>&lt;5%</td>\n</tr>\n<tr>\n<td><strong>Time to acceptable</strong></td>\n<td>Duration from prompt to final</td>\n<td>Decreasing</td>\n</tr>\n</tbody></table>\n<h3>Common Quality Failures and Fixes</h3>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection</th>\n<th>Prevention</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hallucinated facts</td>\n<td>Cross-reference, fact-check prompt</td>\n<td>Provide source documents</td>\n</tr>\n<tr>\n<td>Incomplete response</td>\n<td>Checklist validation</td>\n<td>Explicit requirements in prompt</td>\n</tr>\n<tr>\n<td>Wrong tone</td>\n<td>AI tone check, human spot-check</td>\n<td>Clear tone examples in prompt</td>\n</tr>\n<tr>\n<td>Inconsistent with previous</td>\n<td>Compare against prior outputs</td>\n<td>Include relevant context</td>\n</tr>\n<tr>\n<td>Template broken</td>\n<td>Format validation</td>\n<td>Structured output formats</td>\n</tr>\n</tbody></table>\n<h3>Self-Check</h3>\n<hr>\n<h2>Up Next</h2>\n<p>In <strong>Module 2.4: Human-AI Handoffs</strong>, you&#39;ll learn how to design the boundaries between AI work and human work—the critical transition points where quality and control live.</p>\n",
      "sections": [
        {
          "id": "why-this-matters",
          "title": "WHY This Matters",
          "type": "why",
          "content": "AI doesn't know when its output is wrong. It can produce confident, well-formatted nonsense. Without quality controls, you're gambling with every AI output.\n\n**The quality imperative:**\n- AI outputs need verification (it doesn't self-correct)\n- Early feedback is cheaper than late discovery\n- Iteration is often faster than perfect prompts\n- Quality gates protect downstream work\n\nProfessionals don't ship first drafts. AI operators don't ship unvalidated outputs.\n\n---",
          "htmlContent": "<p>AI doesn&#39;t know when its output is wrong. It can produce confident, well-formatted nonsense. Without quality controls, you&#39;re gambling with every AI output.</p>\n<p><strong>The quality imperative:</strong></p>\n<ul>\n<li>AI outputs need verification (it doesn&#39;t self-correct)</li>\n<li>Early feedback is cheaper than late discovery</li>\n<li>Iteration is often faster than perfect prompts</li>\n<li>Quality gates protect downstream work</li>\n</ul>\n<p>Professionals don&#39;t ship first drafts. AI operators don&#39;t ship unvalidated outputs.</p>\n<hr>\n"
        },
        {
          "id": "what-you-need-to-know",
          "title": "WHAT You Need to Know",
          "type": "what",
          "content": "### Quality Gate Architecture\n\n**Quality gate types:**\n\n| Gate Type | Mechanism | Use When |\n|-----------|-----------|----------|\n| **Automated** | Rule-based checks | Format, length, required elements |\n| **AI-Assisted** | Second AI evaluates first | Subjective quality, consistency |\n| **Human** | Person reviews | High stakes, nuanced judgment |\n| **Statistical** | Sampling approach | High volume, lower stakes |\n\n### Validation Strategies\n\n**1. Checklist Validation**\nDefine explicit criteria and verify each:\n\n```\n✓ Contains executive summary\n✓ All sections from template present\n✓ No placeholder text remaining\n✓ Word count within range (800-1200)\n✓ No competitor mentions\n✓ CTA included\n✗ Links verified (needs review)\n```\n\n**2. AI Cross-Check**\nUse a second prompt to evaluate the first output:\n\n```\nReview this [content type] against these criteria:\n1. [Criterion 1]: Pass/Fail + Explanation\n2. [Criterion 2]: Pass/Fail + Explanation\n...\n\nOutput: JSON with pass/fail for each criterion and overall assessment.\n```\n\n**3. Rubric Scoring**\n\n### The Iteration Loop\n\n**When to iterate vs. when to restart:**\n\n| Situation | Action | Rationale |\n|-----------|--------|-----------|\n| 80%+ acceptable, minor issues | Iterate | Refine what's working |\n| 50-80% acceptable, structural issues | Targeted regenerate | Keep good parts, redo bad |\n| <50% acceptable | Restart with better prompt | Foundation is wrong |\n| Wrong direction entirely | Restart with new approach | Prompt design failed |\n\n**Effective iteration prompts:**\n\n| Issue | Iteration Approach |\n|-------|-------------------|\n| Too long | \"Condense to X words, preserving [key elements]\" |\n| Missing elements | \"Add [specific element] after [location]\" |\n| Wrong tone | \"Rewrite in [desired tone], maintaining content\" |\n| Factual error | \"[X] is incorrect. The correct information is [Y]. Revise.\" |\n| Structural problem | \"Reorganize to follow [structure]: first X, then Y, then Z\" |\n\n### Building Feedback Loops\n\n**Feedback capture template:**\n\n| Field | Purpose |\n|-------|---------|\n| Prompt used | Reference for refinement |\n| Model/provider | Track performance by model |\n| Output quality (1-5) | Quantitative tracking |\n| Issues found | Specific problems to address |\n| Iterations needed | Efficiency metric |\n| Final outcome | Did it ship? Why/why not? |\n| Improvement notes | What would work better? |\n\n---",
          "htmlContent": "<h3>Quality Gate Architecture</h3>\n<p><strong>Quality gate types:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Gate Type</th>\n<th>Mechanism</th>\n<th>Use When</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Automated</strong></td>\n<td>Rule-based checks</td>\n<td>Format, length, required elements</td>\n</tr>\n<tr>\n<td><strong>AI-Assisted</strong></td>\n<td>Second AI evaluates first</td>\n<td>Subjective quality, consistency</td>\n</tr>\n<tr>\n<td><strong>Human</strong></td>\n<td>Person reviews</td>\n<td>High stakes, nuanced judgment</td>\n</tr>\n<tr>\n<td><strong>Statistical</strong></td>\n<td>Sampling approach</td>\n<td>High volume, lower stakes</td>\n</tr>\n</tbody></table>\n<h3>Validation Strategies</h3>\n<p><strong>1. Checklist Validation</strong>\nDefine explicit criteria and verify each:</p>\n<pre><code>✓ Contains executive summary\n✓ All sections from template present\n✓ No placeholder text remaining\n✓ Word count within range (800-1200)\n✓ No competitor mentions\n✓ CTA included\n✗ Links verified (needs review)\n</code></pre>\n<p><strong>2. AI Cross-Check</strong>\nUse a second prompt to evaluate the first output:</p>\n<pre><code>Review this [content type] against these criteria:\n1. [Criterion 1]: Pass/Fail + Explanation\n2. [Criterion 2]: Pass/Fail + Explanation\n...\n\nOutput: JSON with pass/fail for each criterion and overall assessment.\n</code></pre>\n<p><strong>3. Rubric Scoring</strong></p>\n<h3>The Iteration Loop</h3>\n<p><strong>When to iterate vs. when to restart:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Situation</th>\n<th>Action</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>80%+ acceptable, minor issues</td>\n<td>Iterate</td>\n<td>Refine what&#39;s working</td>\n</tr>\n<tr>\n<td>50-80% acceptable, structural issues</td>\n<td>Targeted regenerate</td>\n<td>Keep good parts, redo bad</td>\n</tr>\n<tr>\n<td>&lt;50% acceptable</td>\n<td>Restart with better prompt</td>\n<td>Foundation is wrong</td>\n</tr>\n<tr>\n<td>Wrong direction entirely</td>\n<td>Restart with new approach</td>\n<td>Prompt design failed</td>\n</tr>\n</tbody></table>\n<p><strong>Effective iteration prompts:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Issue</th>\n<th>Iteration Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Too long</td>\n<td>&quot;Condense to X words, preserving [key elements]&quot;</td>\n</tr>\n<tr>\n<td>Missing elements</td>\n<td>&quot;Add [specific element] after [location]&quot;</td>\n</tr>\n<tr>\n<td>Wrong tone</td>\n<td>&quot;Rewrite in [desired tone], maintaining content&quot;</td>\n</tr>\n<tr>\n<td>Factual error</td>\n<td>&quot;[X] is incorrect. The correct information is [Y]. Revise.&quot;</td>\n</tr>\n<tr>\n<td>Structural problem</td>\n<td>&quot;Reorganize to follow [structure]: first X, then Y, then Z&quot;</td>\n</tr>\n</tbody></table>\n<h3>Building Feedback Loops</h3>\n<p><strong>Feedback capture template:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Prompt used</td>\n<td>Reference for refinement</td>\n</tr>\n<tr>\n<td>Model/provider</td>\n<td>Track performance by model</td>\n</tr>\n<tr>\n<td>Output quality (1-5)</td>\n<td>Quantitative tracking</td>\n</tr>\n<tr>\n<td>Issues found</td>\n<td>Specific problems to address</td>\n</tr>\n<tr>\n<td>Iterations needed</td>\n<td>Efficiency metric</td>\n</tr>\n<tr>\n<td>Final outcome</td>\n<td>Did it ship? Why/why not?</td>\n</tr>\n<tr>\n<td>Improvement notes</td>\n<td>What would work better?</td>\n</tr>\n</tbody></table>\n<hr>\n"
        },
        {
          "id": "how-to-apply-this",
          "title": "HOW to Apply This",
          "type": "how",
          "content": "### Exercise: Design Quality Gates\n\n### Quality Metrics to Track\n\n| Metric | What It Measures | Target |\n|--------|------------------|--------|\n| **First-pass rate** | % outputs acceptable without iteration | 70%+ |\n| **Iteration count** | Average revisions needed | <2 |\n| **Human intervention rate** | % requiring human correction | <20% |\n| **Quality score trend** | Rubric scores over time | Improving |\n| **Rejection rate** | % outputs not usable | <5% |\n| **Time to acceptable** | Duration from prompt to final | Decreasing |\n\n### Common Quality Failures and Fixes\n\n| Failure Mode | Detection | Prevention |\n|--------------|-----------|------------|\n| Hallucinated facts | Cross-reference, fact-check prompt | Provide source documents |\n| Incomplete response | Checklist validation | Explicit requirements in prompt |\n| Wrong tone | AI tone check, human spot-check | Clear tone examples in prompt |\n| Inconsistent with previous | Compare against prior outputs | Include relevant context |\n| Template broken | Format validation | Structured output formats |\n\n### Self-Check\n\n---",
          "htmlContent": "<h3>Exercise: Design Quality Gates</h3>\n<h3>Quality Metrics to Track</h3>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>What It Measures</th>\n<th>Target</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>First-pass rate</strong></td>\n<td>% outputs acceptable without iteration</td>\n<td>70%+</td>\n</tr>\n<tr>\n<td><strong>Iteration count</strong></td>\n<td>Average revisions needed</td>\n<td>&lt;2</td>\n</tr>\n<tr>\n<td><strong>Human intervention rate</strong></td>\n<td>% requiring human correction</td>\n<td>&lt;20%</td>\n</tr>\n<tr>\n<td><strong>Quality score trend</strong></td>\n<td>Rubric scores over time</td>\n<td>Improving</td>\n</tr>\n<tr>\n<td><strong>Rejection rate</strong></td>\n<td>% outputs not usable</td>\n<td>&lt;5%</td>\n</tr>\n<tr>\n<td><strong>Time to acceptable</strong></td>\n<td>Duration from prompt to final</td>\n<td>Decreasing</td>\n</tr>\n</tbody></table>\n<h3>Common Quality Failures and Fixes</h3>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection</th>\n<th>Prevention</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hallucinated facts</td>\n<td>Cross-reference, fact-check prompt</td>\n<td>Provide source documents</td>\n</tr>\n<tr>\n<td>Incomplete response</td>\n<td>Checklist validation</td>\n<td>Explicit requirements in prompt</td>\n</tr>\n<tr>\n<td>Wrong tone</td>\n<td>AI tone check, human spot-check</td>\n<td>Clear tone examples in prompt</td>\n</tr>\n<tr>\n<td>Inconsistent with previous</td>\n<td>Compare against prior outputs</td>\n<td>Include relevant context</td>\n</tr>\n<tr>\n<td>Template broken</td>\n<td>Format validation</td>\n<td>Structured output formats</td>\n</tr>\n</tbody></table>\n<h3>Self-Check</h3>\n<hr>\n"
        },
        {
          "id": "up-next",
          "title": "Up Next",
          "type": "generic",
          "content": "In **Module 2.4: Human-AI Handoffs**, you'll learn how to design the boundaries between AI work and human work—the critical transition points where quality and control live.",
          "htmlContent": "<p>In <strong>Module 2.4: Human-AI Handoffs</strong>, you&#39;ll learn how to design the boundaries between AI work and human work—the critical transition points where quality and control live.</p>\n"
        }
      ],
      "concepts": [
        {
          "id": "quality-gates",
          "term": "quality gates",
          "definition": "**Quality gates** are checkpoints where output must meet specific criteria before proceeding. For AI workflows, gates typically check:\n\n- **Completeness**: Did the AI address all requirements?\n- **Accuracy**: Are facts and figures correct?\n- **Format**: Does output match expected structure?\n- **Tone**: Is the voice appropriate?\n- **Constraints**: Were restrictions followed?",
          "htmlDefinition": "<p><strong>Quality gates</strong> are checkpoints where output must meet specific criteria before proceeding. For AI workflows, gates typically check:</p>\n<ul>\n<li><strong>Completeness</strong>: Did the AI address all requirements?</li>\n<li><strong>Accuracy</strong>: Are facts and figures correct?</li>\n<li><strong>Format</strong>: Does output match expected structure?</li>\n<li><strong>Tone</strong>: Is the voice appropriate?</li>\n<li><strong>Constraints</strong>: Were restrictions followed?</li>\n</ul>\n"
        },
        {
          "id": "rubric-scoring",
          "term": "rubric scoring",
          "definition": "**Rubric scoring** uses defined quality levels (not just pass/fail) to assess output. This enables nuanced evaluation and tracking quality over time.\n\nExample rubric dimensions:\n- **Accuracy**: 1 (errors) → 5 (verified correct)\n- **Completeness**: 1 (missing major elements) → 5 (comprehensive)\n- **Clarity**: 1 (confusing) → 5 (crystal clear)\n- **Tone**: 1 (inappropriate) → 5 (perfectly matched)",
          "htmlDefinition": "<p><strong>Rubric scoring</strong> uses defined quality levels (not just pass/fail) to assess output. This enables nuanced evaluation and tracking quality over time.</p>\n<p>Example rubric dimensions:</p>\n<ul>\n<li><strong>Accuracy</strong>: 1 (errors) → 5 (verified correct)</li>\n<li><strong>Completeness</strong>: 1 (missing major elements) → 5 (comprehensive)</li>\n<li><strong>Clarity</strong>: 1 (confusing) → 5 (crystal clear)</li>\n<li><strong>Tone</strong>: 1 (inappropriate) → 5 (perfectly matched)</li>\n</ul>\n"
        },
        {
          "id": "feedback-loop",
          "term": "feedback loop",
          "definition": "A **feedback loop** captures information about output quality and uses it to improve future performance. In AI workflows:\n\n- **Input**: What was the prompt?\n- **Output**: What did AI produce?\n- **Evaluation**: How good was it? (score, pass/fail)\n- **Learning**: What can improve? (prompt refinement, examples)",
          "htmlDefinition": "<p>A <strong>feedback loop</strong> captures information about output quality and uses it to improve future performance. In AI workflows:</p>\n<ul>\n<li><strong>Input</strong>: What was the prompt?</li>\n<li><strong>Output</strong>: What did AI produce?</li>\n<li><strong>Evaluation</strong>: How good was it? (score, pass/fail)</li>\n<li><strong>Learning</strong>: What can improve? (prompt refinement, examples)</li>\n</ul>\n"
        }
      ],
      "exercises": [
        {
          "id": "quality-gate-design",
          "title": "Scenario",
          "instructions": "You're building an AI workflow to generate weekly customer intelligence reports. The reports:\n- Summarize key customer feedback themes\n- Highlight urgent issues requiring attention\n- Include sentiment trends\n- Are read by the executive team\n\n**Design the quality system:**\n\n1. **Define 5+ specific quality criteria** (what makes a good report?)\n\n2. **Create a rubric** with 1-5 scoring for each criterion\n\n3. **Design validation checkpoints:**\n   - What can be checked automatically?\n   - What needs AI cross-check?\n   - What requires human review?\n\n4. **Plan the iteration loop:**\n   - What issues would trigger iteration?\n   - What would trigger full restart?\n   - How many iterations maximum before human escalation?\n\n5. **Design feedback capture:**\n   - What would you log for each report?\n   - How would you use this data to improve?",
          "htmlInstructions": "<p>You&#39;re building an AI workflow to generate weekly customer intelligence reports. The reports:</p>\n<ul>\n<li>Summarize key customer feedback themes</li>\n<li>Highlight urgent issues requiring attention</li>\n<li>Include sentiment trends</li>\n<li>Are read by the executive team</li>\n</ul>\n<p><strong>Design the quality system:</strong></p>\n<ol>\n<li><p><strong>Define 5+ specific quality criteria</strong> (what makes a good report?)</p>\n</li>\n<li><p><strong>Create a rubric</strong> with 1-5 scoring for each criterion</p>\n</li>\n<li><p><strong>Design validation checkpoints:</strong></p>\n<ul>\n<li>What can be checked automatically?</li>\n<li>What needs AI cross-check?</li>\n<li>What requires human review?</li>\n</ul>\n</li>\n<li><p><strong>Plan the iteration loop:</strong></p>\n<ul>\n<li>What issues would trigger iteration?</li>\n<li>What would trigger full restart?</li>\n<li>How many iterations maximum before human escalation?</li>\n</ul>\n</li>\n<li><p><strong>Design feedback capture:</strong></p>\n<ul>\n<li>What would you log for each report?</li>\n<li>How would you use this data to improve?</li>\n</ul>\n</li>\n</ol>\n"
        }
      ],
      "checklists": [
        {
          "id": "module-2.3-complete",
          "items": [
            {
              "id": "module-2.3-complete-0",
              "text": "I can design quality gates for AI workflows",
              "completed": false
            },
            {
              "id": "module-2.3-complete-1",
              "text": "I understand different validation strategies",
              "completed": false
            },
            {
              "id": "module-2.3-complete-2",
              "text": "I know when to iterate vs. restart",
              "completed": false
            },
            {
              "id": "module-2.3-complete-3",
              "text": "I can create quality rubrics",
              "completed": false
            },
            {
              "id": "module-2.3-complete-4",
              "text": "I understand how to capture feedback for improvement",
              "completed": false
            }
          ]
        }
      ]
    },
    {
      "id": "2.4-human-ai-handoffs",
      "slug": "2.4-human-ai-handoffs",
      "title": "Human-AI Handoffs",
      "phase": 2,
      "module": 4,
      "phaseId": "phase-2",
      "estimatedMinutes": 14,
      "bloomLevel": "apply",
      "content": "# Human-AI Handoffs\n\n## WHY This Matters\n\nThe most dangerous AI workflows are the ones where humans are nominally \"in the loop\" but have no practical ability to intervene meaningfully. Effective human-AI collaboration requires:\n\n- **Clear boundaries** between AI and human responsibility\n- **Meaningful checkpoints** where humans can actually add value\n- **Escalation paths** when AI reaches its limits\n- **Context preservation** so humans understand what AI did\n\nGet handoffs wrong, and you get rubber-stamp humans or chaos when things go wrong.\n\n---\n\n## WHAT You Need to Know\n\n### Handoff Design Principles\n\n**Handoff types:**\n\n| Type | Direction | Example |\n|------|-----------|---------|\n| **Initiation** | Human → AI | User submits task to AI system |\n| **Checkpoint** | AI → Human → AI | AI pauses for approval, then continues |\n| **Completion** | AI → Human | AI delivers output for human use |\n| **Escalation** | AI → Human | AI can't complete, needs human help |\n| **Override** | Human → AI | Human intervenes mid-process |\n\n### The Context Package\n\nWhen AI hands off to human, include:\n\n```\nCONTEXT PACKAGE\n├── What was requested (original input)\n├── What AI did (steps taken)\n├── What AI produced (output/draft)\n├── Confidence level (AI's self-assessment)\n├── Flagged concerns (anything uncertain)\n├── Options if applicable (alternatives considered)\n└── What human needs to do (clear next action)\n```\n\n**Example handoff message:**\n\n> **Task**: Draft response to customer complaint #4521\n>\n> **Summary**: Customer upset about delayed shipment. Requested refund.\n>\n> **Draft response**: [AI-generated response]\n>\n> **Confidence**: Medium (85%)\n>\n> **Flags**:\n> - Customer mentions legal action (escalation policy may apply)\n> - Account shows 3 prior complaints (retention risk)\n>\n> **Your action**: Review draft and decide on compensation offer\n\n### Escalation Path Design\n\n**Escalation matrix:**\n\n| Trigger | Response | Human Action |\n|---------|----------|--------------|\n| Low confidence (<70%) | Flag for review | Verify before sending |\n| Policy keyword detected | Hard stop | Human must decide |\n| Multiple valid options | Present choices | Human selects |\n| Task failure | Report with diagnosis | Human troubleshoots |\n| High-value customer | Require approval | Human approves |\n\n### Human Oversight Patterns\n\n**Pattern 1: Pre-flight approval**\n```\nHuman defines task → AI executes → Human uses output\n```\nBest for: Routine tasks, experienced operators\n\n**Pattern 2: Checkpoint approval**\n```\nAI proposes → Human approves → AI executes\n```\nBest for: Consequential actions, early adoption\n\n**Pattern 3: Post-hoc review**\n```\nAI executes → Human reviews sample → Feedback loop\n```\nBest for: High-volume, lower-stakes\n\n**Pattern 4: Exception-based**\n```\nAI executes normally → Escalates exceptions → Human handles edge cases\n```\nBest for: Mature workflows, clear rules\n\n---\n\n### Production HITL Patterns\n\nTheory is useful. But production systems reveal specific patterns that work at scale.\n\n**Pattern A: Approval Gates**\n\nUse when: Before irreversible actions (delete, send, publish, charge)\n\n```\nAI completes task → PAUSE → Human reviews → Approve/Edit/Reject → Continue or stop\n```\n\nImplementation details:\n- Set timeout with default action (approve or reject if no response)\n- Allow inline editing of AI output before approval\n- Track approval rates to identify automation candidates\n\n**Pattern B: Confidence-Based Routing**\n\nTuning guidance:\n- Start with low threshold (high human involvement)\n- Gradually raise as you verify AI quality\n- Monitor for \"confident but wrong\" failures\n\n**Pattern C: Escalation Ladders**\n\nUse when: Graduated authority levels\n\n```\nAI agent → L1 Support → L2 Specialist → Manager → Executive\n```\n\nEach level has:\n- Defined scope of authority\n- Clear handoff criteria\n- Required context package\n- Time-to-response expectations\n\n**Pattern D: Async Feedback Loops**\n\nUse when: Quality matters but latency doesn't\n\n```\nAI completes → Delivers to user → Human reviews later → Corrections feed back to training\n```\n\nKey benefits:\n- No workflow blocking\n- Batch review efficiency\n- Continuous improvement signal\n- No single point of failure\n\n**Pattern E: Audit Trails**\n\nUse when: Compliance/traceability without blocking\n\n```\nAI acts → Logs action + reasoning → Continues → Periodic human audit\n```\n\nLog structure:\n- WHO: User/system that triggered action\n- WHAT: Exact action taken\n- WHEN: Timestamp\n- WHY: AI's reasoning/confidence\n- OUTCOME: Result of action\n\n### HITL Production Considerations\n\n| Factor | Impact | Mitigation |\n|--------|--------|------------|\n| **Latency** | 0.5-2s per human decision | Use async patterns, batch reviews |\n| **Bottlenecks** | Human capacity limits scale | Confidence routing, clear thresholds |\n| **Quality variance** | Reviewers have different standards | Training, calibration sessions, rubrics |\n| **Fatigue** | Alert fatigue reduces effectiveness | Prioritize escalations, rotate reviewers |\n| **Audit compliance** | Required in regulated industries | Structured logging from day one |\n\n### Regulatory Note: EU AI Act\n\nArticle 14 requires \"effective oversight by natural persons\" for high-risk AI systems.\n\n**HITL isn't optional for:**\n- Hiring and recruitment tools\n- Credit scoring and lending\n- Legal and judicial AI\n- Medical diagnosis assistance\n- Critical infrastructure control\n\nIf you're building in these domains, HITL must be architected from the start—not bolted on later.\n\n### Designing Meaningful Checkpoints\n\n**Bad checkpoint** (rubber stamp):\n- Human sees: \"AI did something. OK?\"\n- Human can: Click \"approve\" or \"reject\"\n- Human knows: Nothing useful\n- Result: Automatic approval, no real oversight\n\n**Good checkpoint**:\n- Human sees: Context, draft, confidence, concerns\n- Human can: Approve, edit, reject, escalate, redirect\n- Human knows: What AI did, why, and risk areas\n- Result: Genuine review, meaningful intervention\n\n---\n\n## HOW to Apply This\n\n### Exercise: Design a Handoff System\n\n### Handoff Anti-Patterns\n\n| Anti-Pattern | Problem | Solution |\n|--------------|---------|----------|\n| **Rubber stamp** | Human approves without review | Make review necessary and fast |\n| **Context loss** | Human doesn't know what AI did | Include comprehensive context |\n| **Hidden escalation** | AI decides when to escalate | Clear, auditable rules |\n| **Escalation overload** | Too many escalations | Tune triggers, improve AI |\n| **No return path** | Human can't redirect AI | Enable mid-workflow intervention |\n| **Blame ambiguity** | Unclear who's responsible | Explicit ownership at each stage |\n\n### Checklist for Handoff Design\n\n| Element | Question |\n|---------|----------|\n| **Triggers** | When does handoff occur? |\n| **Context** | What information transfers? |\n| **Actions** | What can the recipient do? |\n| **Responsibility** | Who owns the outcome? |\n| **Escalation** | What if recipient can't handle it? |\n| **Logging** | Is the handoff recorded? |\n| **Recovery** | What if handoff fails? |\n\n### Self-Check\n\n---\n\n## Phase 2 Complete!\n\nYou've mastered Workflow Engineering fundamentals. Before moving to Phase 3, complete:\n\n**Lab 3: Workflow Mapping** — Document a real workflow and identify AI integration opportunities\n\n**Lab 4: Quality Gate Design** — Create a quality assurance system for an AI workflow\n\n**Phase 2 Deliverable: Workflow Automation Proposal** — Document a real workflow with AI integration opportunities, including ROI analysis",
      "htmlContent": "<h1>Human-AI Handoffs</h1>\n<h2>WHY This Matters</h2>\n<p>The most dangerous AI workflows are the ones where humans are nominally &quot;in the loop&quot; but have no practical ability to intervene meaningfully. Effective human-AI collaboration requires:</p>\n<ul>\n<li><strong>Clear boundaries</strong> between AI and human responsibility</li>\n<li><strong>Meaningful checkpoints</strong> where humans can actually add value</li>\n<li><strong>Escalation paths</strong> when AI reaches its limits</li>\n<li><strong>Context preservation</strong> so humans understand what AI did</li>\n</ul>\n<p>Get handoffs wrong, and you get rubber-stamp humans or chaos when things go wrong.</p>\n<hr>\n<h2>WHAT You Need to Know</h2>\n<h3>Handoff Design Principles</h3>\n<p><strong>Handoff types:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n<th>Direction</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Initiation</strong></td>\n<td>Human → AI</td>\n<td>User submits task to AI system</td>\n</tr>\n<tr>\n<td><strong>Checkpoint</strong></td>\n<td>AI → Human → AI</td>\n<td>AI pauses for approval, then continues</td>\n</tr>\n<tr>\n<td><strong>Completion</strong></td>\n<td>AI → Human</td>\n<td>AI delivers output for human use</td>\n</tr>\n<tr>\n<td><strong>Escalation</strong></td>\n<td>AI → Human</td>\n<td>AI can&#39;t complete, needs human help</td>\n</tr>\n<tr>\n<td><strong>Override</strong></td>\n<td>Human → AI</td>\n<td>Human intervenes mid-process</td>\n</tr>\n</tbody></table>\n<h3>The Context Package</h3>\n<p>When AI hands off to human, include:</p>\n<pre><code>CONTEXT PACKAGE\n├── What was requested (original input)\n├── What AI did (steps taken)\n├── What AI produced (output/draft)\n├── Confidence level (AI&#39;s self-assessment)\n├── Flagged concerns (anything uncertain)\n├── Options if applicable (alternatives considered)\n└── What human needs to do (clear next action)\n</code></pre>\n<p><strong>Example handoff message:</strong></p>\n<blockquote>\n<p><strong>Task</strong>: Draft response to customer complaint #4521</p>\n<p><strong>Summary</strong>: Customer upset about delayed shipment. Requested refund.</p>\n<p><strong>Draft response</strong>: [AI-generated response]</p>\n<p><strong>Confidence</strong>: Medium (85%)</p>\n<p><strong>Flags</strong>:</p>\n<ul>\n<li>Customer mentions legal action (escalation policy may apply)</li>\n<li>Account shows 3 prior complaints (retention risk)</li>\n</ul>\n<p><strong>Your action</strong>: Review draft and decide on compensation offer</p>\n</blockquote>\n<h3>Escalation Path Design</h3>\n<p><strong>Escalation matrix:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Trigger</th>\n<th>Response</th>\n<th>Human Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Low confidence (&lt;70%)</td>\n<td>Flag for review</td>\n<td>Verify before sending</td>\n</tr>\n<tr>\n<td>Policy keyword detected</td>\n<td>Hard stop</td>\n<td>Human must decide</td>\n</tr>\n<tr>\n<td>Multiple valid options</td>\n<td>Present choices</td>\n<td>Human selects</td>\n</tr>\n<tr>\n<td>Task failure</td>\n<td>Report with diagnosis</td>\n<td>Human troubleshoots</td>\n</tr>\n<tr>\n<td>High-value customer</td>\n<td>Require approval</td>\n<td>Human approves</td>\n</tr>\n</tbody></table>\n<h3>Human Oversight Patterns</h3>\n<p><strong>Pattern 1: Pre-flight approval</strong></p>\n<pre><code>Human defines task → AI executes → Human uses output\n</code></pre>\n<p>Best for: Routine tasks, experienced operators</p>\n<p><strong>Pattern 2: Checkpoint approval</strong></p>\n<pre><code>AI proposes → Human approves → AI executes\n</code></pre>\n<p>Best for: Consequential actions, early adoption</p>\n<p><strong>Pattern 3: Post-hoc review</strong></p>\n<pre><code>AI executes → Human reviews sample → Feedback loop\n</code></pre>\n<p>Best for: High-volume, lower-stakes</p>\n<p><strong>Pattern 4: Exception-based</strong></p>\n<pre><code>AI executes normally → Escalates exceptions → Human handles edge cases\n</code></pre>\n<p>Best for: Mature workflows, clear rules</p>\n<hr>\n<h3>Production HITL Patterns</h3>\n<p>Theory is useful. But production systems reveal specific patterns that work at scale.</p>\n<p><strong>Pattern A: Approval Gates</strong></p>\n<p>Use when: Before irreversible actions (delete, send, publish, charge)</p>\n<pre><code>AI completes task → PAUSE → Human reviews → Approve/Edit/Reject → Continue or stop\n</code></pre>\n<p>Implementation details:</p>\n<ul>\n<li>Set timeout with default action (approve or reject if no response)</li>\n<li>Allow inline editing of AI output before approval</li>\n<li>Track approval rates to identify automation candidates</li>\n</ul>\n<p><strong>Pattern B: Confidence-Based Routing</strong></p>\n<p>Tuning guidance:</p>\n<ul>\n<li>Start with low threshold (high human involvement)</li>\n<li>Gradually raise as you verify AI quality</li>\n<li>Monitor for &quot;confident but wrong&quot; failures</li>\n</ul>\n<p><strong>Pattern C: Escalation Ladders</strong></p>\n<p>Use when: Graduated authority levels</p>\n<pre><code>AI agent → L1 Support → L2 Specialist → Manager → Executive\n</code></pre>\n<p>Each level has:</p>\n<ul>\n<li>Defined scope of authority</li>\n<li>Clear handoff criteria</li>\n<li>Required context package</li>\n<li>Time-to-response expectations</li>\n</ul>\n<p><strong>Pattern D: Async Feedback Loops</strong></p>\n<p>Use when: Quality matters but latency doesn&#39;t</p>\n<pre><code>AI completes → Delivers to user → Human reviews later → Corrections feed back to training\n</code></pre>\n<p>Key benefits:</p>\n<ul>\n<li>No workflow blocking</li>\n<li>Batch review efficiency</li>\n<li>Continuous improvement signal</li>\n<li>No single point of failure</li>\n</ul>\n<p><strong>Pattern E: Audit Trails</strong></p>\n<p>Use when: Compliance/traceability without blocking</p>\n<pre><code>AI acts → Logs action + reasoning → Continues → Periodic human audit\n</code></pre>\n<p>Log structure:</p>\n<ul>\n<li>WHO: User/system that triggered action</li>\n<li>WHAT: Exact action taken</li>\n<li>WHEN: Timestamp</li>\n<li>WHY: AI&#39;s reasoning/confidence</li>\n<li>OUTCOME: Result of action</li>\n</ul>\n<h3>HITL Production Considerations</h3>\n<table>\n<thead>\n<tr>\n<th>Factor</th>\n<th>Impact</th>\n<th>Mitigation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Latency</strong></td>\n<td>0.5-2s per human decision</td>\n<td>Use async patterns, batch reviews</td>\n</tr>\n<tr>\n<td><strong>Bottlenecks</strong></td>\n<td>Human capacity limits scale</td>\n<td>Confidence routing, clear thresholds</td>\n</tr>\n<tr>\n<td><strong>Quality variance</strong></td>\n<td>Reviewers have different standards</td>\n<td>Training, calibration sessions, rubrics</td>\n</tr>\n<tr>\n<td><strong>Fatigue</strong></td>\n<td>Alert fatigue reduces effectiveness</td>\n<td>Prioritize escalations, rotate reviewers</td>\n</tr>\n<tr>\n<td><strong>Audit compliance</strong></td>\n<td>Required in regulated industries</td>\n<td>Structured logging from day one</td>\n</tr>\n</tbody></table>\n<h3>Regulatory Note: EU AI Act</h3>\n<p>Article 14 requires &quot;effective oversight by natural persons&quot; for high-risk AI systems.</p>\n<p><strong>HITL isn&#39;t optional for:</strong></p>\n<ul>\n<li>Hiring and recruitment tools</li>\n<li>Credit scoring and lending</li>\n<li>Legal and judicial AI</li>\n<li>Medical diagnosis assistance</li>\n<li>Critical infrastructure control</li>\n</ul>\n<p>If you&#39;re building in these domains, HITL must be architected from the start—not bolted on later.</p>\n<h3>Designing Meaningful Checkpoints</h3>\n<p><strong>Bad checkpoint</strong> (rubber stamp):</p>\n<ul>\n<li>Human sees: &quot;AI did something. OK?&quot;</li>\n<li>Human can: Click &quot;approve&quot; or &quot;reject&quot;</li>\n<li>Human knows: Nothing useful</li>\n<li>Result: Automatic approval, no real oversight</li>\n</ul>\n<p><strong>Good checkpoint</strong>:</p>\n<ul>\n<li>Human sees: Context, draft, confidence, concerns</li>\n<li>Human can: Approve, edit, reject, escalate, redirect</li>\n<li>Human knows: What AI did, why, and risk areas</li>\n<li>Result: Genuine review, meaningful intervention</li>\n</ul>\n<hr>\n<h2>HOW to Apply This</h2>\n<h3>Exercise: Design a Handoff System</h3>\n<h3>Handoff Anti-Patterns</h3>\n<table>\n<thead>\n<tr>\n<th>Anti-Pattern</th>\n<th>Problem</th>\n<th>Solution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Rubber stamp</strong></td>\n<td>Human approves without review</td>\n<td>Make review necessary and fast</td>\n</tr>\n<tr>\n<td><strong>Context loss</strong></td>\n<td>Human doesn&#39;t know what AI did</td>\n<td>Include comprehensive context</td>\n</tr>\n<tr>\n<td><strong>Hidden escalation</strong></td>\n<td>AI decides when to escalate</td>\n<td>Clear, auditable rules</td>\n</tr>\n<tr>\n<td><strong>Escalation overload</strong></td>\n<td>Too many escalations</td>\n<td>Tune triggers, improve AI</td>\n</tr>\n<tr>\n<td><strong>No return path</strong></td>\n<td>Human can&#39;t redirect AI</td>\n<td>Enable mid-workflow intervention</td>\n</tr>\n<tr>\n<td><strong>Blame ambiguity</strong></td>\n<td>Unclear who&#39;s responsible</td>\n<td>Explicit ownership at each stage</td>\n</tr>\n</tbody></table>\n<h3>Checklist for Handoff Design</h3>\n<table>\n<thead>\n<tr>\n<th>Element</th>\n<th>Question</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Triggers</strong></td>\n<td>When does handoff occur?</td>\n</tr>\n<tr>\n<td><strong>Context</strong></td>\n<td>What information transfers?</td>\n</tr>\n<tr>\n<td><strong>Actions</strong></td>\n<td>What can the recipient do?</td>\n</tr>\n<tr>\n<td><strong>Responsibility</strong></td>\n<td>Who owns the outcome?</td>\n</tr>\n<tr>\n<td><strong>Escalation</strong></td>\n<td>What if recipient can&#39;t handle it?</td>\n</tr>\n<tr>\n<td><strong>Logging</strong></td>\n<td>Is the handoff recorded?</td>\n</tr>\n<tr>\n<td><strong>Recovery</strong></td>\n<td>What if handoff fails?</td>\n</tr>\n</tbody></table>\n<h3>Self-Check</h3>\n<hr>\n<h2>Phase 2 Complete!</h2>\n<p>You&#39;ve mastered Workflow Engineering fundamentals. Before moving to Phase 3, complete:</p>\n<p><strong>Lab 3: Workflow Mapping</strong> — Document a real workflow and identify AI integration opportunities</p>\n<p><strong>Lab 4: Quality Gate Design</strong> — Create a quality assurance system for an AI workflow</p>\n<p><strong>Phase 2 Deliverable: Workflow Automation Proposal</strong> — Document a real workflow with AI integration opportunities, including ROI analysis</p>\n",
      "sections": [
        {
          "id": "why-this-matters",
          "title": "WHY This Matters",
          "type": "why",
          "content": "The most dangerous AI workflows are the ones where humans are nominally \"in the loop\" but have no practical ability to intervene meaningfully. Effective human-AI collaboration requires:\n\n- **Clear boundaries** between AI and human responsibility\n- **Meaningful checkpoints** where humans can actually add value\n- **Escalation paths** when AI reaches its limits\n- **Context preservation** so humans understand what AI did\n\nGet handoffs wrong, and you get rubber-stamp humans or chaos when things go wrong.\n\n---",
          "htmlContent": "<p>The most dangerous AI workflows are the ones where humans are nominally &quot;in the loop&quot; but have no practical ability to intervene meaningfully. Effective human-AI collaboration requires:</p>\n<ul>\n<li><strong>Clear boundaries</strong> between AI and human responsibility</li>\n<li><strong>Meaningful checkpoints</strong> where humans can actually add value</li>\n<li><strong>Escalation paths</strong> when AI reaches its limits</li>\n<li><strong>Context preservation</strong> so humans understand what AI did</li>\n</ul>\n<p>Get handoffs wrong, and you get rubber-stamp humans or chaos when things go wrong.</p>\n<hr>\n"
        },
        {
          "id": "what-you-need-to-know",
          "title": "WHAT You Need to Know",
          "type": "what",
          "content": "### Handoff Design Principles\n\n**Handoff types:**\n\n| Type | Direction | Example |\n|------|-----------|---------|\n| **Initiation** | Human → AI | User submits task to AI system |\n| **Checkpoint** | AI → Human → AI | AI pauses for approval, then continues |\n| **Completion** | AI → Human | AI delivers output for human use |\n| **Escalation** | AI → Human | AI can't complete, needs human help |\n| **Override** | Human → AI | Human intervenes mid-process |\n\n### The Context Package\n\nWhen AI hands off to human, include:\n\n```\nCONTEXT PACKAGE\n├── What was requested (original input)\n├── What AI did (steps taken)\n├── What AI produced (output/draft)\n├── Confidence level (AI's self-assessment)\n├── Flagged concerns (anything uncertain)\n├── Options if applicable (alternatives considered)\n└── What human needs to do (clear next action)\n```\n\n**Example handoff message:**\n\n> **Task**: Draft response to customer complaint #4521\n>\n> **Summary**: Customer upset about delayed shipment. Requested refund.\n>\n> **Draft response**: [AI-generated response]\n>\n> **Confidence**: Medium (85%)\n>\n> **Flags**:\n> - Customer mentions legal action (escalation policy may apply)\n> - Account shows 3 prior complaints (retention risk)\n>\n> **Your action**: Review draft and decide on compensation offer\n\n### Escalation Path Design\n\n**Escalation matrix:**\n\n| Trigger | Response | Human Action |\n|---------|----------|--------------|\n| Low confidence (<70%) | Flag for review | Verify before sending |\n| Policy keyword detected | Hard stop | Human must decide |\n| Multiple valid options | Present choices | Human selects |\n| Task failure | Report with diagnosis | Human troubleshoots |\n| High-value customer | Require approval | Human approves |\n\n### Human Oversight Patterns\n\n**Pattern 1: Pre-flight approval**\n```\nHuman defines task → AI executes → Human uses output\n```\nBest for: Routine tasks, experienced operators\n\n**Pattern 2: Checkpoint approval**\n```\nAI proposes → Human approves → AI executes\n```\nBest for: Consequential actions, early adoption\n\n**Pattern 3: Post-hoc review**\n```\nAI executes → Human reviews sample → Feedback loop\n```\nBest for: High-volume, lower-stakes\n\n**Pattern 4: Exception-based**\n```\nAI executes normally → Escalates exceptions → Human handles edge cases\n```\nBest for: Mature workflows, clear rules\n\n---\n\n### Production HITL Patterns\n\nTheory is useful. But production systems reveal specific patterns that work at scale.\n\n**Pattern A: Approval Gates**\n\nUse when: Before irreversible actions (delete, send, publish, charge)\n\n```\nAI completes task → PAUSE → Human reviews → Approve/Edit/Reject → Continue or stop\n```\n\nImplementation details:\n- Set timeout with default action (approve or reject if no response)\n- Allow inline editing of AI output before approval\n- Track approval rates to identify automation candidates\n\n**Pattern B: Confidence-Based Routing**\n\nTuning guidance:\n- Start with low threshold (high human involvement)\n- Gradually raise as you verify AI quality\n- Monitor for \"confident but wrong\" failures\n\n**Pattern C: Escalation Ladders**\n\nUse when: Graduated authority levels\n\n```\nAI agent → L1 Support → L2 Specialist → Manager → Executive\n```\n\nEach level has:\n- Defined scope of authority\n- Clear handoff criteria\n- Required context package\n- Time-to-response expectations\n\n**Pattern D: Async Feedback Loops**\n\nUse when: Quality matters but latency doesn't\n\n```\nAI completes → Delivers to user → Human reviews later → Corrections feed back to training\n```\n\nKey benefits:\n- No workflow blocking\n- Batch review efficiency\n- Continuous improvement signal\n- No single point of failure\n\n**Pattern E: Audit Trails**\n\nUse when: Compliance/traceability without blocking\n\n```\nAI acts → Logs action + reasoning → Continues → Periodic human audit\n```\n\nLog structure:\n- WHO: User/system that triggered action\n- WHAT: Exact action taken\n- WHEN: Timestamp\n- WHY: AI's reasoning/confidence\n- OUTCOME: Result of action\n\n### HITL Production Considerations\n\n| Factor | Impact | Mitigation |\n|--------|--------|------------|\n| **Latency** | 0.5-2s per human decision | Use async patterns, batch reviews |\n| **Bottlenecks** | Human capacity limits scale | Confidence routing, clear thresholds |\n| **Quality variance** | Reviewers have different standards | Training, calibration sessions, rubrics |\n| **Fatigue** | Alert fatigue reduces effectiveness | Prioritize escalations, rotate reviewers |\n| **Audit compliance** | Required in regulated industries | Structured logging from day one |\n\n### Regulatory Note: EU AI Act\n\nArticle 14 requires \"effective oversight by natural persons\" for high-risk AI systems.\n\n**HITL isn't optional for:**\n- Hiring and recruitment tools\n- Credit scoring and lending\n- Legal and judicial AI\n- Medical diagnosis assistance\n- Critical infrastructure control\n\nIf you're building in these domains, HITL must be architected from the start—not bolted on later.\n\n### Designing Meaningful Checkpoints\n\n**Bad checkpoint** (rubber stamp):\n- Human sees: \"AI did something. OK?\"\n- Human can: Click \"approve\" or \"reject\"\n- Human knows: Nothing useful\n- Result: Automatic approval, no real oversight\n\n**Good checkpoint**:\n- Human sees: Context, draft, confidence, concerns\n- Human can: Approve, edit, reject, escalate, redirect\n- Human knows: What AI did, why, and risk areas\n- Result: Genuine review, meaningful intervention\n\n---",
          "htmlContent": "<h3>Handoff Design Principles</h3>\n<p><strong>Handoff types:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n<th>Direction</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Initiation</strong></td>\n<td>Human → AI</td>\n<td>User submits task to AI system</td>\n</tr>\n<tr>\n<td><strong>Checkpoint</strong></td>\n<td>AI → Human → AI</td>\n<td>AI pauses for approval, then continues</td>\n</tr>\n<tr>\n<td><strong>Completion</strong></td>\n<td>AI → Human</td>\n<td>AI delivers output for human use</td>\n</tr>\n<tr>\n<td><strong>Escalation</strong></td>\n<td>AI → Human</td>\n<td>AI can&#39;t complete, needs human help</td>\n</tr>\n<tr>\n<td><strong>Override</strong></td>\n<td>Human → AI</td>\n<td>Human intervenes mid-process</td>\n</tr>\n</tbody></table>\n<h3>The Context Package</h3>\n<p>When AI hands off to human, include:</p>\n<pre><code>CONTEXT PACKAGE\n├── What was requested (original input)\n├── What AI did (steps taken)\n├── What AI produced (output/draft)\n├── Confidence level (AI&#39;s self-assessment)\n├── Flagged concerns (anything uncertain)\n├── Options if applicable (alternatives considered)\n└── What human needs to do (clear next action)\n</code></pre>\n<p><strong>Example handoff message:</strong></p>\n<blockquote>\n<p><strong>Task</strong>: Draft response to customer complaint #4521</p>\n<p><strong>Summary</strong>: Customer upset about delayed shipment. Requested refund.</p>\n<p><strong>Draft response</strong>: [AI-generated response]</p>\n<p><strong>Confidence</strong>: Medium (85%)</p>\n<p><strong>Flags</strong>:</p>\n<ul>\n<li>Customer mentions legal action (escalation policy may apply)</li>\n<li>Account shows 3 prior complaints (retention risk)</li>\n</ul>\n<p><strong>Your action</strong>: Review draft and decide on compensation offer</p>\n</blockquote>\n<h3>Escalation Path Design</h3>\n<p><strong>Escalation matrix:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Trigger</th>\n<th>Response</th>\n<th>Human Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Low confidence (&lt;70%)</td>\n<td>Flag for review</td>\n<td>Verify before sending</td>\n</tr>\n<tr>\n<td>Policy keyword detected</td>\n<td>Hard stop</td>\n<td>Human must decide</td>\n</tr>\n<tr>\n<td>Multiple valid options</td>\n<td>Present choices</td>\n<td>Human selects</td>\n</tr>\n<tr>\n<td>Task failure</td>\n<td>Report with diagnosis</td>\n<td>Human troubleshoots</td>\n</tr>\n<tr>\n<td>High-value customer</td>\n<td>Require approval</td>\n<td>Human approves</td>\n</tr>\n</tbody></table>\n<h3>Human Oversight Patterns</h3>\n<p><strong>Pattern 1: Pre-flight approval</strong></p>\n<pre><code>Human defines task → AI executes → Human uses output\n</code></pre>\n<p>Best for: Routine tasks, experienced operators</p>\n<p><strong>Pattern 2: Checkpoint approval</strong></p>\n<pre><code>AI proposes → Human approves → AI executes\n</code></pre>\n<p>Best for: Consequential actions, early adoption</p>\n<p><strong>Pattern 3: Post-hoc review</strong></p>\n<pre><code>AI executes → Human reviews sample → Feedback loop\n</code></pre>\n<p>Best for: High-volume, lower-stakes</p>\n<p><strong>Pattern 4: Exception-based</strong></p>\n<pre><code>AI executes normally → Escalates exceptions → Human handles edge cases\n</code></pre>\n<p>Best for: Mature workflows, clear rules</p>\n<hr>\n<h3>Production HITL Patterns</h3>\n<p>Theory is useful. But production systems reveal specific patterns that work at scale.</p>\n<p><strong>Pattern A: Approval Gates</strong></p>\n<p>Use when: Before irreversible actions (delete, send, publish, charge)</p>\n<pre><code>AI completes task → PAUSE → Human reviews → Approve/Edit/Reject → Continue or stop\n</code></pre>\n<p>Implementation details:</p>\n<ul>\n<li>Set timeout with default action (approve or reject if no response)</li>\n<li>Allow inline editing of AI output before approval</li>\n<li>Track approval rates to identify automation candidates</li>\n</ul>\n<p><strong>Pattern B: Confidence-Based Routing</strong></p>\n<p>Tuning guidance:</p>\n<ul>\n<li>Start with low threshold (high human involvement)</li>\n<li>Gradually raise as you verify AI quality</li>\n<li>Monitor for &quot;confident but wrong&quot; failures</li>\n</ul>\n<p><strong>Pattern C: Escalation Ladders</strong></p>\n<p>Use when: Graduated authority levels</p>\n<pre><code>AI agent → L1 Support → L2 Specialist → Manager → Executive\n</code></pre>\n<p>Each level has:</p>\n<ul>\n<li>Defined scope of authority</li>\n<li>Clear handoff criteria</li>\n<li>Required context package</li>\n<li>Time-to-response expectations</li>\n</ul>\n<p><strong>Pattern D: Async Feedback Loops</strong></p>\n<p>Use when: Quality matters but latency doesn&#39;t</p>\n<pre><code>AI completes → Delivers to user → Human reviews later → Corrections feed back to training\n</code></pre>\n<p>Key benefits:</p>\n<ul>\n<li>No workflow blocking</li>\n<li>Batch review efficiency</li>\n<li>Continuous improvement signal</li>\n<li>No single point of failure</li>\n</ul>\n<p><strong>Pattern E: Audit Trails</strong></p>\n<p>Use when: Compliance/traceability without blocking</p>\n<pre><code>AI acts → Logs action + reasoning → Continues → Periodic human audit\n</code></pre>\n<p>Log structure:</p>\n<ul>\n<li>WHO: User/system that triggered action</li>\n<li>WHAT: Exact action taken</li>\n<li>WHEN: Timestamp</li>\n<li>WHY: AI&#39;s reasoning/confidence</li>\n<li>OUTCOME: Result of action</li>\n</ul>\n<h3>HITL Production Considerations</h3>\n<table>\n<thead>\n<tr>\n<th>Factor</th>\n<th>Impact</th>\n<th>Mitigation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Latency</strong></td>\n<td>0.5-2s per human decision</td>\n<td>Use async patterns, batch reviews</td>\n</tr>\n<tr>\n<td><strong>Bottlenecks</strong></td>\n<td>Human capacity limits scale</td>\n<td>Confidence routing, clear thresholds</td>\n</tr>\n<tr>\n<td><strong>Quality variance</strong></td>\n<td>Reviewers have different standards</td>\n<td>Training, calibration sessions, rubrics</td>\n</tr>\n<tr>\n<td><strong>Fatigue</strong></td>\n<td>Alert fatigue reduces effectiveness</td>\n<td>Prioritize escalations, rotate reviewers</td>\n</tr>\n<tr>\n<td><strong>Audit compliance</strong></td>\n<td>Required in regulated industries</td>\n<td>Structured logging from day one</td>\n</tr>\n</tbody></table>\n<h3>Regulatory Note: EU AI Act</h3>\n<p>Article 14 requires &quot;effective oversight by natural persons&quot; for high-risk AI systems.</p>\n<p><strong>HITL isn&#39;t optional for:</strong></p>\n<ul>\n<li>Hiring and recruitment tools</li>\n<li>Credit scoring and lending</li>\n<li>Legal and judicial AI</li>\n<li>Medical diagnosis assistance</li>\n<li>Critical infrastructure control</li>\n</ul>\n<p>If you&#39;re building in these domains, HITL must be architected from the start—not bolted on later.</p>\n<h3>Designing Meaningful Checkpoints</h3>\n<p><strong>Bad checkpoint</strong> (rubber stamp):</p>\n<ul>\n<li>Human sees: &quot;AI did something. OK?&quot;</li>\n<li>Human can: Click &quot;approve&quot; or &quot;reject&quot;</li>\n<li>Human knows: Nothing useful</li>\n<li>Result: Automatic approval, no real oversight</li>\n</ul>\n<p><strong>Good checkpoint</strong>:</p>\n<ul>\n<li>Human sees: Context, draft, confidence, concerns</li>\n<li>Human can: Approve, edit, reject, escalate, redirect</li>\n<li>Human knows: What AI did, why, and risk areas</li>\n<li>Result: Genuine review, meaningful intervention</li>\n</ul>\n<hr>\n"
        },
        {
          "id": "how-to-apply-this",
          "title": "HOW to Apply This",
          "type": "how",
          "content": "### Exercise: Design a Handoff System\n\n### Handoff Anti-Patterns\n\n| Anti-Pattern | Problem | Solution |\n|--------------|---------|----------|\n| **Rubber stamp** | Human approves without review | Make review necessary and fast |\n| **Context loss** | Human doesn't know what AI did | Include comprehensive context |\n| **Hidden escalation** | AI decides when to escalate | Clear, auditable rules |\n| **Escalation overload** | Too many escalations | Tune triggers, improve AI |\n| **No return path** | Human can't redirect AI | Enable mid-workflow intervention |\n| **Blame ambiguity** | Unclear who's responsible | Explicit ownership at each stage |\n\n### Checklist for Handoff Design\n\n| Element | Question |\n|---------|----------|\n| **Triggers** | When does handoff occur? |\n| **Context** | What information transfers? |\n| **Actions** | What can the recipient do? |\n| **Responsibility** | Who owns the outcome? |\n| **Escalation** | What if recipient can't handle it? |\n| **Logging** | Is the handoff recorded? |\n| **Recovery** | What if handoff fails? |\n\n### Self-Check\n\n---",
          "htmlContent": "<h3>Exercise: Design a Handoff System</h3>\n<h3>Handoff Anti-Patterns</h3>\n<table>\n<thead>\n<tr>\n<th>Anti-Pattern</th>\n<th>Problem</th>\n<th>Solution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Rubber stamp</strong></td>\n<td>Human approves without review</td>\n<td>Make review necessary and fast</td>\n</tr>\n<tr>\n<td><strong>Context loss</strong></td>\n<td>Human doesn&#39;t know what AI did</td>\n<td>Include comprehensive context</td>\n</tr>\n<tr>\n<td><strong>Hidden escalation</strong></td>\n<td>AI decides when to escalate</td>\n<td>Clear, auditable rules</td>\n</tr>\n<tr>\n<td><strong>Escalation overload</strong></td>\n<td>Too many escalations</td>\n<td>Tune triggers, improve AI</td>\n</tr>\n<tr>\n<td><strong>No return path</strong></td>\n<td>Human can&#39;t redirect AI</td>\n<td>Enable mid-workflow intervention</td>\n</tr>\n<tr>\n<td><strong>Blame ambiguity</strong></td>\n<td>Unclear who&#39;s responsible</td>\n<td>Explicit ownership at each stage</td>\n</tr>\n</tbody></table>\n<h3>Checklist for Handoff Design</h3>\n<table>\n<thead>\n<tr>\n<th>Element</th>\n<th>Question</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Triggers</strong></td>\n<td>When does handoff occur?</td>\n</tr>\n<tr>\n<td><strong>Context</strong></td>\n<td>What information transfers?</td>\n</tr>\n<tr>\n<td><strong>Actions</strong></td>\n<td>What can the recipient do?</td>\n</tr>\n<tr>\n<td><strong>Responsibility</strong></td>\n<td>Who owns the outcome?</td>\n</tr>\n<tr>\n<td><strong>Escalation</strong></td>\n<td>What if recipient can&#39;t handle it?</td>\n</tr>\n<tr>\n<td><strong>Logging</strong></td>\n<td>Is the handoff recorded?</td>\n</tr>\n<tr>\n<td><strong>Recovery</strong></td>\n<td>What if handoff fails?</td>\n</tr>\n</tbody></table>\n<h3>Self-Check</h3>\n<hr>\n"
        },
        {
          "id": "phase-2-complete!",
          "title": "Phase 2 Complete!",
          "type": "generic",
          "content": "You've mastered Workflow Engineering fundamentals. Before moving to Phase 3, complete:\n\n**Lab 3: Workflow Mapping** — Document a real workflow and identify AI integration opportunities\n\n**Lab 4: Quality Gate Design** — Create a quality assurance system for an AI workflow\n\n**Phase 2 Deliverable: Workflow Automation Proposal** — Document a real workflow with AI integration opportunities, including ROI analysis",
          "htmlContent": "<p>You&#39;ve mastered Workflow Engineering fundamentals. Before moving to Phase 3, complete:</p>\n<p><strong>Lab 3: Workflow Mapping</strong> — Document a real workflow and identify AI integration opportunities</p>\n<p><strong>Lab 4: Quality Gate Design</strong> — Create a quality assurance system for an AI workflow</p>\n<p><strong>Phase 2 Deliverable: Workflow Automation Proposal</strong> — Document a real workflow with AI integration opportunities, including ROI analysis</p>\n"
        }
      ],
      "concepts": [
        {
          "id": "handoff-design",
          "term": "handoff design",
          "definition": "A **handoff** is any point where control or responsibility transfers between AI and human. Well-designed handoffs:\n\n1. **Preserve context**: Human knows what AI did and why\n2. **Enable verification**: Human can assess AI's work\n3. **Allow intervention**: Human can modify, reject, or redirect\n4. **Define responsibility**: Clear who owns the outcome\n5. **Support escalation**: Path exists when AI can't proceed",
          "htmlDefinition": "<p>A <strong>handoff</strong> is any point where control or responsibility transfers between AI and human. Well-designed handoffs:</p>\n<ol>\n<li><strong>Preserve context</strong>: Human knows what AI did and why</li>\n<li><strong>Enable verification</strong>: Human can assess AI&#39;s work</li>\n<li><strong>Allow intervention</strong>: Human can modify, reject, or redirect</li>\n<li><strong>Define responsibility</strong>: Clear who owns the outcome</li>\n<li><strong>Support escalation</strong>: Path exists when AI can&#39;t proceed</li>\n</ol>\n"
        },
        {
          "id": "escalation",
          "term": "escalation",
          "definition": "**Escalation** occurs when AI recognizes it shouldn't proceed autonomously. Triggers include:\n\n- **Uncertainty**: AI isn't confident in the right answer\n- **Policy**: Situation matches escalation rules\n- **Anomaly**: Input is outside normal patterns\n- **Failure**: AI can't complete the task\n- **Stakes**: Outcome consequences exceed AI's authority",
          "htmlDefinition": "<p><strong>Escalation</strong> occurs when AI recognizes it shouldn&#39;t proceed autonomously. Triggers include:</p>\n<ul>\n<li><strong>Uncertainty</strong>: AI isn&#39;t confident in the right answer</li>\n<li><strong>Policy</strong>: Situation matches escalation rules</li>\n<li><strong>Anomaly</strong>: Input is outside normal patterns</li>\n<li><strong>Failure</strong>: AI can&#39;t complete the task</li>\n<li><strong>Stakes</strong>: Outcome consequences exceed AI&#39;s authority</li>\n</ul>\n"
        },
        {
          "id": "hitl-patterns",
          "term": "hitl patterns",
          "definition": "**Human-in-the-Loop (HITL)** in production means integrating structured human intervention points into autonomous AI systems. The key is knowing WHICH pattern to use WHERE.\n\nFive battle-tested patterns:",
          "htmlDefinition": "<p><strong>Human-in-the-Loop (HITL)</strong> in production means integrating structured human intervention points into autonomous AI systems. The key is knowing WHICH pattern to use WHERE.</p>\n<p>Five battle-tested patterns:</p>\n"
        },
        {
          "id": "confidence-routing",
          "term": "confidence routing",
          "definition": "Use when: High-volume tasks with occasional edge cases\n\n```\nAI processes request → Scores own confidence\n├─ Above threshold (e.g., 85%): Auto-proceed\n└─ Below threshold: Route to human queue\n```\n\n**Critical metric:** Maintain 10-15% human review rate for sustainable operations. Too high = AI isn't good enough. Too low = might be missing problems.",
          "htmlDefinition": "<p>Use when: High-volume tasks with occasional edge cases</p>\n<pre><code>AI processes request → Scores own confidence\n├─ Above threshold (e.g., 85%): Auto-proceed\n└─ Below threshold: Route to human queue\n</code></pre>\n<p><strong>Critical metric:</strong> Maintain 10-15% human review rate for sustainable operations. Too high = AI isn&#39;t good enough. Too low = might be missing problems.</p>\n"
        }
      ],
      "exercises": [
        {
          "id": "handoff-design-exercise",
          "title": "Scenario",
          "instructions": "You're designing an AI system to handle initial customer support inquiries. The system should:\n- Classify incoming tickets by type and urgency\n- Draft initial responses\n- Identify tickets needing human attention\n- Route complex issues to appropriate teams\n\n**Design the handoff system:**\n\n1. **Map all handoff points**\n   - Where does AI receive human input?\n   - Where should AI pause for approval?\n   - Where does AI deliver final output?\n\n2. **Define escalation triggers**\n   - What situations should always go to humans?\n   - What confidence threshold requires review?\n   - What keywords/patterns trigger escalation?\n\n3. **Design the context package**\n   - What information should AI provide at each handoff?\n   - What does the human need to make a good decision?\n\n4. **Create the checkpoint interface**\n   - What options does the human have?\n   - How does human feedback improve future performance?\n\n5. **Plan for failures**\n   - What happens if AI crashes mid-task?\n   - How is work recovered?\n   - Who is notified?",
          "htmlInstructions": "<p>You&#39;re designing an AI system to handle initial customer support inquiries. The system should:</p>\n<ul>\n<li>Classify incoming tickets by type and urgency</li>\n<li>Draft initial responses</li>\n<li>Identify tickets needing human attention</li>\n<li>Route complex issues to appropriate teams</li>\n</ul>\n<p><strong>Design the handoff system:</strong></p>\n<ol>\n<li><p><strong>Map all handoff points</strong></p>\n<ul>\n<li>Where does AI receive human input?</li>\n<li>Where should AI pause for approval?</li>\n<li>Where does AI deliver final output?</li>\n</ul>\n</li>\n<li><p><strong>Define escalation triggers</strong></p>\n<ul>\n<li>What situations should always go to humans?</li>\n<li>What confidence threshold requires review?</li>\n<li>What keywords/patterns trigger escalation?</li>\n</ul>\n</li>\n<li><p><strong>Design the context package</strong></p>\n<ul>\n<li>What information should AI provide at each handoff?</li>\n<li>What does the human need to make a good decision?</li>\n</ul>\n</li>\n<li><p><strong>Create the checkpoint interface</strong></p>\n<ul>\n<li>What options does the human have?</li>\n<li>How does human feedback improve future performance?</li>\n</ul>\n</li>\n<li><p><strong>Plan for failures</strong></p>\n<ul>\n<li>What happens if AI crashes mid-task?</li>\n<li>How is work recovered?</li>\n<li>Who is notified?</li>\n</ul>\n</li>\n</ol>\n"
        }
      ],
      "checklists": [
        {
          "id": "module-2.4-complete",
          "items": [
            {
              "id": "module-2.4-complete-0",
              "text": "I can identify handoff points in a workflow",
              "completed": false
            },
            {
              "id": "module-2.4-complete-1",
              "text": "I understand different types of handoffs",
              "completed": false
            },
            {
              "id": "module-2.4-complete-2",
              "text": "I can design context packages for handoffs",
              "completed": false
            },
            {
              "id": "module-2.4-complete-3",
              "text": "I know how to set up escalation triggers",
              "completed": false
            },
            {
              "id": "module-2.4-complete-4",
              "text": "I can recognize and fix handoff anti-patterns",
              "completed": false
            }
          ]
        }
      ]
    }
  ],
  "labs": [
    {
      "id": "lab-3-workflow-mapping",
      "slug": "lab-3-workflow-mapping",
      "title": "Workflow Mapping",
      "phase": 2,
      "labNumber": 3,
      "estimatedMinutes": 50,
      "objectives": [
        "Document a real workflow with all key elements",
        "Identify AI integration opportunities",
        "Create an AI integration proposal"
      ],
      "prerequisites": [
        "2.1-process-analysis",
        "2.2-task-decomposition"
      ],
      "content": "# Lab 3: Workflow Mapping\n\n## Lab Overview\n\nIn this lab, you'll analyze a real workflow and identify specific opportunities for AI integration. You'll practice the systematic process analysis skills that separate strategic AI operators from those who apply AI randomly.\n\n**What you'll create:**\n- A detailed workflow map\n- AI suitability scores for each step\n- A prioritized AI integration roadmap\n\n---\n\n## Part 1: Select Your Workflow (5 minutes)\n\n### Choose one of these workflows to analyze:\n\n**Option A: Content Creation Pipeline**\nHow content (blog posts, reports, marketing materials) goes from idea to publication.\n\n**Option B: Customer Onboarding**\nHow new customers get from signed contract to actively using your product/service.\n\n**Option C: Hiring Process**\nHow candidates go from job posting to hired employee.\n\n**Option D: Monthly Reporting**\nHow data becomes insights becomes decision-making documents.\n\n**Option E: Your Choice**\nA workflow you know well from your work or experience.\n\n*If you don't have direct experience, choose one and research or imagine the typical steps.*\n\n---\n\n## Part 2: Document Current State (15 minutes)\n\n### Map Every Step\n\nFor your chosen workflow, document each step using this format:\n\n```\nSTEP [#]: [Name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nTrigger: What causes this step to start?\nOwner: Who is responsible?\nInput: What does this step receive?\nActions: What happens? (bullet points)\nOutput: What is produced?\nDuration: How long does it take?\nTools used: What systems/tools?\nDependencies: What must happen first?\nPain points: What problems occur here?\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Document at least 8-10 steps\n\nInclude steps that might seem minor—they often hide AI opportunities.\n\n### Identify Bottlenecks\n\nMark which steps are bottlenecks:\n- Where does work pile up?\n- Where are the longest delays?\n- Where do errors most often occur?\n\n---\n\n## Part 3: Score AI Suitability (10 minutes)\n\n### Score Each Step\n\nFor every step in your workflow, rate these factors (1-5):\n\n| Factor | 1 (Low) | 5 (High) |\n|--------|---------|----------|\n| **Repetitiveness** | Unique every time | Same task repeatedly |\n| **Data intensity** | Minimal data | Heavy data processing |\n| **Time consumption** | Seconds | Hours |\n| **Quality variance** | Always consistent | Quality varies |\n| **Criteria clarity** | Highly subjective | Clear right/wrong |\n\n### Calculate AI Suitability Score\n\n**Total score = Sum of all factors**\n\n| Score | AI Potential | Recommendation |\n|-------|--------------|----------------|\n| 20-25 | Very High | Prioritize for AI automation |\n| 15-19 | High | Strong AI assist candidate |\n| 10-14 | Medium | Selective AI support |\n| 5-9 | Low | Keep human-led |\n\n### Complete the Scoring Matrix\n\n| Step | Rep. | Data | Time | Var. | Clarity | Total | AI Potential |\n|------|------|------|------|------|---------|-------|--------------|\n| 1 | | | | | | | |\n| 2 | | | | | | | |\n| ... | | | | | | | |\n\n---\n\n## Part 4: Identify AI Opportunities (10 minutes)\n\n### For Each High-Potential Step\n\nDocument the AI opportunity:\n\n```\nOPPORTUNITY: [Step name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nCurrent state: [What happens now]\n\nAI integration type:\n☐ Automate (AI does it independently)\n☐ Assist (AI helps human do it faster)\n☐ Augment (AI provides inputs for human decision)\n\nWhat AI would do: [Specific description]\n\nExpected benefits:\n- Time saved: [Estimate]\n- Quality improvement: [Describe]\n- Other value: [Describe]\n\nRequirements:\n- Data needed: [What AI needs access to]\n- Integration: [Systems AI must connect with]\n- Human oversight: [What verification needed]\n\nRisks:\n- [Risk 1]\n- [Risk 2]\n\nImplementation complexity: Low / Medium / High\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Identify 4-6 AI Opportunities\n\nFocus on steps with the highest AI suitability scores.\n\n---\n\n## Part 5: Create Integration Roadmap (10 minutes)\n\n### Prioritization Matrix\n\nPlot your opportunities on this matrix:\n\n```\nHIGH    │ ★ Quick Wins      │ ★ Strategic Bets\nIMPACT  │ (Do first)        │ (Plan carefully)\n        │                   │\n        ├───────────────────┼───────────────────\n        │                   │\nLOW     │ Fill-ins          │ Avoid\nIMPACT  │ (Do if easy)      │ (Don't bother)\n        │                   │\n        └───────────────────┴───────────────────\n              LOW EFFORT         HIGH EFFORT\n```\n\n### Create Phased Roadmap\n\n**Phase 1: Quick Wins (Week 1-2)**\n- [Opportunity 1]\n- [Opportunity 2]\n\n**Phase 2: Core Integration (Month 1-2)**\n- [Opportunity 3]\n- [Opportunity 4]\n\n**Phase 3: Advanced (Quarter 1-2)**\n- [Opportunity 5]\n- [Opportunity 6]\n\n### Success Metrics\n\nFor each phase, define:\n- What will be measured?\n- What's the target improvement?\n- How will you know it's working?\n\n---\n\n## Part 6: Visualize the Future State (5 minutes)\n\n### Create Before/After Comparison\n\n**Before (Current State):**\n```\n[Step 1] → [Step 2] → [Step 3] → [Step 4] → ...\n  ↓          ↓          ↓          ↓\n[Pain]    [Pain]     [Pain]     [Pain]\n```\n\n**After (AI-Enhanced):**\n```\n[Step 1]    →   [AI: Step 2+3]   →   [Step 4]   → ...\n    ↓               ↓                   ↓\n[Improved]      [New AI]           [Enhanced]\n```\n\nShow which steps are:\n- Eliminated\n- Automated\n- Augmented\n- Unchanged\n\n---\n\n## Deliverable\n\nCreate a comprehensive document containing:\n\n1. **Workflow Overview**\n   - Name and description\n   - Scope (start and end points)\n   - Key stakeholders\n\n2. **Current State Map**\n   - All steps with full documentation\n   - Bottleneck identification\n   - Pain points summary\n\n3. **AI Suitability Analysis**\n   - Scoring matrix\n   - Methodology notes\n\n4. **AI Opportunities**\n   - 4-6 detailed opportunity descriptions\n   - Integration type for each\n\n5. **Integration Roadmap**\n   - Prioritization matrix\n   - Phased plan\n   - Success metrics\n\n6. **Future State Vision**\n   - Before/after visualization\n   - Expected improvements\n\n---\n\n## Extension Challenge\n\n**Calculate the Business Case**\n\nFor your top AI opportunity:\n1. Estimate current cost (labor hours × rate)\n2. Estimate AI implementation cost\n3. Estimate ongoing AI cost\n4. Calculate ROI and payback period\n\nDocument your assumptions and calculations.",
      "htmlContent": "<h1>Lab 3: Workflow Mapping</h1>\n<h2>Lab Overview</h2>\n<p>In this lab, you&#39;ll analyze a real workflow and identify specific opportunities for AI integration. You&#39;ll practice the systematic process analysis skills that separate strategic AI operators from those who apply AI randomly.</p>\n<p><strong>What you&#39;ll create:</strong></p>\n<ul>\n<li>A detailed workflow map</li>\n<li>AI suitability scores for each step</li>\n<li>A prioritized AI integration roadmap</li>\n</ul>\n<hr>\n<h2>Part 1: Select Your Workflow (5 minutes)</h2>\n<h3>Choose one of these workflows to analyze:</h3>\n<p><strong>Option A: Content Creation Pipeline</strong>\nHow content (blog posts, reports, marketing materials) goes from idea to publication.</p>\n<p><strong>Option B: Customer Onboarding</strong>\nHow new customers get from signed contract to actively using your product/service.</p>\n<p><strong>Option C: Hiring Process</strong>\nHow candidates go from job posting to hired employee.</p>\n<p><strong>Option D: Monthly Reporting</strong>\nHow data becomes insights becomes decision-making documents.</p>\n<p><strong>Option E: Your Choice</strong>\nA workflow you know well from your work or experience.</p>\n<p><em>If you don&#39;t have direct experience, choose one and research or imagine the typical steps.</em></p>\n<hr>\n<h2>Part 2: Document Current State (15 minutes)</h2>\n<h3>Map Every Step</h3>\n<p>For your chosen workflow, document each step using this format:</p>\n<pre><code>STEP [#]: [Name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nTrigger: What causes this step to start?\nOwner: Who is responsible?\nInput: What does this step receive?\nActions: What happens? (bullet points)\nOutput: What is produced?\nDuration: How long does it take?\nTools used: What systems/tools?\nDependencies: What must happen first?\nPain points: What problems occur here?\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<h3>Document at least 8-10 steps</h3>\n<p>Include steps that might seem minor—they often hide AI opportunities.</p>\n<h3>Identify Bottlenecks</h3>\n<p>Mark which steps are bottlenecks:</p>\n<ul>\n<li>Where does work pile up?</li>\n<li>Where are the longest delays?</li>\n<li>Where do errors most often occur?</li>\n</ul>\n<hr>\n<h2>Part 3: Score AI Suitability (10 minutes)</h2>\n<h3>Score Each Step</h3>\n<p>For every step in your workflow, rate these factors (1-5):</p>\n<table>\n<thead>\n<tr>\n<th>Factor</th>\n<th>1 (Low)</th>\n<th>5 (High)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Repetitiveness</strong></td>\n<td>Unique every time</td>\n<td>Same task repeatedly</td>\n</tr>\n<tr>\n<td><strong>Data intensity</strong></td>\n<td>Minimal data</td>\n<td>Heavy data processing</td>\n</tr>\n<tr>\n<td><strong>Time consumption</strong></td>\n<td>Seconds</td>\n<td>Hours</td>\n</tr>\n<tr>\n<td><strong>Quality variance</strong></td>\n<td>Always consistent</td>\n<td>Quality varies</td>\n</tr>\n<tr>\n<td><strong>Criteria clarity</strong></td>\n<td>Highly subjective</td>\n<td>Clear right/wrong</td>\n</tr>\n</tbody></table>\n<h3>Calculate AI Suitability Score</h3>\n<p><strong>Total score = Sum of all factors</strong></p>\n<table>\n<thead>\n<tr>\n<th>Score</th>\n<th>AI Potential</th>\n<th>Recommendation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>20-25</td>\n<td>Very High</td>\n<td>Prioritize for AI automation</td>\n</tr>\n<tr>\n<td>15-19</td>\n<td>High</td>\n<td>Strong AI assist candidate</td>\n</tr>\n<tr>\n<td>10-14</td>\n<td>Medium</td>\n<td>Selective AI support</td>\n</tr>\n<tr>\n<td>5-9</td>\n<td>Low</td>\n<td>Keep human-led</td>\n</tr>\n</tbody></table>\n<h3>Complete the Scoring Matrix</h3>\n<table>\n<thead>\n<tr>\n<th>Step</th>\n<th>Rep.</th>\n<th>Data</th>\n<th>Time</th>\n<th>Var.</th>\n<th>Clarity</th>\n<th>Total</th>\n<th>AI Potential</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>2</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>...</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<hr>\n<h2>Part 4: Identify AI Opportunities (10 minutes)</h2>\n<h3>For Each High-Potential Step</h3>\n<p>Document the AI opportunity:</p>\n<pre><code>OPPORTUNITY: [Step name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nCurrent state: [What happens now]\n\nAI integration type:\n☐ Automate (AI does it independently)\n☐ Assist (AI helps human do it faster)\n☐ Augment (AI provides inputs for human decision)\n\nWhat AI would do: [Specific description]\n\nExpected benefits:\n- Time saved: [Estimate]\n- Quality improvement: [Describe]\n- Other value: [Describe]\n\nRequirements:\n- Data needed: [What AI needs access to]\n- Integration: [Systems AI must connect with]\n- Human oversight: [What verification needed]\n\nRisks:\n- [Risk 1]\n- [Risk 2]\n\nImplementation complexity: Low / Medium / High\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<h3>Identify 4-6 AI Opportunities</h3>\n<p>Focus on steps with the highest AI suitability scores.</p>\n<hr>\n<h2>Part 5: Create Integration Roadmap (10 minutes)</h2>\n<h3>Prioritization Matrix</h3>\n<p>Plot your opportunities on this matrix:</p>\n<pre><code>HIGH    │ ★ Quick Wins      │ ★ Strategic Bets\nIMPACT  │ (Do first)        │ (Plan carefully)\n        │                   │\n        ├───────────────────┼───────────────────\n        │                   │\nLOW     │ Fill-ins          │ Avoid\nIMPACT  │ (Do if easy)      │ (Don&#39;t bother)\n        │                   │\n        └───────────────────┴───────────────────\n              LOW EFFORT         HIGH EFFORT\n</code></pre>\n<h3>Create Phased Roadmap</h3>\n<p><strong>Phase 1: Quick Wins (Week 1-2)</strong></p>\n<ul>\n<li>[Opportunity 1]</li>\n<li>[Opportunity 2]</li>\n</ul>\n<p><strong>Phase 2: Core Integration (Month 1-2)</strong></p>\n<ul>\n<li>[Opportunity 3]</li>\n<li>[Opportunity 4]</li>\n</ul>\n<p><strong>Phase 3: Advanced (Quarter 1-2)</strong></p>\n<ul>\n<li>[Opportunity 5]</li>\n<li>[Opportunity 6]</li>\n</ul>\n<h3>Success Metrics</h3>\n<p>For each phase, define:</p>\n<ul>\n<li>What will be measured?</li>\n<li>What&#39;s the target improvement?</li>\n<li>How will you know it&#39;s working?</li>\n</ul>\n<hr>\n<h2>Part 6: Visualize the Future State (5 minutes)</h2>\n<h3>Create Before/After Comparison</h3>\n<p><strong>Before (Current State):</strong></p>\n<pre><code>[Step 1] → [Step 2] → [Step 3] → [Step 4] → ...\n  ↓          ↓          ↓          ↓\n[Pain]    [Pain]     [Pain]     [Pain]\n</code></pre>\n<p><strong>After (AI-Enhanced):</strong></p>\n<pre><code>[Step 1]    →   [AI: Step 2+3]   →   [Step 4]   → ...\n    ↓               ↓                   ↓\n[Improved]      [New AI]           [Enhanced]\n</code></pre>\n<p>Show which steps are:</p>\n<ul>\n<li>Eliminated</li>\n<li>Automated</li>\n<li>Augmented</li>\n<li>Unchanged</li>\n</ul>\n<hr>\n<h2>Deliverable</h2>\n<p>Create a comprehensive document containing:</p>\n<ol>\n<li><p><strong>Workflow Overview</strong></p>\n<ul>\n<li>Name and description</li>\n<li>Scope (start and end points)</li>\n<li>Key stakeholders</li>\n</ul>\n</li>\n<li><p><strong>Current State Map</strong></p>\n<ul>\n<li>All steps with full documentation</li>\n<li>Bottleneck identification</li>\n<li>Pain points summary</li>\n</ul>\n</li>\n<li><p><strong>AI Suitability Analysis</strong></p>\n<ul>\n<li>Scoring matrix</li>\n<li>Methodology notes</li>\n</ul>\n</li>\n<li><p><strong>AI Opportunities</strong></p>\n<ul>\n<li>4-6 detailed opportunity descriptions</li>\n<li>Integration type for each</li>\n</ul>\n</li>\n<li><p><strong>Integration Roadmap</strong></p>\n<ul>\n<li>Prioritization matrix</li>\n<li>Phased plan</li>\n<li>Success metrics</li>\n</ul>\n</li>\n<li><p><strong>Future State Vision</strong></p>\n<ul>\n<li>Before/after visualization</li>\n<li>Expected improvements</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h2>Extension Challenge</h2>\n<p><strong>Calculate the Business Case</strong></p>\n<p>For your top AI opportunity:</p>\n<ol>\n<li>Estimate current cost (labor hours × rate)</li>\n<li>Estimate AI implementation cost</li>\n<li>Estimate ongoing AI cost</li>\n<li>Calculate ROI and payback period</li>\n</ol>\n<p>Document your assumptions and calculations.</p>\n",
      "sections": [
        {
          "id": "lab-overview",
          "title": "Lab Overview",
          "type": "generic",
          "content": "In this lab, you'll analyze a real workflow and identify specific opportunities for AI integration. You'll practice the systematic process analysis skills that separate strategic AI operators from those who apply AI randomly.\n\n**What you'll create:**\n- A detailed workflow map\n- AI suitability scores for each step\n- A prioritized AI integration roadmap\n\n---",
          "htmlContent": "<p>In this lab, you&#39;ll analyze a real workflow and identify specific opportunities for AI integration. You&#39;ll practice the systematic process analysis skills that separate strategic AI operators from those who apply AI randomly.</p>\n<p><strong>What you&#39;ll create:</strong></p>\n<ul>\n<li>A detailed workflow map</li>\n<li>AI suitability scores for each step</li>\n<li>A prioritized AI integration roadmap</li>\n</ul>\n<hr>\n"
        },
        {
          "id": "part-1:-select-your-workflow-(5-minutes)",
          "title": "Part 1: Select Your Workflow (5 minutes)",
          "type": "generic",
          "content": "### Choose one of these workflows to analyze:\n\n**Option A: Content Creation Pipeline**\nHow content (blog posts, reports, marketing materials) goes from idea to publication.\n\n**Option B: Customer Onboarding**\nHow new customers get from signed contract to actively using your product/service.\n\n**Option C: Hiring Process**\nHow candidates go from job posting to hired employee.\n\n**Option D: Monthly Reporting**\nHow data becomes insights becomes decision-making documents.\n\n**Option E: Your Choice**\nA workflow you know well from your work or experience.\n\n*If you don't have direct experience, choose one and research or imagine the typical steps.*\n\n---",
          "htmlContent": "<h3>Choose one of these workflows to analyze:</h3>\n<p><strong>Option A: Content Creation Pipeline</strong>\nHow content (blog posts, reports, marketing materials) goes from idea to publication.</p>\n<p><strong>Option B: Customer Onboarding</strong>\nHow new customers get from signed contract to actively using your product/service.</p>\n<p><strong>Option C: Hiring Process</strong>\nHow candidates go from job posting to hired employee.</p>\n<p><strong>Option D: Monthly Reporting</strong>\nHow data becomes insights becomes decision-making documents.</p>\n<p><strong>Option E: Your Choice</strong>\nA workflow you know well from your work or experience.</p>\n<p><em>If you don&#39;t have direct experience, choose one and research or imagine the typical steps.</em></p>\n<hr>\n"
        },
        {
          "id": "part-2:-document-current-state-(15-minutes)",
          "title": "Part 2: Document Current State (15 minutes)",
          "type": "generic",
          "content": "### Map Every Step\n\nFor your chosen workflow, document each step using this format:\n\n```\nSTEP [#]: [Name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nTrigger: What causes this step to start?\nOwner: Who is responsible?\nInput: What does this step receive?\nActions: What happens? (bullet points)\nOutput: What is produced?\nDuration: How long does it take?\nTools used: What systems/tools?\nDependencies: What must happen first?\nPain points: What problems occur here?\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Document at least 8-10 steps\n\nInclude steps that might seem minor—they often hide AI opportunities.\n\n### Identify Bottlenecks\n\nMark which steps are bottlenecks:\n- Where does work pile up?\n- Where are the longest delays?\n- Where do errors most often occur?\n\n---",
          "htmlContent": "<h3>Map Every Step</h3>\n<p>For your chosen workflow, document each step using this format:</p>\n<pre><code>STEP [#]: [Name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nTrigger: What causes this step to start?\nOwner: Who is responsible?\nInput: What does this step receive?\nActions: What happens? (bullet points)\nOutput: What is produced?\nDuration: How long does it take?\nTools used: What systems/tools?\nDependencies: What must happen first?\nPain points: What problems occur here?\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<h3>Document at least 8-10 steps</h3>\n<p>Include steps that might seem minor—they often hide AI opportunities.</p>\n<h3>Identify Bottlenecks</h3>\n<p>Mark which steps are bottlenecks:</p>\n<ul>\n<li>Where does work pile up?</li>\n<li>Where are the longest delays?</li>\n<li>Where do errors most often occur?</li>\n</ul>\n<hr>\n"
        },
        {
          "id": "part-3:-score-ai-suitability-(10-minutes)",
          "title": "Part 3: Score AI Suitability (10 minutes)",
          "type": "generic",
          "content": "### Score Each Step\n\nFor every step in your workflow, rate these factors (1-5):\n\n| Factor | 1 (Low) | 5 (High) |\n|--------|---------|----------|\n| **Repetitiveness** | Unique every time | Same task repeatedly |\n| **Data intensity** | Minimal data | Heavy data processing |\n| **Time consumption** | Seconds | Hours |\n| **Quality variance** | Always consistent | Quality varies |\n| **Criteria clarity** | Highly subjective | Clear right/wrong |\n\n### Calculate AI Suitability Score\n\n**Total score = Sum of all factors**\n\n| Score | AI Potential | Recommendation |\n|-------|--------------|----------------|\n| 20-25 | Very High | Prioritize for AI automation |\n| 15-19 | High | Strong AI assist candidate |\n| 10-14 | Medium | Selective AI support |\n| 5-9 | Low | Keep human-led |\n\n### Complete the Scoring Matrix\n\n| Step | Rep. | Data | Time | Var. | Clarity | Total | AI Potential |\n|------|------|------|------|------|---------|-------|--------------|\n| 1 | | | | | | | |\n| 2 | | | | | | | |\n| ... | | | | | | | |\n\n---",
          "htmlContent": "<h3>Score Each Step</h3>\n<p>For every step in your workflow, rate these factors (1-5):</p>\n<table>\n<thead>\n<tr>\n<th>Factor</th>\n<th>1 (Low)</th>\n<th>5 (High)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Repetitiveness</strong></td>\n<td>Unique every time</td>\n<td>Same task repeatedly</td>\n</tr>\n<tr>\n<td><strong>Data intensity</strong></td>\n<td>Minimal data</td>\n<td>Heavy data processing</td>\n</tr>\n<tr>\n<td><strong>Time consumption</strong></td>\n<td>Seconds</td>\n<td>Hours</td>\n</tr>\n<tr>\n<td><strong>Quality variance</strong></td>\n<td>Always consistent</td>\n<td>Quality varies</td>\n</tr>\n<tr>\n<td><strong>Criteria clarity</strong></td>\n<td>Highly subjective</td>\n<td>Clear right/wrong</td>\n</tr>\n</tbody></table>\n<h3>Calculate AI Suitability Score</h3>\n<p><strong>Total score = Sum of all factors</strong></p>\n<table>\n<thead>\n<tr>\n<th>Score</th>\n<th>AI Potential</th>\n<th>Recommendation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>20-25</td>\n<td>Very High</td>\n<td>Prioritize for AI automation</td>\n</tr>\n<tr>\n<td>15-19</td>\n<td>High</td>\n<td>Strong AI assist candidate</td>\n</tr>\n<tr>\n<td>10-14</td>\n<td>Medium</td>\n<td>Selective AI support</td>\n</tr>\n<tr>\n<td>5-9</td>\n<td>Low</td>\n<td>Keep human-led</td>\n</tr>\n</tbody></table>\n<h3>Complete the Scoring Matrix</h3>\n<table>\n<thead>\n<tr>\n<th>Step</th>\n<th>Rep.</th>\n<th>Data</th>\n<th>Time</th>\n<th>Var.</th>\n<th>Clarity</th>\n<th>Total</th>\n<th>AI Potential</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>2</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>...</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<hr>\n"
        },
        {
          "id": "part-4:-identify-ai-opportunities-(10-minutes)",
          "title": "Part 4: Identify AI Opportunities (10 minutes)",
          "type": "generic",
          "content": "### For Each High-Potential Step\n\nDocument the AI opportunity:\n\n```\nOPPORTUNITY: [Step name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nCurrent state: [What happens now]\n\nAI integration type:\n☐ Automate (AI does it independently)\n☐ Assist (AI helps human do it faster)\n☐ Augment (AI provides inputs for human decision)\n\nWhat AI would do: [Specific description]\n\nExpected benefits:\n- Time saved: [Estimate]\n- Quality improvement: [Describe]\n- Other value: [Describe]\n\nRequirements:\n- Data needed: [What AI needs access to]\n- Integration: [Systems AI must connect with]\n- Human oversight: [What verification needed]\n\nRisks:\n- [Risk 1]\n- [Risk 2]\n\nImplementation complexity: Low / Medium / High\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Identify 4-6 AI Opportunities\n\nFocus on steps with the highest AI suitability scores.\n\n---",
          "htmlContent": "<h3>For Each High-Potential Step</h3>\n<p>Document the AI opportunity:</p>\n<pre><code>OPPORTUNITY: [Step name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nCurrent state: [What happens now]\n\nAI integration type:\n☐ Automate (AI does it independently)\n☐ Assist (AI helps human do it faster)\n☐ Augment (AI provides inputs for human decision)\n\nWhat AI would do: [Specific description]\n\nExpected benefits:\n- Time saved: [Estimate]\n- Quality improvement: [Describe]\n- Other value: [Describe]\n\nRequirements:\n- Data needed: [What AI needs access to]\n- Integration: [Systems AI must connect with]\n- Human oversight: [What verification needed]\n\nRisks:\n- [Risk 1]\n- [Risk 2]\n\nImplementation complexity: Low / Medium / High\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<h3>Identify 4-6 AI Opportunities</h3>\n<p>Focus on steps with the highest AI suitability scores.</p>\n<hr>\n"
        },
        {
          "id": "part-5:-create-integration-roadmap-(10-minutes)",
          "title": "Part 5: Create Integration Roadmap (10 minutes)",
          "type": "generic",
          "content": "### Prioritization Matrix\n\nPlot your opportunities on this matrix:\n\n```\nHIGH    │ ★ Quick Wins      │ ★ Strategic Bets\nIMPACT  │ (Do first)        │ (Plan carefully)\n        │                   │\n        ├───────────────────┼───────────────────\n        │                   │\nLOW     │ Fill-ins          │ Avoid\nIMPACT  │ (Do if easy)      │ (Don't bother)\n        │                   │\n        └───────────────────┴───────────────────\n              LOW EFFORT         HIGH EFFORT\n```\n\n### Create Phased Roadmap\n\n**Phase 1: Quick Wins (Week 1-2)**\n- [Opportunity 1]\n- [Opportunity 2]\n\n**Phase 2: Core Integration (Month 1-2)**\n- [Opportunity 3]\n- [Opportunity 4]\n\n**Phase 3: Advanced (Quarter 1-2)**\n- [Opportunity 5]\n- [Opportunity 6]\n\n### Success Metrics\n\nFor each phase, define:\n- What will be measured?\n- What's the target improvement?\n- How will you know it's working?\n\n---",
          "htmlContent": "<h3>Prioritization Matrix</h3>\n<p>Plot your opportunities on this matrix:</p>\n<pre><code>HIGH    │ ★ Quick Wins      │ ★ Strategic Bets\nIMPACT  │ (Do first)        │ (Plan carefully)\n        │                   │\n        ├───────────────────┼───────────────────\n        │                   │\nLOW     │ Fill-ins          │ Avoid\nIMPACT  │ (Do if easy)      │ (Don&#39;t bother)\n        │                   │\n        └───────────────────┴───────────────────\n              LOW EFFORT         HIGH EFFORT\n</code></pre>\n<h3>Create Phased Roadmap</h3>\n<p><strong>Phase 1: Quick Wins (Week 1-2)</strong></p>\n<ul>\n<li>[Opportunity 1]</li>\n<li>[Opportunity 2]</li>\n</ul>\n<p><strong>Phase 2: Core Integration (Month 1-2)</strong></p>\n<ul>\n<li>[Opportunity 3]</li>\n<li>[Opportunity 4]</li>\n</ul>\n<p><strong>Phase 3: Advanced (Quarter 1-2)</strong></p>\n<ul>\n<li>[Opportunity 5]</li>\n<li>[Opportunity 6]</li>\n</ul>\n<h3>Success Metrics</h3>\n<p>For each phase, define:</p>\n<ul>\n<li>What will be measured?</li>\n<li>What&#39;s the target improvement?</li>\n<li>How will you know it&#39;s working?</li>\n</ul>\n<hr>\n"
        },
        {
          "id": "part-6:-visualize-the-future-state-(5-minutes)",
          "title": "Part 6: Visualize the Future State (5 minutes)",
          "type": "generic",
          "content": "### Create Before/After Comparison\n\n**Before (Current State):**\n```\n[Step 1] → [Step 2] → [Step 3] → [Step 4] → ...\n  ↓          ↓          ↓          ↓\n[Pain]    [Pain]     [Pain]     [Pain]\n```\n\n**After (AI-Enhanced):**\n```\n[Step 1]    →   [AI: Step 2+3]   →   [Step 4]   → ...\n    ↓               ↓                   ↓\n[Improved]      [New AI]           [Enhanced]\n```\n\nShow which steps are:\n- Eliminated\n- Automated\n- Augmented\n- Unchanged\n\n---",
          "htmlContent": "<h3>Create Before/After Comparison</h3>\n<p><strong>Before (Current State):</strong></p>\n<pre><code>[Step 1] → [Step 2] → [Step 3] → [Step 4] → ...\n  ↓          ↓          ↓          ↓\n[Pain]    [Pain]     [Pain]     [Pain]\n</code></pre>\n<p><strong>After (AI-Enhanced):</strong></p>\n<pre><code>[Step 1]    →   [AI: Step 2+3]   →   [Step 4]   → ...\n    ↓               ↓                   ↓\n[Improved]      [New AI]           [Enhanced]\n</code></pre>\n<p>Show which steps are:</p>\n<ul>\n<li>Eliminated</li>\n<li>Automated</li>\n<li>Augmented</li>\n<li>Unchanged</li>\n</ul>\n<hr>\n"
        },
        {
          "id": "deliverable",
          "title": "Deliverable",
          "type": "generic",
          "content": "Create a comprehensive document containing:\n\n1. **Workflow Overview**\n   - Name and description\n   - Scope (start and end points)\n   - Key stakeholders\n\n2. **Current State Map**\n   - All steps with full documentation\n   - Bottleneck identification\n   - Pain points summary\n\n3. **AI Suitability Analysis**\n   - Scoring matrix\n   - Methodology notes\n\n4. **AI Opportunities**\n   - 4-6 detailed opportunity descriptions\n   - Integration type for each\n\n5. **Integration Roadmap**\n   - Prioritization matrix\n   - Phased plan\n   - Success metrics\n\n6. **Future State Vision**\n   - Before/after visualization\n   - Expected improvements\n\n---",
          "htmlContent": "<p>Create a comprehensive document containing:</p>\n<ol>\n<li><p><strong>Workflow Overview</strong></p>\n<ul>\n<li>Name and description</li>\n<li>Scope (start and end points)</li>\n<li>Key stakeholders</li>\n</ul>\n</li>\n<li><p><strong>Current State Map</strong></p>\n<ul>\n<li>All steps with full documentation</li>\n<li>Bottleneck identification</li>\n<li>Pain points summary</li>\n</ul>\n</li>\n<li><p><strong>AI Suitability Analysis</strong></p>\n<ul>\n<li>Scoring matrix</li>\n<li>Methodology notes</li>\n</ul>\n</li>\n<li><p><strong>AI Opportunities</strong></p>\n<ul>\n<li>4-6 detailed opportunity descriptions</li>\n<li>Integration type for each</li>\n</ul>\n</li>\n<li><p><strong>Integration Roadmap</strong></p>\n<ul>\n<li>Prioritization matrix</li>\n<li>Phased plan</li>\n<li>Success metrics</li>\n</ul>\n</li>\n<li><p><strong>Future State Vision</strong></p>\n<ul>\n<li>Before/after visualization</li>\n<li>Expected improvements</li>\n</ul>\n</li>\n</ol>\n<hr>\n"
        },
        {
          "id": "extension-challenge",
          "title": "Extension Challenge",
          "type": "generic",
          "content": "**Calculate the Business Case**\n\nFor your top AI opportunity:\n1. Estimate current cost (labor hours × rate)\n2. Estimate AI implementation cost\n3. Estimate ongoing AI cost\n4. Calculate ROI and payback period\n\nDocument your assumptions and calculations.",
          "htmlContent": "<p><strong>Calculate the Business Case</strong></p>\n<p>For your top AI opportunity:</p>\n<ol>\n<li>Estimate current cost (labor hours × rate)</li>\n<li>Estimate AI implementation cost</li>\n<li>Estimate ongoing AI cost</li>\n<li>Calculate ROI and payback period</li>\n</ol>\n<p>Document your assumptions and calculations.</p>\n"
        }
      ]
    },
    {
      "id": "lab-3b-agent-evaluation",
      "slug": "lab-3b-agent-evaluation",
      "title": "Agent Evaluation & Benchmarking",
      "phase": 2,
      "labNumber": 3.5,
      "estimatedMinutes": 75,
      "objectives": [
        "Build a golden test set for agent evaluation",
        "Define measurable success criteria and scoring rubrics",
        "Run systematic evaluations and measure consistency",
        "Document failure modes and propose improvements"
      ],
      "prerequisites": [
        "2.1-process-analysis",
        "2.2-task-decomposition",
        "lab-3-workflow-mapping"
      ],
      "content": "# Lab 3b: Agent Evaluation & Benchmarking\n\n## Lab Overview\n\nYou've designed AI workflows. But how do you know if they actually work?\n\n\"It seems good\" isn't good enough. This lab teaches you to systematically evaluate AI agents with the same rigor used by leading AI labs—but adapted for business applications.\n\n**What you'll create:**\n- A golden test set with input-output pairs\n- Evaluation criteria and scoring rubrics\n- Consistency measurements across multiple runs\n- Failure mode documentation with improvement recommendations\n\n---\n\n## Why Evaluation Matters\n\n> **\"If you can't measure it, you can't improve it.\"**\n\nAI evaluation differs from traditional software testing:\n\n| Traditional Software | AI Applications |\n|---------------------|-----------------|\n| Given input X, always produces output Y | Same input may produce different outputs |\n| Bugs are deterministic | Failures may be subtle (wrong tone, missed nuance) |\n| Edge cases are finite | Edge cases are infinite |\n| Pass/fail is binary | Quality is often subjective |\n\nWithout systematic evaluation, you're flying blind.\n\n---\n\n## The Agent Evaluation Matrix\n\nBefore testing, understand what you're measuring:\n\n| Dimension | What It Tests | Key Questions |\n|-----------|---------------|---------------|\n| **Task Success** | Goal completion | Did it do what was asked? |\n| **Output Quality** | Result correctness | Is the output accurate and useful? |\n| **Tool Use** | Capability selection | Did it use the right tools correctly? |\n| **Reasoning** | Decision quality | Was the logic sound? |\n| **Safety** | Boundary respect | Did it stay within scope? |\n| **Reliability** | Consistency | Does it work the same way each time? |\n\n---\n\n## Part 1: Define Success Criteria (20 minutes)\n\n### Step 1: Choose Your Evaluation Target\n\nSelect an AI workflow to evaluate. This could be:\n- An existing AI assistant or chatbot\n- A workflow you designed in Lab 3\n- A prompt chain you've built\n- A customer support automation\n\n**Document your target:**\n\n```\nEVALUATION TARGET\n─────────────────\nSystem name: ________________________________\nPurpose: ____________________________________\nKey tasks it performs:\n1. _________________________________________\n2. _________________________________________\n3. _________________________________________\n```\n\n### Step 2: Define Success Criteria\n\nFor each key task, define what \"success\" looks like:\n\n**Example for a customer support agent:**\n\n| Task | Success Criteria | Measurement |\n|------|------------------|-------------|\n| Answer product questions | Factually correct, references documentation | Manual verification against source |\n| Handle refund requests | Follows policy, captures required info | Checklist completion rate |\n| Escalate complex issues | Recognizes triggers, routes correctly | Escalation accuracy rate |\n\n**Your turn—define 5 success criteria:**\n\n```\nSUCCESS CRITERIA\n────────────────\nTask 1: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n\nTask 2: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n\nTask 3: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n\nTask 4: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n\nTask 5: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n```\n\n### Step 3: Create a Scoring Rubric\n\nConvert subjective criteria to scores:\n\n**Example rubric for \"response quality\":**\n\n| Score | Description | Indicators |\n|-------|-------------|------------|\n| 5 | Excellent | Complete, accurate, well-structured, appropriate tone |\n| 4 | Good | Mostly complete, minor omissions, good tone |\n| 3 | Acceptable | Addresses main point, some gaps, adequate tone |\n| 2 | Poor | Incomplete or partially incorrect, tone issues |\n| 1 | Failing | Wrong answer, inappropriate, or harmful |\n\n**Create rubrics for your top 3 criteria:**\n\n---\n\n## Part 2: Build Your Test Set (25 minutes)\n\n### Golden Examples (10 test cases)\n\nGolden examples are input-output pairs where you know what \"good\" looks like.\n\n**Structure:**\n\n```\nTEST CASE #___\n──────────────\nInput: [What the user says/submits]\nExpected output: [What a good response includes]\nSuccess criteria: [Which criteria this tests]\nEvaluation notes: [What to look for when scoring]\n```\n\n**Create 10 golden examples:**\n\n**Test Cases 1-4: Happy Path**\nStandard requests that should work perfectly.\n\n```\nTEST CASE #1 (Happy Path)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n```\nTEST CASE #2 (Happy Path)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n```\nTEST CASE #3 (Happy Path)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n```\nTEST CASE #4 (Happy Path)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n**Test Cases 5-7: Edge Cases**\nAmbiguous, unusual, or boundary-testing inputs.\n\n```\nTEST CASE #5 (Edge Case - Ambiguous)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n```\nTEST CASE #6 (Edge Case - Unusual)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n```\nTEST CASE #7 (Edge Case - Boundary)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n**Test Cases 8-9: Adversarial**\nAttempts to confuse, manipulate, or break the system.\n\n```\nTEST CASE #8 (Adversarial - Confusion)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n```\nTEST CASE #9 (Adversarial - Manipulation)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n**Test Case 10: Out of Scope**\nRequest the agent should refuse or escalate.\n\n```\nTEST CASE #10 (Out of Scope)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n---\n\n## Part 3: Run Evaluation (20 minutes)\n\n### Execute Your Test Suite\n\nRun each test case through your AI system. For each response:\n\n1. **Record the actual output**\n2. **Score against your rubric**\n3. **Note any unexpected behaviors**\n\n**Evaluation Log Template:**\n\n```\nTEST CASE #___\n──────────────\nInput: [copy from test set]\n\nActual Output:\n______________________________________________\n______________________________________________\n______________________________________________\n\nScores:\n  Criterion 1 (________): ___ / 5\n  Criterion 2 (________): ___ / 5\n  Criterion 3 (________): ___ / 5\n\nOverall: ___ / 5\n\nNotes:\n______________________________________________\n```\n\n### Measure Consistency\n\nRun 3 of your test cases **three times each**. Record variation:\n\n```\nCONSISTENCY CHECK\n─────────────────\nTest Case #___:\n  Run 1 score: ___\n  Run 2 score: ___\n  Run 3 score: ___\n  Variance: ___\n  Notes on differences: _______________________\n\nTest Case #___:\n  Run 1 score: ___\n  Run 2 score: ___\n  Run 3 score: ___\n  Variance: ___\n  Notes on differences: _______________________\n\nTest Case #___:\n  Run 1 score: ___\n  Run 2 score: ___\n  Run 3 score: ___\n  Variance: ___\n  Notes on differences: _______________________\n```\n\n---\n\n## Part 4: Document Failure Modes (10 minutes)\n\n### Failure Categorization\n\nReview your results and categorize any failures:\n\n| Failure Type | Description | Your Examples |\n|--------------|-------------|---------------|\n| **Wrong answer** | Factually incorrect output | |\n| **Incomplete** | Missing key information | |\n| **Wrong tool** | Used inappropriate capability | |\n| **Scope violation** | Acted outside boundaries | |\n| **Tone mismatch** | Inappropriate communication style | |\n| **Hallucination** | Made up information | |\n| **Inconsistency** | Different answers to same input | |\n\n### Pattern Analysis\n\nLook for patterns in your failures:\n\n```\nFAILURE PATTERNS\n────────────────\nMost common failure type: _______________________\nTriggers for failures: __________________________\nPercentage of tests with issues: _____%\n\nRoot cause hypotheses:\n1. ____________________________________________\n2. ____________________________________________\n3. ____________________________________________\n```\n\n### Improvement Recommendations\n\nBased on your analysis, what changes would improve performance?\n\n```\nRECOMMENDED IMPROVEMENTS\n────────────────────────\nPriority 1 (Critical):\n  Problem: _____________________________________\n  Solution: ____________________________________\n  Expected impact: _____________________________\n\nPriority 2 (Important):\n  Problem: _____________________________________\n  Solution: ____________________________________\n  Expected impact: _____________________________\n\nPriority 3 (Nice to have):\n  Problem: _____________________________________\n  Solution: ____________________________________\n  Expected impact: _____________________________\n```\n\n---\n\n## Lab Deliverable\n\nCompile your work into an **Agent Evaluation Report**:\n\n```\nAGENT EVALUATION REPORT\n═══════════════════════\n\nSystem Evaluated: _______________________________\nDate: __________________________________________\nEvaluator: _____________________________________\n\nEXECUTIVE SUMMARY\n─────────────────\nOverall score: ___ / 5\nTests passed: ___ / 10\nConsistency score: ___ (low variance = good)\nProduction ready: Yes / No / With changes\n\nKEY METRICS\n───────────\nTask success rate: ____%\nAverage quality score: ___ / 5\nFailure rate: ____%\nTop failure type: _______________\n\nDETAILED FINDINGS\n─────────────────\nStrengths:\n• ___________________________________________\n• ___________________________________________\n\nWeaknesses:\n• ___________________________________________\n• ___________________________________________\n\nRECOMMENDATIONS\n───────────────\n1. ___________________________________________\n2. ___________________________________________\n3. ___________________________________________\n\nAPPENDIX\n────────\n[Attach your test cases and evaluation logs]\n```\n\n---\n\n## Industry Benchmarks Reference\n\nFor context, here's how leading benchmarks evaluate agents:\n\n| Benchmark | What It Tests | Current Best Performance |\n|-----------|---------------|-------------------------|\n| **AgentBench** | 8 environments (OS, web, DB) | ~45% average |\n| **WebArena** | 812 web tasks | ~60% (humans: 78%) |\n| **GAIA** | 466 reasoning + tool tasks | Varies by difficulty |\n| **ToolEmu** | Safety in high-stakes scenarios | Focus on failure prevention |\n\nYour business evaluation doesn't need to match these formats—but understanding them helps you think systematically.\n\n---\n\n## Reflection Questions\n\n1. What surprised you most about your evaluation results?\n2. How would you explain these results to a non-technical stakeholder?\n3. What's the minimum acceptable score for production deployment?\n4. How often should you re-run this evaluation?\n5. What test cases would you add for the next evaluation cycle?\n\n---\n\n## Next Steps\n\nAfter completing this lab:\n- **Lab 4**: Design quality gates based on your evaluation criteria\n- **Module 3.4**: Learn deployment strategies that incorporate evaluation\n- **Lab 5**: Build an assistant with evaluation built-in",
      "htmlContent": "<h1>Lab 3b: Agent Evaluation &amp; Benchmarking</h1>\n<h2>Lab Overview</h2>\n<p>You&#39;ve designed AI workflows. But how do you know if they actually work?</p>\n<p>&quot;It seems good&quot; isn&#39;t good enough. This lab teaches you to systematically evaluate AI agents with the same rigor used by leading AI labs—but adapted for business applications.</p>\n<p><strong>What you&#39;ll create:</strong></p>\n<ul>\n<li>A golden test set with input-output pairs</li>\n<li>Evaluation criteria and scoring rubrics</li>\n<li>Consistency measurements across multiple runs</li>\n<li>Failure mode documentation with improvement recommendations</li>\n</ul>\n<hr>\n<h2>Why Evaluation Matters</h2>\n<blockquote>\n<p><strong>&quot;If you can&#39;t measure it, you can&#39;t improve it.&quot;</strong></p>\n</blockquote>\n<p>AI evaluation differs from traditional software testing:</p>\n<table>\n<thead>\n<tr>\n<th>Traditional Software</th>\n<th>AI Applications</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Given input X, always produces output Y</td>\n<td>Same input may produce different outputs</td>\n</tr>\n<tr>\n<td>Bugs are deterministic</td>\n<td>Failures may be subtle (wrong tone, missed nuance)</td>\n</tr>\n<tr>\n<td>Edge cases are finite</td>\n<td>Edge cases are infinite</td>\n</tr>\n<tr>\n<td>Pass/fail is binary</td>\n<td>Quality is often subjective</td>\n</tr>\n</tbody></table>\n<p>Without systematic evaluation, you&#39;re flying blind.</p>\n<hr>\n<h2>The Agent Evaluation Matrix</h2>\n<p>Before testing, understand what you&#39;re measuring:</p>\n<table>\n<thead>\n<tr>\n<th>Dimension</th>\n<th>What It Tests</th>\n<th>Key Questions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Task Success</strong></td>\n<td>Goal completion</td>\n<td>Did it do what was asked?</td>\n</tr>\n<tr>\n<td><strong>Output Quality</strong></td>\n<td>Result correctness</td>\n<td>Is the output accurate and useful?</td>\n</tr>\n<tr>\n<td><strong>Tool Use</strong></td>\n<td>Capability selection</td>\n<td>Did it use the right tools correctly?</td>\n</tr>\n<tr>\n<td><strong>Reasoning</strong></td>\n<td>Decision quality</td>\n<td>Was the logic sound?</td>\n</tr>\n<tr>\n<td><strong>Safety</strong></td>\n<td>Boundary respect</td>\n<td>Did it stay within scope?</td>\n</tr>\n<tr>\n<td><strong>Reliability</strong></td>\n<td>Consistency</td>\n<td>Does it work the same way each time?</td>\n</tr>\n</tbody></table>\n<hr>\n<h2>Part 1: Define Success Criteria (20 minutes)</h2>\n<h3>Step 1: Choose Your Evaluation Target</h3>\n<p>Select an AI workflow to evaluate. This could be:</p>\n<ul>\n<li>An existing AI assistant or chatbot</li>\n<li>A workflow you designed in Lab 3</li>\n<li>A prompt chain you&#39;ve built</li>\n<li>A customer support automation</li>\n</ul>\n<p><strong>Document your target:</strong></p>\n<pre><code>EVALUATION TARGET\n─────────────────\nSystem name: ________________________________\nPurpose: ____________________________________\nKey tasks it performs:\n1. _________________________________________\n2. _________________________________________\n3. _________________________________________\n</code></pre>\n<h3>Step 2: Define Success Criteria</h3>\n<p>For each key task, define what &quot;success&quot; looks like:</p>\n<p><strong>Example for a customer support agent:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th>Success Criteria</th>\n<th>Measurement</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Answer product questions</td>\n<td>Factually correct, references documentation</td>\n<td>Manual verification against source</td>\n</tr>\n<tr>\n<td>Handle refund requests</td>\n<td>Follows policy, captures required info</td>\n<td>Checklist completion rate</td>\n</tr>\n<tr>\n<td>Escalate complex issues</td>\n<td>Recognizes triggers, routes correctly</td>\n<td>Escalation accuracy rate</td>\n</tr>\n</tbody></table>\n<p><strong>Your turn—define 5 success criteria:</strong></p>\n<pre><code>SUCCESS CRITERIA\n────────────────\nTask 1: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n\nTask 2: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n\nTask 3: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n\nTask 4: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n\nTask 5: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n</code></pre>\n<h3>Step 3: Create a Scoring Rubric</h3>\n<p>Convert subjective criteria to scores:</p>\n<p><strong>Example rubric for &quot;response quality&quot;:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Score</th>\n<th>Description</th>\n<th>Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>5</td>\n<td>Excellent</td>\n<td>Complete, accurate, well-structured, appropriate tone</td>\n</tr>\n<tr>\n<td>4</td>\n<td>Good</td>\n<td>Mostly complete, minor omissions, good tone</td>\n</tr>\n<tr>\n<td>3</td>\n<td>Acceptable</td>\n<td>Addresses main point, some gaps, adequate tone</td>\n</tr>\n<tr>\n<td>2</td>\n<td>Poor</td>\n<td>Incomplete or partially incorrect, tone issues</td>\n</tr>\n<tr>\n<td>1</td>\n<td>Failing</td>\n<td>Wrong answer, inappropriate, or harmful</td>\n</tr>\n</tbody></table>\n<p><strong>Create rubrics for your top 3 criteria:</strong></p>\n<hr>\n<h2>Part 2: Build Your Test Set (25 minutes)</h2>\n<h3>Golden Examples (10 test cases)</h3>\n<p>Golden examples are input-output pairs where you know what &quot;good&quot; looks like.</p>\n<p><strong>Structure:</strong></p>\n<pre><code>TEST CASE #___\n──────────────\nInput: [What the user says/submits]\nExpected output: [What a good response includes]\nSuccess criteria: [Which criteria this tests]\nEvaluation notes: [What to look for when scoring]\n</code></pre>\n<p><strong>Create 10 golden examples:</strong></p>\n<p><strong>Test Cases 1-4: Happy Path</strong>\nStandard requests that should work perfectly.</p>\n<pre><code>TEST CASE #1 (Happy Path)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<pre><code>TEST CASE #2 (Happy Path)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<pre><code>TEST CASE #3 (Happy Path)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<pre><code>TEST CASE #4 (Happy Path)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<p><strong>Test Cases 5-7: Edge Cases</strong>\nAmbiguous, unusual, or boundary-testing inputs.</p>\n<pre><code>TEST CASE #5 (Edge Case - Ambiguous)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<pre><code>TEST CASE #6 (Edge Case - Unusual)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<pre><code>TEST CASE #7 (Edge Case - Boundary)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<p><strong>Test Cases 8-9: Adversarial</strong>\nAttempts to confuse, manipulate, or break the system.</p>\n<pre><code>TEST CASE #8 (Adversarial - Confusion)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<pre><code>TEST CASE #9 (Adversarial - Manipulation)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<p><strong>Test Case 10: Out of Scope</strong>\nRequest the agent should refuse or escalate.</p>\n<pre><code>TEST CASE #10 (Out of Scope)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<hr>\n<h2>Part 3: Run Evaluation (20 minutes)</h2>\n<h3>Execute Your Test Suite</h3>\n<p>Run each test case through your AI system. For each response:</p>\n<ol>\n<li><strong>Record the actual output</strong></li>\n<li><strong>Score against your rubric</strong></li>\n<li><strong>Note any unexpected behaviors</strong></li>\n</ol>\n<p><strong>Evaluation Log Template:</strong></p>\n<pre><code>TEST CASE #___\n──────────────\nInput: [copy from test set]\n\nActual Output:\n______________________________________________\n______________________________________________\n______________________________________________\n\nScores:\n  Criterion 1 (________): ___ / 5\n  Criterion 2 (________): ___ / 5\n  Criterion 3 (________): ___ / 5\n\nOverall: ___ / 5\n\nNotes:\n______________________________________________\n</code></pre>\n<h3>Measure Consistency</h3>\n<p>Run 3 of your test cases <strong>three times each</strong>. Record variation:</p>\n<pre><code>CONSISTENCY CHECK\n─────────────────\nTest Case #___:\n  Run 1 score: ___\n  Run 2 score: ___\n  Run 3 score: ___\n  Variance: ___\n  Notes on differences: _______________________\n\nTest Case #___:\n  Run 1 score: ___\n  Run 2 score: ___\n  Run 3 score: ___\n  Variance: ___\n  Notes on differences: _______________________\n\nTest Case #___:\n  Run 1 score: ___\n  Run 2 score: ___\n  Run 3 score: ___\n  Variance: ___\n  Notes on differences: _______________________\n</code></pre>\n<hr>\n<h2>Part 4: Document Failure Modes (10 minutes)</h2>\n<h3>Failure Categorization</h3>\n<p>Review your results and categorize any failures:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Type</th>\n<th>Description</th>\n<th>Your Examples</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Wrong answer</strong></td>\n<td>Factually incorrect output</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>Incomplete</strong></td>\n<td>Missing key information</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>Wrong tool</strong></td>\n<td>Used inappropriate capability</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>Scope violation</strong></td>\n<td>Acted outside boundaries</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>Tone mismatch</strong></td>\n<td>Inappropriate communication style</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>Hallucination</strong></td>\n<td>Made up information</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>Inconsistency</strong></td>\n<td>Different answers to same input</td>\n<td></td>\n</tr>\n</tbody></table>\n<h3>Pattern Analysis</h3>\n<p>Look for patterns in your failures:</p>\n<pre><code>FAILURE PATTERNS\n────────────────\nMost common failure type: _______________________\nTriggers for failures: __________________________\nPercentage of tests with issues: _____%\n\nRoot cause hypotheses:\n1. ____________________________________________\n2. ____________________________________________\n3. ____________________________________________\n</code></pre>\n<h3>Improvement Recommendations</h3>\n<p>Based on your analysis, what changes would improve performance?</p>\n<pre><code>RECOMMENDED IMPROVEMENTS\n────────────────────────\nPriority 1 (Critical):\n  Problem: _____________________________________\n  Solution: ____________________________________\n  Expected impact: _____________________________\n\nPriority 2 (Important):\n  Problem: _____________________________________\n  Solution: ____________________________________\n  Expected impact: _____________________________\n\nPriority 3 (Nice to have):\n  Problem: _____________________________________\n  Solution: ____________________________________\n  Expected impact: _____________________________\n</code></pre>\n<hr>\n<h2>Lab Deliverable</h2>\n<p>Compile your work into an <strong>Agent Evaluation Report</strong>:</p>\n<pre><code>AGENT EVALUATION REPORT\n═══════════════════════\n\nSystem Evaluated: _______________________________\nDate: __________________________________________\nEvaluator: _____________________________________\n\nEXECUTIVE SUMMARY\n─────────────────\nOverall score: ___ / 5\nTests passed: ___ / 10\nConsistency score: ___ (low variance = good)\nProduction ready: Yes / No / With changes\n\nKEY METRICS\n───────────\nTask success rate: ____%\nAverage quality score: ___ / 5\nFailure rate: ____%\nTop failure type: _______________\n\nDETAILED FINDINGS\n─────────────────\nStrengths:\n• ___________________________________________\n• ___________________________________________\n\nWeaknesses:\n• ___________________________________________\n• ___________________________________________\n\nRECOMMENDATIONS\n───────────────\n1. ___________________________________________\n2. ___________________________________________\n3. ___________________________________________\n\nAPPENDIX\n────────\n[Attach your test cases and evaluation logs]\n</code></pre>\n<hr>\n<h2>Industry Benchmarks Reference</h2>\n<p>For context, here&#39;s how leading benchmarks evaluate agents:</p>\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>What It Tests</th>\n<th>Current Best Performance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>AgentBench</strong></td>\n<td>8 environments (OS, web, DB)</td>\n<td>~45% average</td>\n</tr>\n<tr>\n<td><strong>WebArena</strong></td>\n<td>812 web tasks</td>\n<td>~60% (humans: 78%)</td>\n</tr>\n<tr>\n<td><strong>GAIA</strong></td>\n<td>466 reasoning + tool tasks</td>\n<td>Varies by difficulty</td>\n</tr>\n<tr>\n<td><strong>ToolEmu</strong></td>\n<td>Safety in high-stakes scenarios</td>\n<td>Focus on failure prevention</td>\n</tr>\n</tbody></table>\n<p>Your business evaluation doesn&#39;t need to match these formats—but understanding them helps you think systematically.</p>\n<hr>\n<h2>Reflection Questions</h2>\n<ol>\n<li>What surprised you most about your evaluation results?</li>\n<li>How would you explain these results to a non-technical stakeholder?</li>\n<li>What&#39;s the minimum acceptable score for production deployment?</li>\n<li>How often should you re-run this evaluation?</li>\n<li>What test cases would you add for the next evaluation cycle?</li>\n</ol>\n<hr>\n<h2>Next Steps</h2>\n<p>After completing this lab:</p>\n<ul>\n<li><strong>Lab 4</strong>: Design quality gates based on your evaluation criteria</li>\n<li><strong>Module 3.4</strong>: Learn deployment strategies that incorporate evaluation</li>\n<li><strong>Lab 5</strong>: Build an assistant with evaluation built-in</li>\n</ul>\n",
      "sections": [
        {
          "id": "lab-overview",
          "title": "Lab Overview",
          "type": "generic",
          "content": "You've designed AI workflows. But how do you know if they actually work?\n\n\"It seems good\" isn't good enough. This lab teaches you to systematically evaluate AI agents with the same rigor used by leading AI labs—but adapted for business applications.\n\n**What you'll create:**\n- A golden test set with input-output pairs\n- Evaluation criteria and scoring rubrics\n- Consistency measurements across multiple runs\n- Failure mode documentation with improvement recommendations\n\n---",
          "htmlContent": "<p>You&#39;ve designed AI workflows. But how do you know if they actually work?</p>\n<p>&quot;It seems good&quot; isn&#39;t good enough. This lab teaches you to systematically evaluate AI agents with the same rigor used by leading AI labs—but adapted for business applications.</p>\n<p><strong>What you&#39;ll create:</strong></p>\n<ul>\n<li>A golden test set with input-output pairs</li>\n<li>Evaluation criteria and scoring rubrics</li>\n<li>Consistency measurements across multiple runs</li>\n<li>Failure mode documentation with improvement recommendations</li>\n</ul>\n<hr>\n"
        },
        {
          "id": "why-evaluation-matters",
          "title": "Why Evaluation Matters",
          "type": "why",
          "content": "> **\"If you can't measure it, you can't improve it.\"**\n\nAI evaluation differs from traditional software testing:\n\n| Traditional Software | AI Applications |\n|---------------------|-----------------|\n| Given input X, always produces output Y | Same input may produce different outputs |\n| Bugs are deterministic | Failures may be subtle (wrong tone, missed nuance) |\n| Edge cases are finite | Edge cases are infinite |\n| Pass/fail is binary | Quality is often subjective |\n\nWithout systematic evaluation, you're flying blind.\n\n---",
          "htmlContent": "<blockquote>\n<p><strong>&quot;If you can&#39;t measure it, you can&#39;t improve it.&quot;</strong></p>\n</blockquote>\n<p>AI evaluation differs from traditional software testing:</p>\n<table>\n<thead>\n<tr>\n<th>Traditional Software</th>\n<th>AI Applications</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Given input X, always produces output Y</td>\n<td>Same input may produce different outputs</td>\n</tr>\n<tr>\n<td>Bugs are deterministic</td>\n<td>Failures may be subtle (wrong tone, missed nuance)</td>\n</tr>\n<tr>\n<td>Edge cases are finite</td>\n<td>Edge cases are infinite</td>\n</tr>\n<tr>\n<td>Pass/fail is binary</td>\n<td>Quality is often subjective</td>\n</tr>\n</tbody></table>\n<p>Without systematic evaluation, you&#39;re flying blind.</p>\n<hr>\n"
        },
        {
          "id": "the-agent-evaluation-matrix",
          "title": "The Agent Evaluation Matrix",
          "type": "generic",
          "content": "Before testing, understand what you're measuring:\n\n| Dimension | What It Tests | Key Questions |\n|-----------|---------------|---------------|\n| **Task Success** | Goal completion | Did it do what was asked? |\n| **Output Quality** | Result correctness | Is the output accurate and useful? |\n| **Tool Use** | Capability selection | Did it use the right tools correctly? |\n| **Reasoning** | Decision quality | Was the logic sound? |\n| **Safety** | Boundary respect | Did it stay within scope? |\n| **Reliability** | Consistency | Does it work the same way each time? |\n\n---",
          "htmlContent": "<p>Before testing, understand what you&#39;re measuring:</p>\n<table>\n<thead>\n<tr>\n<th>Dimension</th>\n<th>What It Tests</th>\n<th>Key Questions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Task Success</strong></td>\n<td>Goal completion</td>\n<td>Did it do what was asked?</td>\n</tr>\n<tr>\n<td><strong>Output Quality</strong></td>\n<td>Result correctness</td>\n<td>Is the output accurate and useful?</td>\n</tr>\n<tr>\n<td><strong>Tool Use</strong></td>\n<td>Capability selection</td>\n<td>Did it use the right tools correctly?</td>\n</tr>\n<tr>\n<td><strong>Reasoning</strong></td>\n<td>Decision quality</td>\n<td>Was the logic sound?</td>\n</tr>\n<tr>\n<td><strong>Safety</strong></td>\n<td>Boundary respect</td>\n<td>Did it stay within scope?</td>\n</tr>\n<tr>\n<td><strong>Reliability</strong></td>\n<td>Consistency</td>\n<td>Does it work the same way each time?</td>\n</tr>\n</tbody></table>\n<hr>\n"
        },
        {
          "id": "part-1:-define-success-criteria-(20-minutes)",
          "title": "Part 1: Define Success Criteria (20 minutes)",
          "type": "generic",
          "content": "### Step 1: Choose Your Evaluation Target\n\nSelect an AI workflow to evaluate. This could be:\n- An existing AI assistant or chatbot\n- A workflow you designed in Lab 3\n- A prompt chain you've built\n- A customer support automation\n\n**Document your target:**\n\n```\nEVALUATION TARGET\n─────────────────\nSystem name: ________________________________\nPurpose: ____________________________________\nKey tasks it performs:\n1. _________________________________________\n2. _________________________________________\n3. _________________________________________\n```\n\n### Step 2: Define Success Criteria\n\nFor each key task, define what \"success\" looks like:\n\n**Example for a customer support agent:**\n\n| Task | Success Criteria | Measurement |\n|------|------------------|-------------|\n| Answer product questions | Factually correct, references documentation | Manual verification against source |\n| Handle refund requests | Follows policy, captures required info | Checklist completion rate |\n| Escalate complex issues | Recognizes triggers, routes correctly | Escalation accuracy rate |\n\n**Your turn—define 5 success criteria:**\n\n```\nSUCCESS CRITERIA\n────────────────\nTask 1: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n\nTask 2: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n\nTask 3: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n\nTask 4: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n\nTask 5: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n```\n\n### Step 3: Create a Scoring Rubric\n\nConvert subjective criteria to scores:\n\n**Example rubric for \"response quality\":**\n\n| Score | Description | Indicators |\n|-------|-------------|------------|\n| 5 | Excellent | Complete, accurate, well-structured, appropriate tone |\n| 4 | Good | Mostly complete, minor omissions, good tone |\n| 3 | Acceptable | Addresses main point, some gaps, adequate tone |\n| 2 | Poor | Incomplete or partially incorrect, tone issues |\n| 1 | Failing | Wrong answer, inappropriate, or harmful |\n\n**Create rubrics for your top 3 criteria:**\n\n---",
          "htmlContent": "<h3>Step 1: Choose Your Evaluation Target</h3>\n<p>Select an AI workflow to evaluate. This could be:</p>\n<ul>\n<li>An existing AI assistant or chatbot</li>\n<li>A workflow you designed in Lab 3</li>\n<li>A prompt chain you&#39;ve built</li>\n<li>A customer support automation</li>\n</ul>\n<p><strong>Document your target:</strong></p>\n<pre><code>EVALUATION TARGET\n─────────────────\nSystem name: ________________________________\nPurpose: ____________________________________\nKey tasks it performs:\n1. _________________________________________\n2. _________________________________________\n3. _________________________________________\n</code></pre>\n<h3>Step 2: Define Success Criteria</h3>\n<p>For each key task, define what &quot;success&quot; looks like:</p>\n<p><strong>Example for a customer support agent:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th>Success Criteria</th>\n<th>Measurement</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Answer product questions</td>\n<td>Factually correct, references documentation</td>\n<td>Manual verification against source</td>\n</tr>\n<tr>\n<td>Handle refund requests</td>\n<td>Follows policy, captures required info</td>\n<td>Checklist completion rate</td>\n</tr>\n<tr>\n<td>Escalate complex issues</td>\n<td>Recognizes triggers, routes correctly</td>\n<td>Escalation accuracy rate</td>\n</tr>\n</tbody></table>\n<p><strong>Your turn—define 5 success criteria:</strong></p>\n<pre><code>SUCCESS CRITERIA\n────────────────\nTask 1: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n\nTask 2: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n\nTask 3: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n\nTask 4: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n\nTask 5: ____________________\n  Success means: ____________________\n  Measured by: ____________________\n</code></pre>\n<h3>Step 3: Create a Scoring Rubric</h3>\n<p>Convert subjective criteria to scores:</p>\n<p><strong>Example rubric for &quot;response quality&quot;:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Score</th>\n<th>Description</th>\n<th>Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>5</td>\n<td>Excellent</td>\n<td>Complete, accurate, well-structured, appropriate tone</td>\n</tr>\n<tr>\n<td>4</td>\n<td>Good</td>\n<td>Mostly complete, minor omissions, good tone</td>\n</tr>\n<tr>\n<td>3</td>\n<td>Acceptable</td>\n<td>Addresses main point, some gaps, adequate tone</td>\n</tr>\n<tr>\n<td>2</td>\n<td>Poor</td>\n<td>Incomplete or partially incorrect, tone issues</td>\n</tr>\n<tr>\n<td>1</td>\n<td>Failing</td>\n<td>Wrong answer, inappropriate, or harmful</td>\n</tr>\n</tbody></table>\n<p><strong>Create rubrics for your top 3 criteria:</strong></p>\n<hr>\n"
        },
        {
          "id": "part-2:-build-your-test-set-(25-minutes)",
          "title": "Part 2: Build Your Test Set (25 minutes)",
          "type": "generic",
          "content": "### Golden Examples (10 test cases)\n\nGolden examples are input-output pairs where you know what \"good\" looks like.\n\n**Structure:**\n\n```\nTEST CASE #___\n──────────────\nInput: [What the user says/submits]\nExpected output: [What a good response includes]\nSuccess criteria: [Which criteria this tests]\nEvaluation notes: [What to look for when scoring]\n```\n\n**Create 10 golden examples:**\n\n**Test Cases 1-4: Happy Path**\nStandard requests that should work perfectly.\n\n```\nTEST CASE #1 (Happy Path)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n```\nTEST CASE #2 (Happy Path)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n```\nTEST CASE #3 (Happy Path)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n```\nTEST CASE #4 (Happy Path)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n**Test Cases 5-7: Edge Cases**\nAmbiguous, unusual, or boundary-testing inputs.\n\n```\nTEST CASE #5 (Edge Case - Ambiguous)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n```\nTEST CASE #6 (Edge Case - Unusual)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n```\nTEST CASE #7 (Edge Case - Boundary)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n**Test Cases 8-9: Adversarial**\nAttempts to confuse, manipulate, or break the system.\n\n```\nTEST CASE #8 (Adversarial - Confusion)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n```\nTEST CASE #9 (Adversarial - Manipulation)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n**Test Case 10: Out of Scope**\nRequest the agent should refuse or escalate.\n\n```\nTEST CASE #10 (Out of Scope)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n```\n\n---",
          "htmlContent": "<h3>Golden Examples (10 test cases)</h3>\n<p>Golden examples are input-output pairs where you know what &quot;good&quot; looks like.</p>\n<p><strong>Structure:</strong></p>\n<pre><code>TEST CASE #___\n──────────────\nInput: [What the user says/submits]\nExpected output: [What a good response includes]\nSuccess criteria: [Which criteria this tests]\nEvaluation notes: [What to look for when scoring]\n</code></pre>\n<p><strong>Create 10 golden examples:</strong></p>\n<p><strong>Test Cases 1-4: Happy Path</strong>\nStandard requests that should work perfectly.</p>\n<pre><code>TEST CASE #1 (Happy Path)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<pre><code>TEST CASE #2 (Happy Path)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<pre><code>TEST CASE #3 (Happy Path)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<pre><code>TEST CASE #4 (Happy Path)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<p><strong>Test Cases 5-7: Edge Cases</strong>\nAmbiguous, unusual, or boundary-testing inputs.</p>\n<pre><code>TEST CASE #5 (Edge Case - Ambiguous)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<pre><code>TEST CASE #6 (Edge Case - Unusual)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<pre><code>TEST CASE #7 (Edge Case - Boundary)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<p><strong>Test Cases 8-9: Adversarial</strong>\nAttempts to confuse, manipulate, or break the system.</p>\n<pre><code>TEST CASE #8 (Adversarial - Confusion)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<pre><code>TEST CASE #9 (Adversarial - Manipulation)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<p><strong>Test Case 10: Out of Scope</strong>\nRequest the agent should refuse or escalate.</p>\n<pre><code>TEST CASE #10 (Out of Scope)\nInput: ________________________________________________\nExpected: _____________________________________________\nTests: ________________________________________________\n</code></pre>\n<hr>\n"
        },
        {
          "id": "part-3:-run-evaluation-(20-minutes)",
          "title": "Part 3: Run Evaluation (20 minutes)",
          "type": "generic",
          "content": "### Execute Your Test Suite\n\nRun each test case through your AI system. For each response:\n\n1. **Record the actual output**\n2. **Score against your rubric**\n3. **Note any unexpected behaviors**\n\n**Evaluation Log Template:**\n\n```\nTEST CASE #___\n──────────────\nInput: [copy from test set]\n\nActual Output:\n______________________________________________\n______________________________________________\n______________________________________________\n\nScores:\n  Criterion 1 (________): ___ / 5\n  Criterion 2 (________): ___ / 5\n  Criterion 3 (________): ___ / 5\n\nOverall: ___ / 5\n\nNotes:\n______________________________________________\n```\n\n### Measure Consistency\n\nRun 3 of your test cases **three times each**. Record variation:\n\n```\nCONSISTENCY CHECK\n─────────────────\nTest Case #___:\n  Run 1 score: ___\n  Run 2 score: ___\n  Run 3 score: ___\n  Variance: ___\n  Notes on differences: _______________________\n\nTest Case #___:\n  Run 1 score: ___\n  Run 2 score: ___\n  Run 3 score: ___\n  Variance: ___\n  Notes on differences: _______________________\n\nTest Case #___:\n  Run 1 score: ___\n  Run 2 score: ___\n  Run 3 score: ___\n  Variance: ___\n  Notes on differences: _______________________\n```\n\n---",
          "htmlContent": "<h3>Execute Your Test Suite</h3>\n<p>Run each test case through your AI system. For each response:</p>\n<ol>\n<li><strong>Record the actual output</strong></li>\n<li><strong>Score against your rubric</strong></li>\n<li><strong>Note any unexpected behaviors</strong></li>\n</ol>\n<p><strong>Evaluation Log Template:</strong></p>\n<pre><code>TEST CASE #___\n──────────────\nInput: [copy from test set]\n\nActual Output:\n______________________________________________\n______________________________________________\n______________________________________________\n\nScores:\n  Criterion 1 (________): ___ / 5\n  Criterion 2 (________): ___ / 5\n  Criterion 3 (________): ___ / 5\n\nOverall: ___ / 5\n\nNotes:\n______________________________________________\n</code></pre>\n<h3>Measure Consistency</h3>\n<p>Run 3 of your test cases <strong>three times each</strong>. Record variation:</p>\n<pre><code>CONSISTENCY CHECK\n─────────────────\nTest Case #___:\n  Run 1 score: ___\n  Run 2 score: ___\n  Run 3 score: ___\n  Variance: ___\n  Notes on differences: _______________________\n\nTest Case #___:\n  Run 1 score: ___\n  Run 2 score: ___\n  Run 3 score: ___\n  Variance: ___\n  Notes on differences: _______________________\n\nTest Case #___:\n  Run 1 score: ___\n  Run 2 score: ___\n  Run 3 score: ___\n  Variance: ___\n  Notes on differences: _______________________\n</code></pre>\n<hr>\n"
        },
        {
          "id": "part-4:-document-failure-modes-(10-minutes)",
          "title": "Part 4: Document Failure Modes (10 minutes)",
          "type": "generic",
          "content": "### Failure Categorization\n\nReview your results and categorize any failures:\n\n| Failure Type | Description | Your Examples |\n|--------------|-------------|---------------|\n| **Wrong answer** | Factually incorrect output | |\n| **Incomplete** | Missing key information | |\n| **Wrong tool** | Used inappropriate capability | |\n| **Scope violation** | Acted outside boundaries | |\n| **Tone mismatch** | Inappropriate communication style | |\n| **Hallucination** | Made up information | |\n| **Inconsistency** | Different answers to same input | |\n\n### Pattern Analysis\n\nLook for patterns in your failures:\n\n```\nFAILURE PATTERNS\n────────────────\nMost common failure type: _______________________\nTriggers for failures: __________________________\nPercentage of tests with issues: _____%\n\nRoot cause hypotheses:\n1. ____________________________________________\n2. ____________________________________________\n3. ____________________________________________\n```\n\n### Improvement Recommendations\n\nBased on your analysis, what changes would improve performance?\n\n```\nRECOMMENDED IMPROVEMENTS\n────────────────────────\nPriority 1 (Critical):\n  Problem: _____________________________________\n  Solution: ____________________________________\n  Expected impact: _____________________________\n\nPriority 2 (Important):\n  Problem: _____________________________________\n  Solution: ____________________________________\n  Expected impact: _____________________________\n\nPriority 3 (Nice to have):\n  Problem: _____________________________________\n  Solution: ____________________________________\n  Expected impact: _____________________________\n```\n\n---",
          "htmlContent": "<h3>Failure Categorization</h3>\n<p>Review your results and categorize any failures:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Type</th>\n<th>Description</th>\n<th>Your Examples</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Wrong answer</strong></td>\n<td>Factually incorrect output</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>Incomplete</strong></td>\n<td>Missing key information</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>Wrong tool</strong></td>\n<td>Used inappropriate capability</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>Scope violation</strong></td>\n<td>Acted outside boundaries</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>Tone mismatch</strong></td>\n<td>Inappropriate communication style</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>Hallucination</strong></td>\n<td>Made up information</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>Inconsistency</strong></td>\n<td>Different answers to same input</td>\n<td></td>\n</tr>\n</tbody></table>\n<h3>Pattern Analysis</h3>\n<p>Look for patterns in your failures:</p>\n<pre><code>FAILURE PATTERNS\n────────────────\nMost common failure type: _______________________\nTriggers for failures: __________________________\nPercentage of tests with issues: _____%\n\nRoot cause hypotheses:\n1. ____________________________________________\n2. ____________________________________________\n3. ____________________________________________\n</code></pre>\n<h3>Improvement Recommendations</h3>\n<p>Based on your analysis, what changes would improve performance?</p>\n<pre><code>RECOMMENDED IMPROVEMENTS\n────────────────────────\nPriority 1 (Critical):\n  Problem: _____________________________________\n  Solution: ____________________________________\n  Expected impact: _____________________________\n\nPriority 2 (Important):\n  Problem: _____________________________________\n  Solution: ____________________________________\n  Expected impact: _____________________________\n\nPriority 3 (Nice to have):\n  Problem: _____________________________________\n  Solution: ____________________________________\n  Expected impact: _____________________________\n</code></pre>\n<hr>\n"
        },
        {
          "id": "lab-deliverable",
          "title": "Lab Deliverable",
          "type": "generic",
          "content": "Compile your work into an **Agent Evaluation Report**:\n\n```\nAGENT EVALUATION REPORT\n═══════════════════════\n\nSystem Evaluated: _______________________________\nDate: __________________________________________\nEvaluator: _____________________________________\n\nEXECUTIVE SUMMARY\n─────────────────\nOverall score: ___ / 5\nTests passed: ___ / 10\nConsistency score: ___ (low variance = good)\nProduction ready: Yes / No / With changes\n\nKEY METRICS\n───────────\nTask success rate: ____%\nAverage quality score: ___ / 5\nFailure rate: ____%\nTop failure type: _______________\n\nDETAILED FINDINGS\n─────────────────\nStrengths:\n• ___________________________________________\n• ___________________________________________\n\nWeaknesses:\n• ___________________________________________\n• ___________________________________________\n\nRECOMMENDATIONS\n───────────────\n1. ___________________________________________\n2. ___________________________________________\n3. ___________________________________________\n\nAPPENDIX\n────────\n[Attach your test cases and evaluation logs]\n```\n\n---",
          "htmlContent": "<p>Compile your work into an <strong>Agent Evaluation Report</strong>:</p>\n<pre><code>AGENT EVALUATION REPORT\n═══════════════════════\n\nSystem Evaluated: _______________________________\nDate: __________________________________________\nEvaluator: _____________________________________\n\nEXECUTIVE SUMMARY\n─────────────────\nOverall score: ___ / 5\nTests passed: ___ / 10\nConsistency score: ___ (low variance = good)\nProduction ready: Yes / No / With changes\n\nKEY METRICS\n───────────\nTask success rate: ____%\nAverage quality score: ___ / 5\nFailure rate: ____%\nTop failure type: _______________\n\nDETAILED FINDINGS\n─────────────────\nStrengths:\n• ___________________________________________\n• ___________________________________________\n\nWeaknesses:\n• ___________________________________________\n• ___________________________________________\n\nRECOMMENDATIONS\n───────────────\n1. ___________________________________________\n2. ___________________________________________\n3. ___________________________________________\n\nAPPENDIX\n────────\n[Attach your test cases and evaluation logs]\n</code></pre>\n<hr>\n"
        },
        {
          "id": "industry-benchmarks-reference",
          "title": "Industry Benchmarks Reference",
          "type": "generic",
          "content": "For context, here's how leading benchmarks evaluate agents:\n\n| Benchmark | What It Tests | Current Best Performance |\n|-----------|---------------|-------------------------|\n| **AgentBench** | 8 environments (OS, web, DB) | ~45% average |\n| **WebArena** | 812 web tasks | ~60% (humans: 78%) |\n| **GAIA** | 466 reasoning + tool tasks | Varies by difficulty |\n| **ToolEmu** | Safety in high-stakes scenarios | Focus on failure prevention |\n\nYour business evaluation doesn't need to match these formats—but understanding them helps you think systematically.\n\n---",
          "htmlContent": "<p>For context, here&#39;s how leading benchmarks evaluate agents:</p>\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>What It Tests</th>\n<th>Current Best Performance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>AgentBench</strong></td>\n<td>8 environments (OS, web, DB)</td>\n<td>~45% average</td>\n</tr>\n<tr>\n<td><strong>WebArena</strong></td>\n<td>812 web tasks</td>\n<td>~60% (humans: 78%)</td>\n</tr>\n<tr>\n<td><strong>GAIA</strong></td>\n<td>466 reasoning + tool tasks</td>\n<td>Varies by difficulty</td>\n</tr>\n<tr>\n<td><strong>ToolEmu</strong></td>\n<td>Safety in high-stakes scenarios</td>\n<td>Focus on failure prevention</td>\n</tr>\n</tbody></table>\n<p>Your business evaluation doesn&#39;t need to match these formats—but understanding them helps you think systematically.</p>\n<hr>\n"
        },
        {
          "id": "reflection-questions",
          "title": "Reflection Questions",
          "type": "generic",
          "content": "1. What surprised you most about your evaluation results?\n2. How would you explain these results to a non-technical stakeholder?\n3. What's the minimum acceptable score for production deployment?\n4. How often should you re-run this evaluation?\n5. What test cases would you add for the next evaluation cycle?\n\n---",
          "htmlContent": "<ol>\n<li>What surprised you most about your evaluation results?</li>\n<li>How would you explain these results to a non-technical stakeholder?</li>\n<li>What&#39;s the minimum acceptable score for production deployment?</li>\n<li>How often should you re-run this evaluation?</li>\n<li>What test cases would you add for the next evaluation cycle?</li>\n</ol>\n<hr>\n"
        },
        {
          "id": "next-steps",
          "title": "Next Steps",
          "type": "generic",
          "content": "After completing this lab:\n- **Lab 4**: Design quality gates based on your evaluation criteria\n- **Module 3.4**: Learn deployment strategies that incorporate evaluation\n- **Lab 5**: Build an assistant with evaluation built-in",
          "htmlContent": "<p>After completing this lab:</p>\n<ul>\n<li><strong>Lab 4</strong>: Design quality gates based on your evaluation criteria</li>\n<li><strong>Module 3.4</strong>: Learn deployment strategies that incorporate evaluation</li>\n<li><strong>Lab 5</strong>: Build an assistant with evaluation built-in</li>\n</ul>\n"
        }
      ]
    },
    {
      "id": "lab-3b-data-hygiene",
      "slug": "lab-3b-data-hygiene",
      "title": "Data Hygiene & Preparation",
      "phase": 2,
      "labNumber": 3.5,
      "estimatedMinutes": 45,
      "objectives": [
        "Transform unstructured data into AI-ready formats",
        "Identify and handle common data quality issues",
        "Validate LLM-processed output for hallucinations",
        "Document a repeatable data preparation pipeline"
      ],
      "prerequisites": [
        "2.1-process-analysis",
        "2.2-task-decomposition"
      ],
      "content": "# Lab 3b: Data Hygiene & Preparation\n\n## Lab Overview\n\nAI doesn't fail because it's dumb—it fails because the input data is bad. In this lab, you'll confront the \"Garbage In, Garbage Out\" reality that derails most AI projects.\n\nYou'll take messy, real-world data and transform it into clean, structured formats that AI can actually use. This is the unglamorous but essential skill that separates successful AI operators from those who wonder why their automations keep breaking.\n\n**What you'll create:**\n- A cleaned, structured dataset from messy source material\n- A validation report identifying errors and hallucinations\n- A documented data preparation pipeline\n\n---\n\n## The Reality Check\n\nBefore we start, acknowledge this truth:\n\n> **80% of AI project time is spent on data preparation, not prompting.**\n\nMost AI tutorials skip this step. They give you clean JSON and pretend that's how data arrives. In reality, you'll face:\n- PDFs with inconsistent formatting\n- Emails with embedded tables that don't parse\n- Spreadsheets where \"N/A\" means five different things\n- Text files with encoding issues\n- Data that's technically correct but semantically wrong\n\nThis lab teaches you to handle the mess.\n\n---\n\n## Part 1: Assess the Damage (10 minutes)\n\n### Your Messy Dataset\n\nChoose one of these scenarios (or use your own real data):\n\n**Option A: Customer Feedback Dump**\n```\nEmail from: john.smith@company.com\nSubject: RE: RE: RE: Your Product\nDate: 3/15/24\n\nHonestly the product is great but shipping took FOREVER (like 2 weeks??)\nand the instructions were useless. Would rate 7/10 overall. Maybe 8 if\nyou fix the manual.\n\nBTW my order # was somewhere around 50234 or 50243, can't remember.\n\n---\nOriginal message from support@yourcompany.com:\nThank you for your purchase! How was your experience?\n---\n\nSent from my iPhone\n```\n\n**Option B: Meeting Notes Chaos**\n```\nMEETING - product review??? or was it planning\nAttendees: Sarah, Mike, that new guy (James?), Lisa was on zoom but kept cutting out\n\n- discussed Q4 roadmap (see attached... wait I don't think I attached it)\n- Mike wants to push feature X to Q1 \"for reasons\"\n- budget is either 50k or 500k depending on who you ask\n- ACTION: someone needs to follow up with finance (Lisa?)\n- Next meeting: soon\n\nNote: half of this was off the record so don't share with leadership\n```\n\n**Option C: Product Catalog Nightmare**\n```\nSKU: PRD-001 | Widget Pro | $49.99 | Blue, Red, Green | In Stock\nPRD-002,Widget Basic,$29.99,Blue only,backordered until ???\nSKU PRD-003 - Widget Mini - 19.99 - all colors - discontinued BUT still selling\nPRD004 | Gadget Max | fifty nine dollars | Chrome | In Stock (I think)\nprd-005;Gadget Mini;$39.99;Black;in stock;NEW ARRIVAL!!\n```\n\n**Option D: Your Real Data**\nUse an actual messy dataset from your work (sanitize any sensitive info).\n\n### Document the Problems\n\nCreate an inventory of data quality issues:\n\n| Issue Type | Example from Data | Frequency | Severity |\n|------------|-------------------|-----------|----------|\n| Inconsistent formatting | | | High/Med/Low |\n| Missing values | | | |\n| Ambiguous content | | | |\n| Mixed data types | | | |\n| Embedded noise | | | |\n| Encoding issues | | | |\n\nIdentify at least 5 distinct problems.\n\n---\n\n## Part 2: Design Your Target Schema (5 minutes)\n\n### Define What \"Clean\" Looks Like\n\nBefore cleaning, decide your destination. Create a target schema:\n\n**For Customer Feedback:**\n```json\n{\n  \"feedback_id\": \"string (generated)\",\n  \"customer_email\": \"string (valid email)\",\n  \"date\": \"YYYY-MM-DD\",\n  \"order_number\": \"integer or null\",\n  \"rating\": \"integer 1-10 or null\",\n  \"sentiment\": \"positive | negative | mixed | neutral\",\n  \"shipping_feedback\": \"string or null\",\n  \"product_feedback\": \"string or null\",\n  \"suggestions\": [\"array of strings\"],\n  \"confidence_score\": \"float 0-1\"\n}\n```\n\n**For Meeting Notes:**\n```json\n{\n  \"meeting_id\": \"string (generated)\",\n  \"date\": \"YYYY-MM-DD or null\",\n  \"meeting_type\": \"string\",\n  \"attendees\": [\"array of names\"],\n  \"decisions\": [\"array of strings\"],\n  \"action_items\": [\n    {\n      \"task\": \"string\",\n      \"owner\": \"string or null\",\n      \"deadline\": \"string or null\"\n    }\n  ],\n  \"confidential_flag\": \"boolean\",\n  \"notes\": \"string\"\n}\n```\n\n### Document Your Schema\n\nWrite out:\n1. Every field name\n2. Expected data type\n3. Required vs. optional\n4. Validation rules (if any)\n5. What to do when data is missing\n\n---\n\n## Part 3: Build Your Cleaning Prompt (15 minutes)\n\n### The Transformation Prompt\n\nCreate a prompt that transforms messy input into your clean schema.\n\n**Prompt Template:**\n```\nYou are a data cleaning specialist. Transform the following unstructured\ntext into the specified JSON format.\n\nRULES:\n1. Extract only information that is explicitly stated\n2. Use null for missing or unclear values—NEVER guess\n3. Flag low-confidence extractions with confidence_score < 0.7\n4. Preserve original meaning—do not interpret or summarize unless specified\n5. If a value could be multiple things, choose the most likely and note alternatives\n\nTARGET SCHEMA:\n[Your schema here]\n\nHANDLING AMBIGUITY:\n- Dates: If format unclear, use ISO 8601 (YYYY-MM-DD) and note original\n- Numbers: If ranges given, use midpoint and note range\n- Names: Use as written, even if likely misspelled\n- Missing data: Use null, not empty string or \"unknown\"\n\nINPUT DATA:\n\"\"\"\n[Messy data here]\n\"\"\"\n\nOUTPUT:\nProvide valid JSON only. No explanations before or after.\n```\n\n### Test and Iterate\n\n1. Run your prompt on the messy data\n2. Examine the output critically\n3. Identify where it went wrong\n4. Refine your prompt\n5. Repeat until output is acceptable\n\n### Document Your Iterations\n\n| Iteration | Problem Found | Prompt Change Made | Result |\n|-----------|---------------|-------------------|--------|\n| 1 | | | |\n| 2 | | | |\n| 3 | | | |\n\n---\n\n## Part 4: Validate for Hallucinations (10 minutes)\n\n### The Critical Step Most People Skip\n\nLLMs will confidently produce structured output even when they're making things up. You MUST validate.\n\n### Validation Checklist\n\nFor each field in your output, verify:\n\n| Field | Source Text Evidence | Extraction Correct? | Hallucination Risk |\n|-------|---------------------|--------------------|--------------------|\n| | Quote the original | Yes/No/Partial | High/Med/Low |\n| | | | |\n\n### Common Hallucination Patterns\n\nWatch for these:\n- **Fabricated specifics**: LLM adds precise numbers that weren't in source\n- **Over-inference**: LLM concludes things that aren't stated\n- **Format conformity**: LLM forces data into schema even when it shouldn't fit\n- **Plausible defaults**: LLM uses reasonable-sounding values instead of null\n\n### Create a Validation Prompt\n\n```\nReview this extraction for accuracy. Compare the OUTPUT against the SOURCE.\n\nSOURCE:\n\"\"\"\n[Original messy data]\n\"\"\"\n\nEXTRACTED OUTPUT:\n[Your cleaned JSON]\n\nFor each field, answer:\n1. Is there explicit evidence in the source? Quote it.\n2. Is the extraction accurate to the source?\n3. Was anything added that isn't in the source?\n4. Confidence level: HIGH (direct quote) / MEDIUM (reasonable inference) / LOW (uncertain) / HALLUCINATION (not in source)\n\nFormat as a validation report.\n```\n\n### Document Findings\n\nHow many fields were:\n- Correctly extracted: ___\n- Partially correct: ___\n- Hallucinated: ___\n- Appropriately null: ___\n\n---\n\n## Part 5: Build the Pipeline (5 minutes)\n\n### Document Your Process\n\nCreate a repeatable pipeline:\n\n```\nDATA PREPARATION PIPELINE: [Name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nINPUT FORMAT:\n- Source type: [Email / Document / Spreadsheet / etc.]\n- Expected volume: [Records per batch]\n- Update frequency: [One-time / Daily / Real-time]\n\nSTEP 1: PRE-PROCESSING\n- [ ] Remove headers/footers/signatures\n- [ ] Standardize encoding (UTF-8)\n- [ ] Split into individual records\n- [ ] [Other pre-processing]\n\nSTEP 2: LLM TRANSFORMATION\n- Model: [GPT-4 / Claude / etc.]\n- Prompt version: [Link or paste]\n- Temperature: [Recommended: 0 for extraction]\n- Expected tokens: [Estimate]\n\nSTEP 3: VALIDATION\n- [ ] Run validation prompt\n- [ ] Check for hallucinations\n- [ ] Verify required fields present\n- [ ] Spot-check [N] random records manually\n\nSTEP 4: POST-PROCESSING\n- [ ] Parse JSON\n- [ ] Apply business rules\n- [ ] Handle validation failures\n- [ ] Load to destination\n\nERROR HANDLING:\n- If parsing fails: [Action]\n- If validation fails: [Action]\n- If confidence < threshold: [Action]\n\nESTIMATED COST PER RECORD:\n- Input tokens: ~[X]\n- Output tokens: ~[X]\n- Cost at current rates: $[X]\n```\n\n---\n\n## Deliverable\n\nCreate a comprehensive document containing:\n\n1. **Problem Assessment**\n   - Original messy data sample\n   - Inventory of data quality issues\n   - Severity ratings\n\n2. **Target Schema**\n   - Complete field definitions\n   - Validation rules\n   - Missing data handling\n\n3. **Transformation Prompt**\n   - Final working prompt\n   - Iteration history\n   - Known limitations\n\n4. **Validation Report**\n   - Sample validation results\n   - Hallucination analysis\n   - Accuracy metrics\n\n5. **Pipeline Documentation**\n   - Step-by-step process\n   - Error handling\n   - Cost estimates\n\n6. **Lessons Learned**\n   - What surprised you?\n   - What would you do differently?\n   - Recommendations for similar data\n\n---\n\n## Extension Challenge\n\n**Scale It Up**\n\n1. Take 10 records of the same messy data type\n2. Process them through your pipeline\n3. Calculate:\n   - Average processing time\n   - Cost per record\n   - Accuracy rate\n   - Failure rate\n\n4. Identify:\n   - Which data patterns cause problems?\n   - Where does the pipeline break?\n   - What manual intervention is still needed?\n\nDocument your findings and propose pipeline improvements.",
      "htmlContent": "<h1>Lab 3b: Data Hygiene &amp; Preparation</h1>\n<h2>Lab Overview</h2>\n<p>AI doesn&#39;t fail because it&#39;s dumb—it fails because the input data is bad. In this lab, you&#39;ll confront the &quot;Garbage In, Garbage Out&quot; reality that derails most AI projects.</p>\n<p>You&#39;ll take messy, real-world data and transform it into clean, structured formats that AI can actually use. This is the unglamorous but essential skill that separates successful AI operators from those who wonder why their automations keep breaking.</p>\n<p><strong>What you&#39;ll create:</strong></p>\n<ul>\n<li>A cleaned, structured dataset from messy source material</li>\n<li>A validation report identifying errors and hallucinations</li>\n<li>A documented data preparation pipeline</li>\n</ul>\n<hr>\n<h2>The Reality Check</h2>\n<p>Before we start, acknowledge this truth:</p>\n<blockquote>\n<p><strong>80% of AI project time is spent on data preparation, not prompting.</strong></p>\n</blockquote>\n<p>Most AI tutorials skip this step. They give you clean JSON and pretend that&#39;s how data arrives. In reality, you&#39;ll face:</p>\n<ul>\n<li>PDFs with inconsistent formatting</li>\n<li>Emails with embedded tables that don&#39;t parse</li>\n<li>Spreadsheets where &quot;N/A&quot; means five different things</li>\n<li>Text files with encoding issues</li>\n<li>Data that&#39;s technically correct but semantically wrong</li>\n</ul>\n<p>This lab teaches you to handle the mess.</p>\n<hr>\n<h2>Part 1: Assess the Damage (10 minutes)</h2>\n<h3>Your Messy Dataset</h3>\n<p>Choose one of these scenarios (or use your own real data):</p>\n<p><strong>Option A: Customer Feedback Dump</strong></p>\n<pre><code>Email from: john.smith@company.com\nSubject: RE: RE: RE: Your Product\nDate: 3/15/24\n\nHonestly the product is great but shipping took FOREVER (like 2 weeks??)\nand the instructions were useless. Would rate 7/10 overall. Maybe 8 if\nyou fix the manual.\n\nBTW my order # was somewhere around 50234 or 50243, can&#39;t remember.\n\n---\nOriginal message from support@yourcompany.com:\nThank you for your purchase! How was your experience?\n---\n\nSent from my iPhone\n</code></pre>\n<p><strong>Option B: Meeting Notes Chaos</strong></p>\n<pre><code>MEETING - product review??? or was it planning\nAttendees: Sarah, Mike, that new guy (James?), Lisa was on zoom but kept cutting out\n\n- discussed Q4 roadmap (see attached... wait I don&#39;t think I attached it)\n- Mike wants to push feature X to Q1 &quot;for reasons&quot;\n- budget is either 50k or 500k depending on who you ask\n- ACTION: someone needs to follow up with finance (Lisa?)\n- Next meeting: soon\n\nNote: half of this was off the record so don&#39;t share with leadership\n</code></pre>\n<p><strong>Option C: Product Catalog Nightmare</strong></p>\n<pre><code>SKU: PRD-001 | Widget Pro | $49.99 | Blue, Red, Green | In Stock\nPRD-002,Widget Basic,$29.99,Blue only,backordered until ???\nSKU PRD-003 - Widget Mini - 19.99 - all colors - discontinued BUT still selling\nPRD004 | Gadget Max | fifty nine dollars | Chrome | In Stock (I think)\nprd-005;Gadget Mini;$39.99;Black;in stock;NEW ARRIVAL!!\n</code></pre>\n<p><strong>Option D: Your Real Data</strong>\nUse an actual messy dataset from your work (sanitize any sensitive info).</p>\n<h3>Document the Problems</h3>\n<p>Create an inventory of data quality issues:</p>\n<table>\n<thead>\n<tr>\n<th>Issue Type</th>\n<th>Example from Data</th>\n<th>Frequency</th>\n<th>Severity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Inconsistent formatting</td>\n<td></td>\n<td></td>\n<td>High/Med/Low</td>\n</tr>\n<tr>\n<td>Missing values</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Ambiguous content</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Mixed data types</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Embedded noise</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Encoding issues</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<p>Identify at least 5 distinct problems.</p>\n<hr>\n<h2>Part 2: Design Your Target Schema (5 minutes)</h2>\n<h3>Define What &quot;Clean&quot; Looks Like</h3>\n<p>Before cleaning, decide your destination. Create a target schema:</p>\n<p><strong>For Customer Feedback:</strong></p>\n<pre><code class=\"language-json\">{\n  &quot;feedback_id&quot;: &quot;string (generated)&quot;,\n  &quot;customer_email&quot;: &quot;string (valid email)&quot;,\n  &quot;date&quot;: &quot;YYYY-MM-DD&quot;,\n  &quot;order_number&quot;: &quot;integer or null&quot;,\n  &quot;rating&quot;: &quot;integer 1-10 or null&quot;,\n  &quot;sentiment&quot;: &quot;positive | negative | mixed | neutral&quot;,\n  &quot;shipping_feedback&quot;: &quot;string or null&quot;,\n  &quot;product_feedback&quot;: &quot;string or null&quot;,\n  &quot;suggestions&quot;: [&quot;array of strings&quot;],\n  &quot;confidence_score&quot;: &quot;float 0-1&quot;\n}\n</code></pre>\n<p><strong>For Meeting Notes:</strong></p>\n<pre><code class=\"language-json\">{\n  &quot;meeting_id&quot;: &quot;string (generated)&quot;,\n  &quot;date&quot;: &quot;YYYY-MM-DD or null&quot;,\n  &quot;meeting_type&quot;: &quot;string&quot;,\n  &quot;attendees&quot;: [&quot;array of names&quot;],\n  &quot;decisions&quot;: [&quot;array of strings&quot;],\n  &quot;action_items&quot;: [\n    {\n      &quot;task&quot;: &quot;string&quot;,\n      &quot;owner&quot;: &quot;string or null&quot;,\n      &quot;deadline&quot;: &quot;string or null&quot;\n    }\n  ],\n  &quot;confidential_flag&quot;: &quot;boolean&quot;,\n  &quot;notes&quot;: &quot;string&quot;\n}\n</code></pre>\n<h3>Document Your Schema</h3>\n<p>Write out:</p>\n<ol>\n<li>Every field name</li>\n<li>Expected data type</li>\n<li>Required vs. optional</li>\n<li>Validation rules (if any)</li>\n<li>What to do when data is missing</li>\n</ol>\n<hr>\n<h2>Part 3: Build Your Cleaning Prompt (15 minutes)</h2>\n<h3>The Transformation Prompt</h3>\n<p>Create a prompt that transforms messy input into your clean schema.</p>\n<p><strong>Prompt Template:</strong></p>\n<pre><code>You are a data cleaning specialist. Transform the following unstructured\ntext into the specified JSON format.\n\nRULES:\n1. Extract only information that is explicitly stated\n2. Use null for missing or unclear values—NEVER guess\n3. Flag low-confidence extractions with confidence_score &lt; 0.7\n4. Preserve original meaning—do not interpret or summarize unless specified\n5. If a value could be multiple things, choose the most likely and note alternatives\n\nTARGET SCHEMA:\n[Your schema here]\n\nHANDLING AMBIGUITY:\n- Dates: If format unclear, use ISO 8601 (YYYY-MM-DD) and note original\n- Numbers: If ranges given, use midpoint and note range\n- Names: Use as written, even if likely misspelled\n- Missing data: Use null, not empty string or &quot;unknown&quot;\n\nINPUT DATA:\n&quot;&quot;&quot;\n[Messy data here]\n&quot;&quot;&quot;\n\nOUTPUT:\nProvide valid JSON only. No explanations before or after.\n</code></pre>\n<h3>Test and Iterate</h3>\n<ol>\n<li>Run your prompt on the messy data</li>\n<li>Examine the output critically</li>\n<li>Identify where it went wrong</li>\n<li>Refine your prompt</li>\n<li>Repeat until output is acceptable</li>\n</ol>\n<h3>Document Your Iterations</h3>\n<table>\n<thead>\n<tr>\n<th>Iteration</th>\n<th>Problem Found</th>\n<th>Prompt Change Made</th>\n<th>Result</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>2</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>3</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<hr>\n<h2>Part 4: Validate for Hallucinations (10 minutes)</h2>\n<h3>The Critical Step Most People Skip</h3>\n<p>LLMs will confidently produce structured output even when they&#39;re making things up. You MUST validate.</p>\n<h3>Validation Checklist</h3>\n<p>For each field in your output, verify:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Source Text Evidence</th>\n<th>Extraction Correct?</th>\n<th>Hallucination Risk</th>\n</tr>\n</thead>\n<tbody><tr>\n<td></td>\n<td>Quote the original</td>\n<td>Yes/No/Partial</td>\n<td>High/Med/Low</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<h3>Common Hallucination Patterns</h3>\n<p>Watch for these:</p>\n<ul>\n<li><strong>Fabricated specifics</strong>: LLM adds precise numbers that weren&#39;t in source</li>\n<li><strong>Over-inference</strong>: LLM concludes things that aren&#39;t stated</li>\n<li><strong>Format conformity</strong>: LLM forces data into schema even when it shouldn&#39;t fit</li>\n<li><strong>Plausible defaults</strong>: LLM uses reasonable-sounding values instead of null</li>\n</ul>\n<h3>Create a Validation Prompt</h3>\n<pre><code>Review this extraction for accuracy. Compare the OUTPUT against the SOURCE.\n\nSOURCE:\n&quot;&quot;&quot;\n[Original messy data]\n&quot;&quot;&quot;\n\nEXTRACTED OUTPUT:\n[Your cleaned JSON]\n\nFor each field, answer:\n1. Is there explicit evidence in the source? Quote it.\n2. Is the extraction accurate to the source?\n3. Was anything added that isn&#39;t in the source?\n4. Confidence level: HIGH (direct quote) / MEDIUM (reasonable inference) / LOW (uncertain) / HALLUCINATION (not in source)\n\nFormat as a validation report.\n</code></pre>\n<h3>Document Findings</h3>\n<p>How many fields were:</p>\n<ul>\n<li>Correctly extracted: ___</li>\n<li>Partially correct: ___</li>\n<li>Hallucinated: ___</li>\n<li>Appropriately null: ___</li>\n</ul>\n<hr>\n<h2>Part 5: Build the Pipeline (5 minutes)</h2>\n<h3>Document Your Process</h3>\n<p>Create a repeatable pipeline:</p>\n<pre><code>DATA PREPARATION PIPELINE: [Name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nINPUT FORMAT:\n- Source type: [Email / Document / Spreadsheet / etc.]\n- Expected volume: [Records per batch]\n- Update frequency: [One-time / Daily / Real-time]\n\nSTEP 1: PRE-PROCESSING\n- [ ] Remove headers/footers/signatures\n- [ ] Standardize encoding (UTF-8)\n- [ ] Split into individual records\n- [ ] [Other pre-processing]\n\nSTEP 2: LLM TRANSFORMATION\n- Model: [GPT-4 / Claude / etc.]\n- Prompt version: [Link or paste]\n- Temperature: [Recommended: 0 for extraction]\n- Expected tokens: [Estimate]\n\nSTEP 3: VALIDATION\n- [ ] Run validation prompt\n- [ ] Check for hallucinations\n- [ ] Verify required fields present\n- [ ] Spot-check [N] random records manually\n\nSTEP 4: POST-PROCESSING\n- [ ] Parse JSON\n- [ ] Apply business rules\n- [ ] Handle validation failures\n- [ ] Load to destination\n\nERROR HANDLING:\n- If parsing fails: [Action]\n- If validation fails: [Action]\n- If confidence &lt; threshold: [Action]\n\nESTIMATED COST PER RECORD:\n- Input tokens: ~[X]\n- Output tokens: ~[X]\n- Cost at current rates: $[X]\n</code></pre>\n<hr>\n<h2>Deliverable</h2>\n<p>Create a comprehensive document containing:</p>\n<ol>\n<li><p><strong>Problem Assessment</strong></p>\n<ul>\n<li>Original messy data sample</li>\n<li>Inventory of data quality issues</li>\n<li>Severity ratings</li>\n</ul>\n</li>\n<li><p><strong>Target Schema</strong></p>\n<ul>\n<li>Complete field definitions</li>\n<li>Validation rules</li>\n<li>Missing data handling</li>\n</ul>\n</li>\n<li><p><strong>Transformation Prompt</strong></p>\n<ul>\n<li>Final working prompt</li>\n<li>Iteration history</li>\n<li>Known limitations</li>\n</ul>\n</li>\n<li><p><strong>Validation Report</strong></p>\n<ul>\n<li>Sample validation results</li>\n<li>Hallucination analysis</li>\n<li>Accuracy metrics</li>\n</ul>\n</li>\n<li><p><strong>Pipeline Documentation</strong></p>\n<ul>\n<li>Step-by-step process</li>\n<li>Error handling</li>\n<li>Cost estimates</li>\n</ul>\n</li>\n<li><p><strong>Lessons Learned</strong></p>\n<ul>\n<li>What surprised you?</li>\n<li>What would you do differently?</li>\n<li>Recommendations for similar data</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h2>Extension Challenge</h2>\n<p><strong>Scale It Up</strong></p>\n<ol>\n<li><p>Take 10 records of the same messy data type</p>\n</li>\n<li><p>Process them through your pipeline</p>\n</li>\n<li><p>Calculate:</p>\n<ul>\n<li>Average processing time</li>\n<li>Cost per record</li>\n<li>Accuracy rate</li>\n<li>Failure rate</li>\n</ul>\n</li>\n<li><p>Identify:</p>\n<ul>\n<li>Which data patterns cause problems?</li>\n<li>Where does the pipeline break?</li>\n<li>What manual intervention is still needed?</li>\n</ul>\n</li>\n</ol>\n<p>Document your findings and propose pipeline improvements.</p>\n",
      "sections": [
        {
          "id": "lab-overview",
          "title": "Lab Overview",
          "type": "generic",
          "content": "AI doesn't fail because it's dumb—it fails because the input data is bad. In this lab, you'll confront the \"Garbage In, Garbage Out\" reality that derails most AI projects.\n\nYou'll take messy, real-world data and transform it into clean, structured formats that AI can actually use. This is the unglamorous but essential skill that separates successful AI operators from those who wonder why their automations keep breaking.\n\n**What you'll create:**\n- A cleaned, structured dataset from messy source material\n- A validation report identifying errors and hallucinations\n- A documented data preparation pipeline\n\n---",
          "htmlContent": "<p>AI doesn&#39;t fail because it&#39;s dumb—it fails because the input data is bad. In this lab, you&#39;ll confront the &quot;Garbage In, Garbage Out&quot; reality that derails most AI projects.</p>\n<p>You&#39;ll take messy, real-world data and transform it into clean, structured formats that AI can actually use. This is the unglamorous but essential skill that separates successful AI operators from those who wonder why their automations keep breaking.</p>\n<p><strong>What you&#39;ll create:</strong></p>\n<ul>\n<li>A cleaned, structured dataset from messy source material</li>\n<li>A validation report identifying errors and hallucinations</li>\n<li>A documented data preparation pipeline</li>\n</ul>\n<hr>\n"
        },
        {
          "id": "the-reality-check",
          "title": "The Reality Check",
          "type": "generic",
          "content": "Before we start, acknowledge this truth:\n\n> **80% of AI project time is spent on data preparation, not prompting.**\n\nMost AI tutorials skip this step. They give you clean JSON and pretend that's how data arrives. In reality, you'll face:\n- PDFs with inconsistent formatting\n- Emails with embedded tables that don't parse\n- Spreadsheets where \"N/A\" means five different things\n- Text files with encoding issues\n- Data that's technically correct but semantically wrong\n\nThis lab teaches you to handle the mess.\n\n---",
          "htmlContent": "<p>Before we start, acknowledge this truth:</p>\n<blockquote>\n<p><strong>80% of AI project time is spent on data preparation, not prompting.</strong></p>\n</blockquote>\n<p>Most AI tutorials skip this step. They give you clean JSON and pretend that&#39;s how data arrives. In reality, you&#39;ll face:</p>\n<ul>\n<li>PDFs with inconsistent formatting</li>\n<li>Emails with embedded tables that don&#39;t parse</li>\n<li>Spreadsheets where &quot;N/A&quot; means five different things</li>\n<li>Text files with encoding issues</li>\n<li>Data that&#39;s technically correct but semantically wrong</li>\n</ul>\n<p>This lab teaches you to handle the mess.</p>\n<hr>\n"
        },
        {
          "id": "part-1:-assess-the-damage-(10-minutes)",
          "title": "Part 1: Assess the Damage (10 minutes)",
          "type": "generic",
          "content": "### Your Messy Dataset\n\nChoose one of these scenarios (or use your own real data):\n\n**Option A: Customer Feedback Dump**\n```\nEmail from: john.smith@company.com\nSubject: RE: RE: RE: Your Product\nDate: 3/15/24\n\nHonestly the product is great but shipping took FOREVER (like 2 weeks??)\nand the instructions were useless. Would rate 7/10 overall. Maybe 8 if\nyou fix the manual.\n\nBTW my order # was somewhere around 50234 or 50243, can't remember.\n\n---\nOriginal message from support@yourcompany.com:\nThank you for your purchase! How was your experience?\n---\n\nSent from my iPhone\n```\n\n**Option B: Meeting Notes Chaos**\n```\nMEETING - product review??? or was it planning\nAttendees: Sarah, Mike, that new guy (James?), Lisa was on zoom but kept cutting out\n\n- discussed Q4 roadmap (see attached... wait I don't think I attached it)\n- Mike wants to push feature X to Q1 \"for reasons\"\n- budget is either 50k or 500k depending on who you ask\n- ACTION: someone needs to follow up with finance (Lisa?)\n- Next meeting: soon\n\nNote: half of this was off the record so don't share with leadership\n```\n\n**Option C: Product Catalog Nightmare**\n```\nSKU: PRD-001 | Widget Pro | $49.99 | Blue, Red, Green | In Stock\nPRD-002,Widget Basic,$29.99,Blue only,backordered until ???\nSKU PRD-003 - Widget Mini - 19.99 - all colors - discontinued BUT still selling\nPRD004 | Gadget Max | fifty nine dollars | Chrome | In Stock (I think)\nprd-005;Gadget Mini;$39.99;Black;in stock;NEW ARRIVAL!!\n```\n\n**Option D: Your Real Data**\nUse an actual messy dataset from your work (sanitize any sensitive info).\n\n### Document the Problems\n\nCreate an inventory of data quality issues:\n\n| Issue Type | Example from Data | Frequency | Severity |\n|------------|-------------------|-----------|----------|\n| Inconsistent formatting | | | High/Med/Low |\n| Missing values | | | |\n| Ambiguous content | | | |\n| Mixed data types | | | |\n| Embedded noise | | | |\n| Encoding issues | | | |\n\nIdentify at least 5 distinct problems.\n\n---",
          "htmlContent": "<h3>Your Messy Dataset</h3>\n<p>Choose one of these scenarios (or use your own real data):</p>\n<p><strong>Option A: Customer Feedback Dump</strong></p>\n<pre><code>Email from: john.smith@company.com\nSubject: RE: RE: RE: Your Product\nDate: 3/15/24\n\nHonestly the product is great but shipping took FOREVER (like 2 weeks??)\nand the instructions were useless. Would rate 7/10 overall. Maybe 8 if\nyou fix the manual.\n\nBTW my order # was somewhere around 50234 or 50243, can&#39;t remember.\n\n---\nOriginal message from support@yourcompany.com:\nThank you for your purchase! How was your experience?\n---\n\nSent from my iPhone\n</code></pre>\n<p><strong>Option B: Meeting Notes Chaos</strong></p>\n<pre><code>MEETING - product review??? or was it planning\nAttendees: Sarah, Mike, that new guy (James?), Lisa was on zoom but kept cutting out\n\n- discussed Q4 roadmap (see attached... wait I don&#39;t think I attached it)\n- Mike wants to push feature X to Q1 &quot;for reasons&quot;\n- budget is either 50k or 500k depending on who you ask\n- ACTION: someone needs to follow up with finance (Lisa?)\n- Next meeting: soon\n\nNote: half of this was off the record so don&#39;t share with leadership\n</code></pre>\n<p><strong>Option C: Product Catalog Nightmare</strong></p>\n<pre><code>SKU: PRD-001 | Widget Pro | $49.99 | Blue, Red, Green | In Stock\nPRD-002,Widget Basic,$29.99,Blue only,backordered until ???\nSKU PRD-003 - Widget Mini - 19.99 - all colors - discontinued BUT still selling\nPRD004 | Gadget Max | fifty nine dollars | Chrome | In Stock (I think)\nprd-005;Gadget Mini;$39.99;Black;in stock;NEW ARRIVAL!!\n</code></pre>\n<p><strong>Option D: Your Real Data</strong>\nUse an actual messy dataset from your work (sanitize any sensitive info).</p>\n<h3>Document the Problems</h3>\n<p>Create an inventory of data quality issues:</p>\n<table>\n<thead>\n<tr>\n<th>Issue Type</th>\n<th>Example from Data</th>\n<th>Frequency</th>\n<th>Severity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Inconsistent formatting</td>\n<td></td>\n<td></td>\n<td>High/Med/Low</td>\n</tr>\n<tr>\n<td>Missing values</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Ambiguous content</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Mixed data types</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Embedded noise</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Encoding issues</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<p>Identify at least 5 distinct problems.</p>\n<hr>\n"
        },
        {
          "id": "part-2:-design-your-target-schema-(5-minutes)",
          "title": "Part 2: Design Your Target Schema (5 minutes)",
          "type": "generic",
          "content": "### Define What \"Clean\" Looks Like\n\nBefore cleaning, decide your destination. Create a target schema:\n\n**For Customer Feedback:**\n```json\n{\n  \"feedback_id\": \"string (generated)\",\n  \"customer_email\": \"string (valid email)\",\n  \"date\": \"YYYY-MM-DD\",\n  \"order_number\": \"integer or null\",\n  \"rating\": \"integer 1-10 or null\",\n  \"sentiment\": \"positive | negative | mixed | neutral\",\n  \"shipping_feedback\": \"string or null\",\n  \"product_feedback\": \"string or null\",\n  \"suggestions\": [\"array of strings\"],\n  \"confidence_score\": \"float 0-1\"\n}\n```\n\n**For Meeting Notes:**\n```json\n{\n  \"meeting_id\": \"string (generated)\",\n  \"date\": \"YYYY-MM-DD or null\",\n  \"meeting_type\": \"string\",\n  \"attendees\": [\"array of names\"],\n  \"decisions\": [\"array of strings\"],\n  \"action_items\": [\n    {\n      \"task\": \"string\",\n      \"owner\": \"string or null\",\n      \"deadline\": \"string or null\"\n    }\n  ],\n  \"confidential_flag\": \"boolean\",\n  \"notes\": \"string\"\n}\n```\n\n### Document Your Schema\n\nWrite out:\n1. Every field name\n2. Expected data type\n3. Required vs. optional\n4. Validation rules (if any)\n5. What to do when data is missing\n\n---",
          "htmlContent": "<h3>Define What &quot;Clean&quot; Looks Like</h3>\n<p>Before cleaning, decide your destination. Create a target schema:</p>\n<p><strong>For Customer Feedback:</strong></p>\n<pre><code class=\"language-json\">{\n  &quot;feedback_id&quot;: &quot;string (generated)&quot;,\n  &quot;customer_email&quot;: &quot;string (valid email)&quot;,\n  &quot;date&quot;: &quot;YYYY-MM-DD&quot;,\n  &quot;order_number&quot;: &quot;integer or null&quot;,\n  &quot;rating&quot;: &quot;integer 1-10 or null&quot;,\n  &quot;sentiment&quot;: &quot;positive | negative | mixed | neutral&quot;,\n  &quot;shipping_feedback&quot;: &quot;string or null&quot;,\n  &quot;product_feedback&quot;: &quot;string or null&quot;,\n  &quot;suggestions&quot;: [&quot;array of strings&quot;],\n  &quot;confidence_score&quot;: &quot;float 0-1&quot;\n}\n</code></pre>\n<p><strong>For Meeting Notes:</strong></p>\n<pre><code class=\"language-json\">{\n  &quot;meeting_id&quot;: &quot;string (generated)&quot;,\n  &quot;date&quot;: &quot;YYYY-MM-DD or null&quot;,\n  &quot;meeting_type&quot;: &quot;string&quot;,\n  &quot;attendees&quot;: [&quot;array of names&quot;],\n  &quot;decisions&quot;: [&quot;array of strings&quot;],\n  &quot;action_items&quot;: [\n    {\n      &quot;task&quot;: &quot;string&quot;,\n      &quot;owner&quot;: &quot;string or null&quot;,\n      &quot;deadline&quot;: &quot;string or null&quot;\n    }\n  ],\n  &quot;confidential_flag&quot;: &quot;boolean&quot;,\n  &quot;notes&quot;: &quot;string&quot;\n}\n</code></pre>\n<h3>Document Your Schema</h3>\n<p>Write out:</p>\n<ol>\n<li>Every field name</li>\n<li>Expected data type</li>\n<li>Required vs. optional</li>\n<li>Validation rules (if any)</li>\n<li>What to do when data is missing</li>\n</ol>\n<hr>\n"
        },
        {
          "id": "part-3:-build-your-cleaning-prompt-(15-minutes)",
          "title": "Part 3: Build Your Cleaning Prompt (15 minutes)",
          "type": "generic",
          "content": "### The Transformation Prompt\n\nCreate a prompt that transforms messy input into your clean schema.\n\n**Prompt Template:**\n```\nYou are a data cleaning specialist. Transform the following unstructured\ntext into the specified JSON format.\n\nRULES:\n1. Extract only information that is explicitly stated\n2. Use null for missing or unclear values—NEVER guess\n3. Flag low-confidence extractions with confidence_score < 0.7\n4. Preserve original meaning—do not interpret or summarize unless specified\n5. If a value could be multiple things, choose the most likely and note alternatives\n\nTARGET SCHEMA:\n[Your schema here]\n\nHANDLING AMBIGUITY:\n- Dates: If format unclear, use ISO 8601 (YYYY-MM-DD) and note original\n- Numbers: If ranges given, use midpoint and note range\n- Names: Use as written, even if likely misspelled\n- Missing data: Use null, not empty string or \"unknown\"\n\nINPUT DATA:\n\"\"\"\n[Messy data here]\n\"\"\"\n\nOUTPUT:\nProvide valid JSON only. No explanations before or after.\n```\n\n### Test and Iterate\n\n1. Run your prompt on the messy data\n2. Examine the output critically\n3. Identify where it went wrong\n4. Refine your prompt\n5. Repeat until output is acceptable\n\n### Document Your Iterations\n\n| Iteration | Problem Found | Prompt Change Made | Result |\n|-----------|---------------|-------------------|--------|\n| 1 | | | |\n| 2 | | | |\n| 3 | | | |\n\n---",
          "htmlContent": "<h3>The Transformation Prompt</h3>\n<p>Create a prompt that transforms messy input into your clean schema.</p>\n<p><strong>Prompt Template:</strong></p>\n<pre><code>You are a data cleaning specialist. Transform the following unstructured\ntext into the specified JSON format.\n\nRULES:\n1. Extract only information that is explicitly stated\n2. Use null for missing or unclear values—NEVER guess\n3. Flag low-confidence extractions with confidence_score &lt; 0.7\n4. Preserve original meaning—do not interpret or summarize unless specified\n5. If a value could be multiple things, choose the most likely and note alternatives\n\nTARGET SCHEMA:\n[Your schema here]\n\nHANDLING AMBIGUITY:\n- Dates: If format unclear, use ISO 8601 (YYYY-MM-DD) and note original\n- Numbers: If ranges given, use midpoint and note range\n- Names: Use as written, even if likely misspelled\n- Missing data: Use null, not empty string or &quot;unknown&quot;\n\nINPUT DATA:\n&quot;&quot;&quot;\n[Messy data here]\n&quot;&quot;&quot;\n\nOUTPUT:\nProvide valid JSON only. No explanations before or after.\n</code></pre>\n<h3>Test and Iterate</h3>\n<ol>\n<li>Run your prompt on the messy data</li>\n<li>Examine the output critically</li>\n<li>Identify where it went wrong</li>\n<li>Refine your prompt</li>\n<li>Repeat until output is acceptable</li>\n</ol>\n<h3>Document Your Iterations</h3>\n<table>\n<thead>\n<tr>\n<th>Iteration</th>\n<th>Problem Found</th>\n<th>Prompt Change Made</th>\n<th>Result</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>2</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>3</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<hr>\n"
        },
        {
          "id": "part-4:-validate-for-hallucinations-(10-minutes)",
          "title": "Part 4: Validate for Hallucinations (10 minutes)",
          "type": "generic",
          "content": "### The Critical Step Most People Skip\n\nLLMs will confidently produce structured output even when they're making things up. You MUST validate.\n\n### Validation Checklist\n\nFor each field in your output, verify:\n\n| Field | Source Text Evidence | Extraction Correct? | Hallucination Risk |\n|-------|---------------------|--------------------|--------------------|\n| | Quote the original | Yes/No/Partial | High/Med/Low |\n| | | | |\n\n### Common Hallucination Patterns\n\nWatch for these:\n- **Fabricated specifics**: LLM adds precise numbers that weren't in source\n- **Over-inference**: LLM concludes things that aren't stated\n- **Format conformity**: LLM forces data into schema even when it shouldn't fit\n- **Plausible defaults**: LLM uses reasonable-sounding values instead of null\n\n### Create a Validation Prompt\n\n```\nReview this extraction for accuracy. Compare the OUTPUT against the SOURCE.\n\nSOURCE:\n\"\"\"\n[Original messy data]\n\"\"\"\n\nEXTRACTED OUTPUT:\n[Your cleaned JSON]\n\nFor each field, answer:\n1. Is there explicit evidence in the source? Quote it.\n2. Is the extraction accurate to the source?\n3. Was anything added that isn't in the source?\n4. Confidence level: HIGH (direct quote) / MEDIUM (reasonable inference) / LOW (uncertain) / HALLUCINATION (not in source)\n\nFormat as a validation report.\n```\n\n### Document Findings\n\nHow many fields were:\n- Correctly extracted: ___\n- Partially correct: ___\n- Hallucinated: ___\n- Appropriately null: ___\n\n---",
          "htmlContent": "<h3>The Critical Step Most People Skip</h3>\n<p>LLMs will confidently produce structured output even when they&#39;re making things up. You MUST validate.</p>\n<h3>Validation Checklist</h3>\n<p>For each field in your output, verify:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Source Text Evidence</th>\n<th>Extraction Correct?</th>\n<th>Hallucination Risk</th>\n</tr>\n</thead>\n<tbody><tr>\n<td></td>\n<td>Quote the original</td>\n<td>Yes/No/Partial</td>\n<td>High/Med/Low</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<h3>Common Hallucination Patterns</h3>\n<p>Watch for these:</p>\n<ul>\n<li><strong>Fabricated specifics</strong>: LLM adds precise numbers that weren&#39;t in source</li>\n<li><strong>Over-inference</strong>: LLM concludes things that aren&#39;t stated</li>\n<li><strong>Format conformity</strong>: LLM forces data into schema even when it shouldn&#39;t fit</li>\n<li><strong>Plausible defaults</strong>: LLM uses reasonable-sounding values instead of null</li>\n</ul>\n<h3>Create a Validation Prompt</h3>\n<pre><code>Review this extraction for accuracy. Compare the OUTPUT against the SOURCE.\n\nSOURCE:\n&quot;&quot;&quot;\n[Original messy data]\n&quot;&quot;&quot;\n\nEXTRACTED OUTPUT:\n[Your cleaned JSON]\n\nFor each field, answer:\n1. Is there explicit evidence in the source? Quote it.\n2. Is the extraction accurate to the source?\n3. Was anything added that isn&#39;t in the source?\n4. Confidence level: HIGH (direct quote) / MEDIUM (reasonable inference) / LOW (uncertain) / HALLUCINATION (not in source)\n\nFormat as a validation report.\n</code></pre>\n<h3>Document Findings</h3>\n<p>How many fields were:</p>\n<ul>\n<li>Correctly extracted: ___</li>\n<li>Partially correct: ___</li>\n<li>Hallucinated: ___</li>\n<li>Appropriately null: ___</li>\n</ul>\n<hr>\n"
        },
        {
          "id": "part-5:-build-the-pipeline-(5-minutes)",
          "title": "Part 5: Build the Pipeline (5 minutes)",
          "type": "generic",
          "content": "### Document Your Process\n\nCreate a repeatable pipeline:\n\n```\nDATA PREPARATION PIPELINE: [Name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nINPUT FORMAT:\n- Source type: [Email / Document / Spreadsheet / etc.]\n- Expected volume: [Records per batch]\n- Update frequency: [One-time / Daily / Real-time]\n\nSTEP 1: PRE-PROCESSING\n- [ ] Remove headers/footers/signatures\n- [ ] Standardize encoding (UTF-8)\n- [ ] Split into individual records\n- [ ] [Other pre-processing]\n\nSTEP 2: LLM TRANSFORMATION\n- Model: [GPT-4 / Claude / etc.]\n- Prompt version: [Link or paste]\n- Temperature: [Recommended: 0 for extraction]\n- Expected tokens: [Estimate]\n\nSTEP 3: VALIDATION\n- [ ] Run validation prompt\n- [ ] Check for hallucinations\n- [ ] Verify required fields present\n- [ ] Spot-check [N] random records manually\n\nSTEP 4: POST-PROCESSING\n- [ ] Parse JSON\n- [ ] Apply business rules\n- [ ] Handle validation failures\n- [ ] Load to destination\n\nERROR HANDLING:\n- If parsing fails: [Action]\n- If validation fails: [Action]\n- If confidence < threshold: [Action]\n\nESTIMATED COST PER RECORD:\n- Input tokens: ~[X]\n- Output tokens: ~[X]\n- Cost at current rates: $[X]\n```\n\n---",
          "htmlContent": "<h3>Document Your Process</h3>\n<p>Create a repeatable pipeline:</p>\n<pre><code>DATA PREPARATION PIPELINE: [Name]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nINPUT FORMAT:\n- Source type: [Email / Document / Spreadsheet / etc.]\n- Expected volume: [Records per batch]\n- Update frequency: [One-time / Daily / Real-time]\n\nSTEP 1: PRE-PROCESSING\n- [ ] Remove headers/footers/signatures\n- [ ] Standardize encoding (UTF-8)\n- [ ] Split into individual records\n- [ ] [Other pre-processing]\n\nSTEP 2: LLM TRANSFORMATION\n- Model: [GPT-4 / Claude / etc.]\n- Prompt version: [Link or paste]\n- Temperature: [Recommended: 0 for extraction]\n- Expected tokens: [Estimate]\n\nSTEP 3: VALIDATION\n- [ ] Run validation prompt\n- [ ] Check for hallucinations\n- [ ] Verify required fields present\n- [ ] Spot-check [N] random records manually\n\nSTEP 4: POST-PROCESSING\n- [ ] Parse JSON\n- [ ] Apply business rules\n- [ ] Handle validation failures\n- [ ] Load to destination\n\nERROR HANDLING:\n- If parsing fails: [Action]\n- If validation fails: [Action]\n- If confidence &lt; threshold: [Action]\n\nESTIMATED COST PER RECORD:\n- Input tokens: ~[X]\n- Output tokens: ~[X]\n- Cost at current rates: $[X]\n</code></pre>\n<hr>\n"
        },
        {
          "id": "deliverable",
          "title": "Deliverable",
          "type": "generic",
          "content": "Create a comprehensive document containing:\n\n1. **Problem Assessment**\n   - Original messy data sample\n   - Inventory of data quality issues\n   - Severity ratings\n\n2. **Target Schema**\n   - Complete field definitions\n   - Validation rules\n   - Missing data handling\n\n3. **Transformation Prompt**\n   - Final working prompt\n   - Iteration history\n   - Known limitations\n\n4. **Validation Report**\n   - Sample validation results\n   - Hallucination analysis\n   - Accuracy metrics\n\n5. **Pipeline Documentation**\n   - Step-by-step process\n   - Error handling\n   - Cost estimates\n\n6. **Lessons Learned**\n   - What surprised you?\n   - What would you do differently?\n   - Recommendations for similar data\n\n---",
          "htmlContent": "<p>Create a comprehensive document containing:</p>\n<ol>\n<li><p><strong>Problem Assessment</strong></p>\n<ul>\n<li>Original messy data sample</li>\n<li>Inventory of data quality issues</li>\n<li>Severity ratings</li>\n</ul>\n</li>\n<li><p><strong>Target Schema</strong></p>\n<ul>\n<li>Complete field definitions</li>\n<li>Validation rules</li>\n<li>Missing data handling</li>\n</ul>\n</li>\n<li><p><strong>Transformation Prompt</strong></p>\n<ul>\n<li>Final working prompt</li>\n<li>Iteration history</li>\n<li>Known limitations</li>\n</ul>\n</li>\n<li><p><strong>Validation Report</strong></p>\n<ul>\n<li>Sample validation results</li>\n<li>Hallucination analysis</li>\n<li>Accuracy metrics</li>\n</ul>\n</li>\n<li><p><strong>Pipeline Documentation</strong></p>\n<ul>\n<li>Step-by-step process</li>\n<li>Error handling</li>\n<li>Cost estimates</li>\n</ul>\n</li>\n<li><p><strong>Lessons Learned</strong></p>\n<ul>\n<li>What surprised you?</li>\n<li>What would you do differently?</li>\n<li>Recommendations for similar data</li>\n</ul>\n</li>\n</ol>\n<hr>\n"
        },
        {
          "id": "extension-challenge",
          "title": "Extension Challenge",
          "type": "generic",
          "content": "**Scale It Up**\n\n1. Take 10 records of the same messy data type\n2. Process them through your pipeline\n3. Calculate:\n   - Average processing time\n   - Cost per record\n   - Accuracy rate\n   - Failure rate\n\n4. Identify:\n   - Which data patterns cause problems?\n   - Where does the pipeline break?\n   - What manual intervention is still needed?\n\nDocument your findings and propose pipeline improvements.",
          "htmlContent": "<p><strong>Scale It Up</strong></p>\n<ol>\n<li><p>Take 10 records of the same messy data type</p>\n</li>\n<li><p>Process them through your pipeline</p>\n</li>\n<li><p>Calculate:</p>\n<ul>\n<li>Average processing time</li>\n<li>Cost per record</li>\n<li>Accuracy rate</li>\n<li>Failure rate</li>\n</ul>\n</li>\n<li><p>Identify:</p>\n<ul>\n<li>Which data patterns cause problems?</li>\n<li>Where does the pipeline break?</li>\n<li>What manual intervention is still needed?</li>\n</ul>\n</li>\n</ol>\n<p>Document your findings and propose pipeline improvements.</p>\n"
        }
      ]
    },
    {
      "id": "lab-4-quality-gate-design",
      "slug": "lab-4-quality-gate-design",
      "title": "Quality Gate Design",
      "phase": 2,
      "labNumber": 4,
      "estimatedMinutes": 45,
      "objectives": [
        "Design a multi-layer quality assurance system",
        "Create validation prompts and criteria",
        "Build feedback loops for continuous improvement"
      ],
      "prerequisites": [
        "2.3-quality-and-iteration",
        "2.4-human-ai-handoffs"
      ],
      "content": "# Lab 4: Quality Gate Design\n\n## Lab Overview\n\nAI without quality control is a liability. In this lab, you'll design a comprehensive quality assurance system for an AI workflow—the kind of system that lets you sleep at night knowing your AI isn't embarrassing you.\n\n**What you'll create:**\n- A complete quality gate specification\n- Validation prompts and criteria\n- Handoff protocols\n- Feedback capture system\n\n---\n\n## Part 1: Choose Your Scenario (5 minutes)\n\n### Select an AI workflow to protect:\n\n**Option A: Customer Email Response Generation**\nAI drafts responses to customer inquiries that agents can approve and send.\n\n**Option B: Content Summarization Pipeline**\nAI summarizes long documents into executive briefs for leadership.\n\n**Option C: Data Extraction and Categorization**\nAI extracts structured data from unstructured documents (invoices, contracts, etc.).\n\n**Option D: Personalized Marketing Copy**\nAI generates personalized marketing messages based on customer profiles.\n\n**Option E: Your Choice**\nAn AI workflow you're building or planning.\n\n---\n\n## Part 2: Define Quality Criteria (10 minutes)\n\n### Identify Quality Dimensions\n\nFor your chosen workflow, define what \"quality\" means:\n\n**Dimension 1: Accuracy**\nWhat needs to be factually correct?\n- [Specific accuracy requirement 1]\n- [Specific accuracy requirement 2]\n\n**Dimension 2: Completeness**\nWhat must be included?\n- [Required element 1]\n- [Required element 2]\n\n**Dimension 3: Format/Structure**\nWhat structure is required?\n- [Format requirement 1]\n- [Format requirement 2]\n\n**Dimension 4: Tone/Voice**\nWhat should it sound like?\n- [Tone requirement 1]\n- [Tone requirement 2]\n\n**Dimension 5: Constraints**\nWhat must be avoided?\n- [Constraint 1]\n- [Constraint 2]\n\n### Create Quality Rubric\n\n| Dimension | 1 (Fail) | 3 (Acceptable) | 5 (Excellent) | Weight |\n|-----------|----------|----------------|---------------|--------|\n| Accuracy | [Describe] | [Describe] | [Describe] | X% |\n| Completeness | [Describe] | [Describe] | [Describe] | X% |\n| Format | [Describe] | [Describe] | [Describe] | X% |\n| Tone | [Describe] | [Describe] | [Describe] | X% |\n| Constraints | [Describe] | [Describe] | [Describe] | X% |\n\n**Minimum passing score:** [e.g., 3.5 weighted average]\n\n---\n\n## Part 3: Design Quality Gates (15 minutes)\n\n### Gate 1: Automated Validation\n\nWhat can be checked automatically without AI judgment?\n\n```\nAUTOMATED CHECKS\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n□ Length within range: [min] - [max] words\n□ Required sections present: [list]\n□ Prohibited terms absent: [list]\n□ Format requirements met: [specifics]\n□ Links/references valid: [if applicable]\n□ [Additional automated check]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Gate 2: AI Cross-Validation\n\nDesign a prompt that has AI evaluate AI output:\n\n```\nQUALITY EVALUATION PROMPT\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nYou are a quality reviewer. Evaluate this [content type]\nagainst the following criteria:\n\nCONTENT TO REVIEW:\n\"\"\"\n{ai_output}\n\"\"\"\n\nORIGINAL REQUEST:\n\"\"\"\n{original_input}\n\"\"\"\n\nEvaluate each criterion (1-5) with explanation:\n\n1. ACCURACY\n   - Are all facts verifiable?\n   - Any potential hallucinations?\n   Score: [1-5]\n   Issues: [List any concerns]\n\n2. COMPLETENESS\n   - All required elements present?\n   - Any gaps in coverage?\n   Score: [1-5]\n   Issues: [List any missing items]\n\n3. FORMAT\n   - Matches expected structure?\n   - Professional presentation?\n   Score: [1-5]\n   Issues: [List any format problems]\n\n4. TONE\n   - Appropriate for audience?\n   - Consistent throughout?\n   Score: [1-5]\n   Issues: [List any tone problems]\n\n5. CONSTRAINTS\n   - All restrictions followed?\n   - Any policy violations?\n   Score: [1-5]\n   Issues: [List any violations]\n\nOVERALL ASSESSMENT:\n- Pass/Fail: [Based on minimum threshold]\n- Confidence: [Low/Medium/High]\n- Recommended action: [Approve/Revise/Escalate]\n- Specific fixes needed: [If applicable]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Gate 3: Human Review\n\nDefine when and how humans review:\n\n```\nHUMAN REVIEW TRIGGERS\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nAlways review when:\n□ AI confidence < [threshold]%\n□ [Specific trigger 1]\n□ [Specific trigger 2]\n\nSample review:\n□ [X]% of outputs randomly selected\n□ First [N] outputs of each type\n□ [Other sampling strategy]\n\nEscalation required when:\n□ [Critical condition 1]\n□ [Critical condition 2]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Human Review Interface\n\nWhat should reviewers see?\n\n```\nREVIEW PACKAGE CONTENTS\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n1. Original input/request\n2. AI-generated output\n3. Automated check results\n4. AI evaluation scores\n5. Flagged concerns\n6. Suggested actions\n\nREVIEWER OPTIONS:\n□ Approve as-is\n□ Approve with minor edits\n□ Request revision (with feedback)\n□ Reject (with reason)\n□ Escalate to [role]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n---\n\n## Part 4: Design Feedback Loop (10 minutes)\n\n### Capture Quality Data\n\nWhat should be logged for every output?\n\n```\nQUALITY LOG ENTRY\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nTimestamp: [datetime]\nRequest ID: [unique identifier]\nInput summary: [brief description]\nModel used: [model name/version]\n\nAutomated checks: [pass/fail summary]\nAI evaluation score: [weighted average]\nHuman review: [Approve/Edit/Reject/Escalate]\nHuman edits: [what was changed]\nFinal outcome: [what was shipped]\n\nProcessing time: [duration]\nToken usage: [count]\nCost: [amount]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Define Improvement Triggers\n\nWhen should you adjust the system?\n\n| Metric | Threshold | Action |\n|--------|-----------|--------|\n| First-pass approval rate | < 70% | Review and improve prompts |\n| Average human edit time | > [X] min | Improve AI output quality |\n| Escalation rate | > 10% | Expand AI capability or scope |\n| Customer complaints | Any | Root cause analysis |\n| Cost per output | > [X] | Optimize model selection |\n\n### Feedback Integration Process\n\n```\nWEEKLY QUALITY REVIEW\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n1. Pull quality metrics dashboard\n2. Identify lowest-scoring areas\n3. Sample rejected/edited outputs\n4. Categorize common issues\n5. Prioritize improvements\n6. Update prompts/criteria\n7. Monitor impact\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n---\n\n## Part 5: Visualize the System (5 minutes)\n\n### Create Quality Gate Flow Diagram\n\n```\n[AI Output Generated]\n        │\n        ▼\n┌─────────────────┐\n│ GATE 1: AUTO    │──FAIL──┐\n│ Format, length, │        │\n│ prohibited terms│        │\n└────────┬────────┘        │\n         │ PASS            │\n         ▼                 │\n┌─────────────────┐        │\n│ GATE 2: AI EVAL │──FAIL──┤\n│ Quality scoring │        │\n│ Issue flagging  │        │\n└────────┬────────┘        │\n         │ PASS            │\n         ▼                 │\n┌─────────────────┐        │\n│ GATE 3: HUMAN   │──FAIL──┘\n│ (if triggered)  │        │\n└────────┬────────┘        │\n         │ PASS            │\n         ▼                 ▼\n    [APPROVED]        [REVISION]\n                          │\n                          ▼\n                    [Back to AI]\n```\n\n---\n\n## Deliverable\n\nCreate a Quality Gate Specification Document containing:\n\n1. **Quality Criteria Definition**\n   - All quality dimensions\n   - Detailed rubric with examples\n   - Minimum thresholds\n\n2. **Gate Specifications**\n   - Gate 1: Automated checks (complete list)\n   - Gate 2: AI evaluation prompt (ready to use)\n   - Gate 3: Human review protocol\n\n3. **Handoff Design**\n   - Review package contents\n   - Reviewer options and actions\n   - Escalation paths\n\n4. **Feedback System**\n   - Data capture schema\n   - Improvement triggers\n   - Review process\n\n5. **System Diagram**\n   - Visual flow of quality gates\n   - Decision points and paths\n\n---\n\n## Extension Challenge\n\n**Build a Working Prototype**\n\n1. Create a prompt that generates outputs for your chosen scenario\n2. Create the AI evaluation prompt\n3. Run 10 test cases through both\n4. Calculate your first-pass approval rate\n5. Identify the most common quality issues\n6. Refine your generation prompt based on findings\n7. Re-run and measure improvement\n\nDocument your iteration process and results.",
      "htmlContent": "<h1>Lab 4: Quality Gate Design</h1>\n<h2>Lab Overview</h2>\n<p>AI without quality control is a liability. In this lab, you&#39;ll design a comprehensive quality assurance system for an AI workflow—the kind of system that lets you sleep at night knowing your AI isn&#39;t embarrassing you.</p>\n<p><strong>What you&#39;ll create:</strong></p>\n<ul>\n<li>A complete quality gate specification</li>\n<li>Validation prompts and criteria</li>\n<li>Handoff protocols</li>\n<li>Feedback capture system</li>\n</ul>\n<hr>\n<h2>Part 1: Choose Your Scenario (5 minutes)</h2>\n<h3>Select an AI workflow to protect:</h3>\n<p><strong>Option A: Customer Email Response Generation</strong>\nAI drafts responses to customer inquiries that agents can approve and send.</p>\n<p><strong>Option B: Content Summarization Pipeline</strong>\nAI summarizes long documents into executive briefs for leadership.</p>\n<p><strong>Option C: Data Extraction and Categorization</strong>\nAI extracts structured data from unstructured documents (invoices, contracts, etc.).</p>\n<p><strong>Option D: Personalized Marketing Copy</strong>\nAI generates personalized marketing messages based on customer profiles.</p>\n<p><strong>Option E: Your Choice</strong>\nAn AI workflow you&#39;re building or planning.</p>\n<hr>\n<h2>Part 2: Define Quality Criteria (10 minutes)</h2>\n<h3>Identify Quality Dimensions</h3>\n<p>For your chosen workflow, define what &quot;quality&quot; means:</p>\n<p><strong>Dimension 1: Accuracy</strong>\nWhat needs to be factually correct?</p>\n<ul>\n<li>[Specific accuracy requirement 1]</li>\n<li>[Specific accuracy requirement 2]</li>\n</ul>\n<p><strong>Dimension 2: Completeness</strong>\nWhat must be included?</p>\n<ul>\n<li>[Required element 1]</li>\n<li>[Required element 2]</li>\n</ul>\n<p><strong>Dimension 3: Format/Structure</strong>\nWhat structure is required?</p>\n<ul>\n<li>[Format requirement 1]</li>\n<li>[Format requirement 2]</li>\n</ul>\n<p><strong>Dimension 4: Tone/Voice</strong>\nWhat should it sound like?</p>\n<ul>\n<li>[Tone requirement 1]</li>\n<li>[Tone requirement 2]</li>\n</ul>\n<p><strong>Dimension 5: Constraints</strong>\nWhat must be avoided?</p>\n<ul>\n<li>[Constraint 1]</li>\n<li>[Constraint 2]</li>\n</ul>\n<h3>Create Quality Rubric</h3>\n<table>\n<thead>\n<tr>\n<th>Dimension</th>\n<th>1 (Fail)</th>\n<th>3 (Acceptable)</th>\n<th>5 (Excellent)</th>\n<th>Weight</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Accuracy</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>X%</td>\n</tr>\n<tr>\n<td>Completeness</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>X%</td>\n</tr>\n<tr>\n<td>Format</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>X%</td>\n</tr>\n<tr>\n<td>Tone</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>X%</td>\n</tr>\n<tr>\n<td>Constraints</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>X%</td>\n</tr>\n</tbody></table>\n<p><strong>Minimum passing score:</strong> [e.g., 3.5 weighted average]</p>\n<hr>\n<h2>Part 3: Design Quality Gates (15 minutes)</h2>\n<h3>Gate 1: Automated Validation</h3>\n<p>What can be checked automatically without AI judgment?</p>\n<pre><code>AUTOMATED CHECKS\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n□ Length within range: [min] - [max] words\n□ Required sections present: [list]\n□ Prohibited terms absent: [list]\n□ Format requirements met: [specifics]\n□ Links/references valid: [if applicable]\n□ [Additional automated check]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<h3>Gate 2: AI Cross-Validation</h3>\n<p>Design a prompt that has AI evaluate AI output:</p>\n<pre><code>QUALITY EVALUATION PROMPT\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nYou are a quality reviewer. Evaluate this [content type]\nagainst the following criteria:\n\nCONTENT TO REVIEW:\n&quot;&quot;&quot;\n{ai_output}\n&quot;&quot;&quot;\n\nORIGINAL REQUEST:\n&quot;&quot;&quot;\n{original_input}\n&quot;&quot;&quot;\n\nEvaluate each criterion (1-5) with explanation:\n\n1. ACCURACY\n   - Are all facts verifiable?\n   - Any potential hallucinations?\n   Score: [1-5]\n   Issues: [List any concerns]\n\n2. COMPLETENESS\n   - All required elements present?\n   - Any gaps in coverage?\n   Score: [1-5]\n   Issues: [List any missing items]\n\n3. FORMAT\n   - Matches expected structure?\n   - Professional presentation?\n   Score: [1-5]\n   Issues: [List any format problems]\n\n4. TONE\n   - Appropriate for audience?\n   - Consistent throughout?\n   Score: [1-5]\n   Issues: [List any tone problems]\n\n5. CONSTRAINTS\n   - All restrictions followed?\n   - Any policy violations?\n   Score: [1-5]\n   Issues: [List any violations]\n\nOVERALL ASSESSMENT:\n- Pass/Fail: [Based on minimum threshold]\n- Confidence: [Low/Medium/High]\n- Recommended action: [Approve/Revise/Escalate]\n- Specific fixes needed: [If applicable]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<h3>Gate 3: Human Review</h3>\n<p>Define when and how humans review:</p>\n<pre><code>HUMAN REVIEW TRIGGERS\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nAlways review when:\n□ AI confidence &lt; [threshold]%\n□ [Specific trigger 1]\n□ [Specific trigger 2]\n\nSample review:\n□ [X]% of outputs randomly selected\n□ First [N] outputs of each type\n□ [Other sampling strategy]\n\nEscalation required when:\n□ [Critical condition 1]\n□ [Critical condition 2]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<h3>Human Review Interface</h3>\n<p>What should reviewers see?</p>\n<pre><code>REVIEW PACKAGE CONTENTS\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n1. Original input/request\n2. AI-generated output\n3. Automated check results\n4. AI evaluation scores\n5. Flagged concerns\n6. Suggested actions\n\nREVIEWER OPTIONS:\n□ Approve as-is\n□ Approve with minor edits\n□ Request revision (with feedback)\n□ Reject (with reason)\n□ Escalate to [role]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<hr>\n<h2>Part 4: Design Feedback Loop (10 minutes)</h2>\n<h3>Capture Quality Data</h3>\n<p>What should be logged for every output?</p>\n<pre><code>QUALITY LOG ENTRY\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nTimestamp: [datetime]\nRequest ID: [unique identifier]\nInput summary: [brief description]\nModel used: [model name/version]\n\nAutomated checks: [pass/fail summary]\nAI evaluation score: [weighted average]\nHuman review: [Approve/Edit/Reject/Escalate]\nHuman edits: [what was changed]\nFinal outcome: [what was shipped]\n\nProcessing time: [duration]\nToken usage: [count]\nCost: [amount]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<h3>Define Improvement Triggers</h3>\n<p>When should you adjust the system?</p>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>Threshold</th>\n<th>Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>First-pass approval rate</td>\n<td>&lt; 70%</td>\n<td>Review and improve prompts</td>\n</tr>\n<tr>\n<td>Average human edit time</td>\n<td>&gt; [X] min</td>\n<td>Improve AI output quality</td>\n</tr>\n<tr>\n<td>Escalation rate</td>\n<td>&gt; 10%</td>\n<td>Expand AI capability or scope</td>\n</tr>\n<tr>\n<td>Customer complaints</td>\n<td>Any</td>\n<td>Root cause analysis</td>\n</tr>\n<tr>\n<td>Cost per output</td>\n<td>&gt; [X]</td>\n<td>Optimize model selection</td>\n</tr>\n</tbody></table>\n<h3>Feedback Integration Process</h3>\n<pre><code>WEEKLY QUALITY REVIEW\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n1. Pull quality metrics dashboard\n2. Identify lowest-scoring areas\n3. Sample rejected/edited outputs\n4. Categorize common issues\n5. Prioritize improvements\n6. Update prompts/criteria\n7. Monitor impact\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<hr>\n<h2>Part 5: Visualize the System (5 minutes)</h2>\n<h3>Create Quality Gate Flow Diagram</h3>\n<pre><code>[AI Output Generated]\n        │\n        ▼\n┌─────────────────┐\n│ GATE 1: AUTO    │──FAIL──┐\n│ Format, length, │        │\n│ prohibited terms│        │\n└────────┬────────┘        │\n         │ PASS            │\n         ▼                 │\n┌─────────────────┐        │\n│ GATE 2: AI EVAL │──FAIL──┤\n│ Quality scoring │        │\n│ Issue flagging  │        │\n└────────┬────────┘        │\n         │ PASS            │\n         ▼                 │\n┌─────────────────┐        │\n│ GATE 3: HUMAN   │──FAIL──┘\n│ (if triggered)  │        │\n└────────┬────────┘        │\n         │ PASS            │\n         ▼                 ▼\n    [APPROVED]        [REVISION]\n                          │\n                          ▼\n                    [Back to AI]\n</code></pre>\n<hr>\n<h2>Deliverable</h2>\n<p>Create a Quality Gate Specification Document containing:</p>\n<ol>\n<li><p><strong>Quality Criteria Definition</strong></p>\n<ul>\n<li>All quality dimensions</li>\n<li>Detailed rubric with examples</li>\n<li>Minimum thresholds</li>\n</ul>\n</li>\n<li><p><strong>Gate Specifications</strong></p>\n<ul>\n<li>Gate 1: Automated checks (complete list)</li>\n<li>Gate 2: AI evaluation prompt (ready to use)</li>\n<li>Gate 3: Human review protocol</li>\n</ul>\n</li>\n<li><p><strong>Handoff Design</strong></p>\n<ul>\n<li>Review package contents</li>\n<li>Reviewer options and actions</li>\n<li>Escalation paths</li>\n</ul>\n</li>\n<li><p><strong>Feedback System</strong></p>\n<ul>\n<li>Data capture schema</li>\n<li>Improvement triggers</li>\n<li>Review process</li>\n</ul>\n</li>\n<li><p><strong>System Diagram</strong></p>\n<ul>\n<li>Visual flow of quality gates</li>\n<li>Decision points and paths</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h2>Extension Challenge</h2>\n<p><strong>Build a Working Prototype</strong></p>\n<ol>\n<li>Create a prompt that generates outputs for your chosen scenario</li>\n<li>Create the AI evaluation prompt</li>\n<li>Run 10 test cases through both</li>\n<li>Calculate your first-pass approval rate</li>\n<li>Identify the most common quality issues</li>\n<li>Refine your generation prompt based on findings</li>\n<li>Re-run and measure improvement</li>\n</ol>\n<p>Document your iteration process and results.</p>\n",
      "sections": [
        {
          "id": "lab-overview",
          "title": "Lab Overview",
          "type": "generic",
          "content": "AI without quality control is a liability. In this lab, you'll design a comprehensive quality assurance system for an AI workflow—the kind of system that lets you sleep at night knowing your AI isn't embarrassing you.\n\n**What you'll create:**\n- A complete quality gate specification\n- Validation prompts and criteria\n- Handoff protocols\n- Feedback capture system\n\n---",
          "htmlContent": "<p>AI without quality control is a liability. In this lab, you&#39;ll design a comprehensive quality assurance system for an AI workflow—the kind of system that lets you sleep at night knowing your AI isn&#39;t embarrassing you.</p>\n<p><strong>What you&#39;ll create:</strong></p>\n<ul>\n<li>A complete quality gate specification</li>\n<li>Validation prompts and criteria</li>\n<li>Handoff protocols</li>\n<li>Feedback capture system</li>\n</ul>\n<hr>\n"
        },
        {
          "id": "part-1:-choose-your-scenario-(5-minutes)",
          "title": "Part 1: Choose Your Scenario (5 minutes)",
          "type": "generic",
          "content": "### Select an AI workflow to protect:\n\n**Option A: Customer Email Response Generation**\nAI drafts responses to customer inquiries that agents can approve and send.\n\n**Option B: Content Summarization Pipeline**\nAI summarizes long documents into executive briefs for leadership.\n\n**Option C: Data Extraction and Categorization**\nAI extracts structured data from unstructured documents (invoices, contracts, etc.).\n\n**Option D: Personalized Marketing Copy**\nAI generates personalized marketing messages based on customer profiles.\n\n**Option E: Your Choice**\nAn AI workflow you're building or planning.\n\n---",
          "htmlContent": "<h3>Select an AI workflow to protect:</h3>\n<p><strong>Option A: Customer Email Response Generation</strong>\nAI drafts responses to customer inquiries that agents can approve and send.</p>\n<p><strong>Option B: Content Summarization Pipeline</strong>\nAI summarizes long documents into executive briefs for leadership.</p>\n<p><strong>Option C: Data Extraction and Categorization</strong>\nAI extracts structured data from unstructured documents (invoices, contracts, etc.).</p>\n<p><strong>Option D: Personalized Marketing Copy</strong>\nAI generates personalized marketing messages based on customer profiles.</p>\n<p><strong>Option E: Your Choice</strong>\nAn AI workflow you&#39;re building or planning.</p>\n<hr>\n"
        },
        {
          "id": "part-2:-define-quality-criteria-(10-minutes)",
          "title": "Part 2: Define Quality Criteria (10 minutes)",
          "type": "generic",
          "content": "### Identify Quality Dimensions\n\nFor your chosen workflow, define what \"quality\" means:\n\n**Dimension 1: Accuracy**\nWhat needs to be factually correct?\n- [Specific accuracy requirement 1]\n- [Specific accuracy requirement 2]\n\n**Dimension 2: Completeness**\nWhat must be included?\n- [Required element 1]\n- [Required element 2]\n\n**Dimension 3: Format/Structure**\nWhat structure is required?\n- [Format requirement 1]\n- [Format requirement 2]\n\n**Dimension 4: Tone/Voice**\nWhat should it sound like?\n- [Tone requirement 1]\n- [Tone requirement 2]\n\n**Dimension 5: Constraints**\nWhat must be avoided?\n- [Constraint 1]\n- [Constraint 2]\n\n### Create Quality Rubric\n\n| Dimension | 1 (Fail) | 3 (Acceptable) | 5 (Excellent) | Weight |\n|-----------|----------|----------------|---------------|--------|\n| Accuracy | [Describe] | [Describe] | [Describe] | X% |\n| Completeness | [Describe] | [Describe] | [Describe] | X% |\n| Format | [Describe] | [Describe] | [Describe] | X% |\n| Tone | [Describe] | [Describe] | [Describe] | X% |\n| Constraints | [Describe] | [Describe] | [Describe] | X% |\n\n**Minimum passing score:** [e.g., 3.5 weighted average]\n\n---",
          "htmlContent": "<h3>Identify Quality Dimensions</h3>\n<p>For your chosen workflow, define what &quot;quality&quot; means:</p>\n<p><strong>Dimension 1: Accuracy</strong>\nWhat needs to be factually correct?</p>\n<ul>\n<li>[Specific accuracy requirement 1]</li>\n<li>[Specific accuracy requirement 2]</li>\n</ul>\n<p><strong>Dimension 2: Completeness</strong>\nWhat must be included?</p>\n<ul>\n<li>[Required element 1]</li>\n<li>[Required element 2]</li>\n</ul>\n<p><strong>Dimension 3: Format/Structure</strong>\nWhat structure is required?</p>\n<ul>\n<li>[Format requirement 1]</li>\n<li>[Format requirement 2]</li>\n</ul>\n<p><strong>Dimension 4: Tone/Voice</strong>\nWhat should it sound like?</p>\n<ul>\n<li>[Tone requirement 1]</li>\n<li>[Tone requirement 2]</li>\n</ul>\n<p><strong>Dimension 5: Constraints</strong>\nWhat must be avoided?</p>\n<ul>\n<li>[Constraint 1]</li>\n<li>[Constraint 2]</li>\n</ul>\n<h3>Create Quality Rubric</h3>\n<table>\n<thead>\n<tr>\n<th>Dimension</th>\n<th>1 (Fail)</th>\n<th>3 (Acceptable)</th>\n<th>5 (Excellent)</th>\n<th>Weight</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Accuracy</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>X%</td>\n</tr>\n<tr>\n<td>Completeness</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>X%</td>\n</tr>\n<tr>\n<td>Format</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>X%</td>\n</tr>\n<tr>\n<td>Tone</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>X%</td>\n</tr>\n<tr>\n<td>Constraints</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>[Describe]</td>\n<td>X%</td>\n</tr>\n</tbody></table>\n<p><strong>Minimum passing score:</strong> [e.g., 3.5 weighted average]</p>\n<hr>\n"
        },
        {
          "id": "part-3:-design-quality-gates-(15-minutes)",
          "title": "Part 3: Design Quality Gates (15 minutes)",
          "type": "generic",
          "content": "### Gate 1: Automated Validation\n\nWhat can be checked automatically without AI judgment?\n\n```\nAUTOMATED CHECKS\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n□ Length within range: [min] - [max] words\n□ Required sections present: [list]\n□ Prohibited terms absent: [list]\n□ Format requirements met: [specifics]\n□ Links/references valid: [if applicable]\n□ [Additional automated check]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Gate 2: AI Cross-Validation\n\nDesign a prompt that has AI evaluate AI output:\n\n```\nQUALITY EVALUATION PROMPT\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nYou are a quality reviewer. Evaluate this [content type]\nagainst the following criteria:\n\nCONTENT TO REVIEW:\n\"\"\"\n{ai_output}\n\"\"\"\n\nORIGINAL REQUEST:\n\"\"\"\n{original_input}\n\"\"\"\n\nEvaluate each criterion (1-5) with explanation:\n\n1. ACCURACY\n   - Are all facts verifiable?\n   - Any potential hallucinations?\n   Score: [1-5]\n   Issues: [List any concerns]\n\n2. COMPLETENESS\n   - All required elements present?\n   - Any gaps in coverage?\n   Score: [1-5]\n   Issues: [List any missing items]\n\n3. FORMAT\n   - Matches expected structure?\n   - Professional presentation?\n   Score: [1-5]\n   Issues: [List any format problems]\n\n4. TONE\n   - Appropriate for audience?\n   - Consistent throughout?\n   Score: [1-5]\n   Issues: [List any tone problems]\n\n5. CONSTRAINTS\n   - All restrictions followed?\n   - Any policy violations?\n   Score: [1-5]\n   Issues: [List any violations]\n\nOVERALL ASSESSMENT:\n- Pass/Fail: [Based on minimum threshold]\n- Confidence: [Low/Medium/High]\n- Recommended action: [Approve/Revise/Escalate]\n- Specific fixes needed: [If applicable]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Gate 3: Human Review\n\nDefine when and how humans review:\n\n```\nHUMAN REVIEW TRIGGERS\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nAlways review when:\n□ AI confidence < [threshold]%\n□ [Specific trigger 1]\n□ [Specific trigger 2]\n\nSample review:\n□ [X]% of outputs randomly selected\n□ First [N] outputs of each type\n□ [Other sampling strategy]\n\nEscalation required when:\n□ [Critical condition 1]\n□ [Critical condition 2]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Human Review Interface\n\nWhat should reviewers see?\n\n```\nREVIEW PACKAGE CONTENTS\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n1. Original input/request\n2. AI-generated output\n3. Automated check results\n4. AI evaluation scores\n5. Flagged concerns\n6. Suggested actions\n\nREVIEWER OPTIONS:\n□ Approve as-is\n□ Approve with minor edits\n□ Request revision (with feedback)\n□ Reject (with reason)\n□ Escalate to [role]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n---",
          "htmlContent": "<h3>Gate 1: Automated Validation</h3>\n<p>What can be checked automatically without AI judgment?</p>\n<pre><code>AUTOMATED CHECKS\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n□ Length within range: [min] - [max] words\n□ Required sections present: [list]\n□ Prohibited terms absent: [list]\n□ Format requirements met: [specifics]\n□ Links/references valid: [if applicable]\n□ [Additional automated check]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<h3>Gate 2: AI Cross-Validation</h3>\n<p>Design a prompt that has AI evaluate AI output:</p>\n<pre><code>QUALITY EVALUATION PROMPT\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nYou are a quality reviewer. Evaluate this [content type]\nagainst the following criteria:\n\nCONTENT TO REVIEW:\n&quot;&quot;&quot;\n{ai_output}\n&quot;&quot;&quot;\n\nORIGINAL REQUEST:\n&quot;&quot;&quot;\n{original_input}\n&quot;&quot;&quot;\n\nEvaluate each criterion (1-5) with explanation:\n\n1. ACCURACY\n   - Are all facts verifiable?\n   - Any potential hallucinations?\n   Score: [1-5]\n   Issues: [List any concerns]\n\n2. COMPLETENESS\n   - All required elements present?\n   - Any gaps in coverage?\n   Score: [1-5]\n   Issues: [List any missing items]\n\n3. FORMAT\n   - Matches expected structure?\n   - Professional presentation?\n   Score: [1-5]\n   Issues: [List any format problems]\n\n4. TONE\n   - Appropriate for audience?\n   - Consistent throughout?\n   Score: [1-5]\n   Issues: [List any tone problems]\n\n5. CONSTRAINTS\n   - All restrictions followed?\n   - Any policy violations?\n   Score: [1-5]\n   Issues: [List any violations]\n\nOVERALL ASSESSMENT:\n- Pass/Fail: [Based on minimum threshold]\n- Confidence: [Low/Medium/High]\n- Recommended action: [Approve/Revise/Escalate]\n- Specific fixes needed: [If applicable]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<h3>Gate 3: Human Review</h3>\n<p>Define when and how humans review:</p>\n<pre><code>HUMAN REVIEW TRIGGERS\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nAlways review when:\n□ AI confidence &lt; [threshold]%\n□ [Specific trigger 1]\n□ [Specific trigger 2]\n\nSample review:\n□ [X]% of outputs randomly selected\n□ First [N] outputs of each type\n□ [Other sampling strategy]\n\nEscalation required when:\n□ [Critical condition 1]\n□ [Critical condition 2]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<h3>Human Review Interface</h3>\n<p>What should reviewers see?</p>\n<pre><code>REVIEW PACKAGE CONTENTS\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n1. Original input/request\n2. AI-generated output\n3. Automated check results\n4. AI evaluation scores\n5. Flagged concerns\n6. Suggested actions\n\nREVIEWER OPTIONS:\n□ Approve as-is\n□ Approve with minor edits\n□ Request revision (with feedback)\n□ Reject (with reason)\n□ Escalate to [role]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<hr>\n"
        },
        {
          "id": "part-4:-design-feedback-loop-(10-minutes)",
          "title": "Part 4: Design Feedback Loop (10 minutes)",
          "type": "generic",
          "content": "### Capture Quality Data\n\nWhat should be logged for every output?\n\n```\nQUALITY LOG ENTRY\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nTimestamp: [datetime]\nRequest ID: [unique identifier]\nInput summary: [brief description]\nModel used: [model name/version]\n\nAutomated checks: [pass/fail summary]\nAI evaluation score: [weighted average]\nHuman review: [Approve/Edit/Reject/Escalate]\nHuman edits: [what was changed]\nFinal outcome: [what was shipped]\n\nProcessing time: [duration]\nToken usage: [count]\nCost: [amount]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### Define Improvement Triggers\n\nWhen should you adjust the system?\n\n| Metric | Threshold | Action |\n|--------|-----------|--------|\n| First-pass approval rate | < 70% | Review and improve prompts |\n| Average human edit time | > [X] min | Improve AI output quality |\n| Escalation rate | > 10% | Expand AI capability or scope |\n| Customer complaints | Any | Root cause analysis |\n| Cost per output | > [X] | Optimize model selection |\n\n### Feedback Integration Process\n\n```\nWEEKLY QUALITY REVIEW\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n1. Pull quality metrics dashboard\n2. Identify lowest-scoring areas\n3. Sample rejected/edited outputs\n4. Categorize common issues\n5. Prioritize improvements\n6. Update prompts/criteria\n7. Monitor impact\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n---",
          "htmlContent": "<h3>Capture Quality Data</h3>\n<p>What should be logged for every output?</p>\n<pre><code>QUALITY LOG ENTRY\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nTimestamp: [datetime]\nRequest ID: [unique identifier]\nInput summary: [brief description]\nModel used: [model name/version]\n\nAutomated checks: [pass/fail summary]\nAI evaluation score: [weighted average]\nHuman review: [Approve/Edit/Reject/Escalate]\nHuman edits: [what was changed]\nFinal outcome: [what was shipped]\n\nProcessing time: [duration]\nToken usage: [count]\nCost: [amount]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<h3>Define Improvement Triggers</h3>\n<p>When should you adjust the system?</p>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>Threshold</th>\n<th>Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>First-pass approval rate</td>\n<td>&lt; 70%</td>\n<td>Review and improve prompts</td>\n</tr>\n<tr>\n<td>Average human edit time</td>\n<td>&gt; [X] min</td>\n<td>Improve AI output quality</td>\n</tr>\n<tr>\n<td>Escalation rate</td>\n<td>&gt; 10%</td>\n<td>Expand AI capability or scope</td>\n</tr>\n<tr>\n<td>Customer complaints</td>\n<td>Any</td>\n<td>Root cause analysis</td>\n</tr>\n<tr>\n<td>Cost per output</td>\n<td>&gt; [X]</td>\n<td>Optimize model selection</td>\n</tr>\n</tbody></table>\n<h3>Feedback Integration Process</h3>\n<pre><code>WEEKLY QUALITY REVIEW\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n1. Pull quality metrics dashboard\n2. Identify lowest-scoring areas\n3. Sample rejected/edited outputs\n4. Categorize common issues\n5. Prioritize improvements\n6. Update prompts/criteria\n7. Monitor impact\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre>\n<hr>\n"
        },
        {
          "id": "part-5:-visualize-the-system-(5-minutes)",
          "title": "Part 5: Visualize the System (5 minutes)",
          "type": "generic",
          "content": "### Create Quality Gate Flow Diagram\n\n```\n[AI Output Generated]\n        │\n        ▼\n┌─────────────────┐\n│ GATE 1: AUTO    │──FAIL──┐\n│ Format, length, │        │\n│ prohibited terms│        │\n└────────┬────────┘        │\n         │ PASS            │\n         ▼                 │\n┌─────────────────┐        │\n│ GATE 2: AI EVAL │──FAIL──┤\n│ Quality scoring │        │\n│ Issue flagging  │        │\n└────────┬────────┘        │\n         │ PASS            │\n         ▼                 │\n┌─────────────────┐        │\n│ GATE 3: HUMAN   │──FAIL──┘\n│ (if triggered)  │        │\n└────────┬────────┘        │\n         │ PASS            │\n         ▼                 ▼\n    [APPROVED]        [REVISION]\n                          │\n                          ▼\n                    [Back to AI]\n```\n\n---",
          "htmlContent": "<h3>Create Quality Gate Flow Diagram</h3>\n<pre><code>[AI Output Generated]\n        │\n        ▼\n┌─────────────────┐\n│ GATE 1: AUTO    │──FAIL──┐\n│ Format, length, │        │\n│ prohibited terms│        │\n└────────┬────────┘        │\n         │ PASS            │\n         ▼                 │\n┌─────────────────┐        │\n│ GATE 2: AI EVAL │──FAIL──┤\n│ Quality scoring │        │\n│ Issue flagging  │        │\n└────────┬────────┘        │\n         │ PASS            │\n         ▼                 │\n┌─────────────────┐        │\n│ GATE 3: HUMAN   │──FAIL──┘\n│ (if triggered)  │        │\n└────────┬────────┘        │\n         │ PASS            │\n         ▼                 ▼\n    [APPROVED]        [REVISION]\n                          │\n                          ▼\n                    [Back to AI]\n</code></pre>\n<hr>\n"
        },
        {
          "id": "deliverable",
          "title": "Deliverable",
          "type": "generic",
          "content": "Create a Quality Gate Specification Document containing:\n\n1. **Quality Criteria Definition**\n   - All quality dimensions\n   - Detailed rubric with examples\n   - Minimum thresholds\n\n2. **Gate Specifications**\n   - Gate 1: Automated checks (complete list)\n   - Gate 2: AI evaluation prompt (ready to use)\n   - Gate 3: Human review protocol\n\n3. **Handoff Design**\n   - Review package contents\n   - Reviewer options and actions\n   - Escalation paths\n\n4. **Feedback System**\n   - Data capture schema\n   - Improvement triggers\n   - Review process\n\n5. **System Diagram**\n   - Visual flow of quality gates\n   - Decision points and paths\n\n---",
          "htmlContent": "<p>Create a Quality Gate Specification Document containing:</p>\n<ol>\n<li><p><strong>Quality Criteria Definition</strong></p>\n<ul>\n<li>All quality dimensions</li>\n<li>Detailed rubric with examples</li>\n<li>Minimum thresholds</li>\n</ul>\n</li>\n<li><p><strong>Gate Specifications</strong></p>\n<ul>\n<li>Gate 1: Automated checks (complete list)</li>\n<li>Gate 2: AI evaluation prompt (ready to use)</li>\n<li>Gate 3: Human review protocol</li>\n</ul>\n</li>\n<li><p><strong>Handoff Design</strong></p>\n<ul>\n<li>Review package contents</li>\n<li>Reviewer options and actions</li>\n<li>Escalation paths</li>\n</ul>\n</li>\n<li><p><strong>Feedback System</strong></p>\n<ul>\n<li>Data capture schema</li>\n<li>Improvement triggers</li>\n<li>Review process</li>\n</ul>\n</li>\n<li><p><strong>System Diagram</strong></p>\n<ul>\n<li>Visual flow of quality gates</li>\n<li>Decision points and paths</li>\n</ul>\n</li>\n</ol>\n<hr>\n"
        },
        {
          "id": "extension-challenge",
          "title": "Extension Challenge",
          "type": "generic",
          "content": "**Build a Working Prototype**\n\n1. Create a prompt that generates outputs for your chosen scenario\n2. Create the AI evaluation prompt\n3. Run 10 test cases through both\n4. Calculate your first-pass approval rate\n5. Identify the most common quality issues\n6. Refine your generation prompt based on findings\n7. Re-run and measure improvement\n\nDocument your iteration process and results.",
          "htmlContent": "<p><strong>Build a Working Prototype</strong></p>\n<ol>\n<li>Create a prompt that generates outputs for your chosen scenario</li>\n<li>Create the AI evaluation prompt</li>\n<li>Run 10 test cases through both</li>\n<li>Calculate your first-pass approval rate</li>\n<li>Identify the most common quality issues</li>\n<li>Refine your generation prompt based on findings</li>\n<li>Re-run and measure improvement</li>\n</ol>\n<p>Document your iteration process and results.</p>\n"
        }
      ]
    }
  ]
}